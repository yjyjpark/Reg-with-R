[["index.html", "R에 의한 회귀분석 머리말", " R에 의한 회귀분석 박동련 2024-02-23 머리말 회귀분석은 사회과학, 공학, 물리학, 생물학, 경영학, 경제학, 의학 등 거의 모든 학문 분야에서 가장 널리 사용되고 있는 통계적 분석 방법이라 할 수 있다. 우리가 설명하거나 예측하려고 하는 특정 현상을 나타내는 변수(반응변수)가 있고, 그 변수의 변동과 관련이 있을 것으로 예상되는 다른 변수(설명변수)들 사이에 존재할 수 있는 수학적 관계를 규명하는 통계적 방법이 바로 회귀분석인 것이다. 이 책은 회귀분석을 처음 접하는 독자들을 대상으로 작성하였으며, 구성은 다음과 같다. 1장에서는 회귀분석의 기본 개념에 대한 소개를 하였고, 2장과 3장에서는 하나의 설명변수만 사용하는 단순회귀모형에 대해 다루고 있다. 최소제곱추정량 유도 및 회귀모형에 대한 통계적 추론 등을 다루고 있다. 4장에서는 여러 개의 설명변수를 사용하는 다중회귀모형에 대해 다루고 있으며, 5장에서는 회귀모형의 가정 만족 여부를 확인하는 잔차분석을 소개하고 있다. 다중회귀모형은 실제 상황에서 많이 사용되는 모형이기 때문에, 4장부터는 R을 활용하는 실습문제를 많이 다루고 있다. R의 기초적인 사용법 및 패키지 tidyverse에 대한 소개 없이 사용하고 있으며, R code에는 프롬프트(&gt; 또는 +)를 제거하였고, console 창에 출력되는 실행 결과물은 ##으로 시작되도록 하였다. dplyr과 ggplot2 등을 포함한 R 사용법에 대한 소개는 R과 통계분석 에서 볼 수 있다. 이 책을 작성할 때의 R 세션 정보는 다음과 같다. sessionInfo() ## R version 4.3.2 (2023-10-31 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 11 x64 (build 22631) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=Korean_Korea.utf8 LC_CTYPE=Korean_Korea.utf8 ## [3] LC_MONETARY=Korean_Korea.utf8 LC_NUMERIC=C ## [5] LC_TIME=Korean_Korea.utf8 ## ## time zone: Asia/Seoul ## tzcode source: internal ## ## attached base packages: ## [1] stats graphics grDevices utils datasets methods base ## ## other attached packages: ## [1] lubridate_1.9.3 forcats_1.0.0 stringr_1.5.1 dplyr_1.1.4 ## [5] purrr_1.0.2 readr_2.1.5 tidyr_1.3.1 tibble_3.2.1 ## [9] ggplot2_3.4.4 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] gtable_0.3.4 jsonlite_1.8.8 compiler_4.3.2 tidyselect_1.2.0 ## [5] jquerylib_0.1.4 scales_1.3.0 yaml_2.3.8 fastmap_1.1.1 ## [9] R6_2.5.1 generics_0.1.3 knitr_1.45 bookdown_0.37 ## [13] munsell_0.5.0 bslib_0.6.1 pillar_1.9.0 tzdb_0.4.0 ## [17] rlang_1.1.3 utf8_1.2.4 stringi_1.8.3 cachem_1.0.8 ## [21] xfun_0.41 sass_0.4.8 timechange_0.3.0 cli_3.6.2 ## [25] withr_3.0.0 magrittr_2.0.3 digest_0.6.34 grid_4.3.2 ## [29] rstudioapi_0.15.0 hms_1.1.3 lifecycle_1.0.4 vctrs_0.6.5 ## [33] evaluate_0.23 glue_1.7.0 fansi_1.0.6 colorspace_2.1-0 ## [37] rmarkdown_2.25 tools_4.3.2 pkgconfig_2.0.3 htmltools_0.5.7 "],["ch1.html", "1 장 회귀모형 1.1 반응변수와 설명변수 사이의 관계 설정 1.2 회귀분석의 목적 1.3 회귀모형의 종류 1.4 R의 역할", " 1 장 회귀모형 사회현상, 자연현상, 경영현상이나 경제현상 등을 측정한 변수들의 변동은 그 현상과 관련된 여러 변수들과 관련이 있다고 볼 수 있다. 어떤 현상을 과학적 시각으로 설명하기 위해서는 해당 현상과 관련이 있는 변수들 사이의 관계를 설명하기 위한 적절한 모형을 설정해야 할 것이다. 회귀분석(regression analysis)이란 특정 현상과 그 현상에 영향을 주거나 혹은 관련이 있는 변수들 사이의 관계를 분석하고 모형화하기 위한 통계적 기법이다. 회귀분석에서는 변수들 사이의 함수관계를 이론적 근거나 경험적 판단에 의해서 설정하고, 관측된 자료에 의해서 함수관계를 추정해서 변수들 사이의 관계를 설명하거나 예측하는데 사용된다. 따라서 회귀분석은 사회과학, 공학, 물리학, 생물학, 경영학, 경제학, 의학 등 거의 모든 학문 분야에서 가장 널리 사용되고 있는 통계적 분석 방법인 것이다. 회귀모형을 설정할 때 우리가 관심이 있는 특정 현상의 변동을 나타내는 변수를 종속변수(dependent variable) 혹은 반응변수(response variable) 또는 결과변수(outcome variable)라고 하며, 일반적으로 \\(Y\\)로 표시한다. 반응변수가 나타내는 특정 현상에 연관되어 있을 것으로 판단되는 변수를 독립변수(independent variable) 혹은 설명변수(explanatory variable) 또는 예측변수(predictor variable)라고 하고, 일반적으로 \\(X\\)로 표시한다. 만일 설명변수의 수가 \\(k\\)개 있다고 하면 \\(X_{1}, X_{2}, \\ldots, X_{k}\\)로 표시한다. 예를 들어, 회사 마케팅 부서에서 특정 제품의 매출액 변동을 예측할 수 있는 모형을 설정하고자 할 때 제품의 매출액 변동에 영향을 줄 수 있는 변수들로 광고비 지출액, 가격수준, 기술수준, 디자인 선호수준, 소득수준, 영엽사원의 수, 경쟁제품의 수 등을 생각할 수 있는데, 이 경우에 반응변수는 제품의 매출액이 되고, 광고비 지출액 등 나머지 변수들은 설명변수가 된다. 또 다른 예로써 투자자문회사에서 고객의 투자행위에 대한 예측모형을 개발할 때 투자행위에 영향을 미칠 수 있는 변수들로 연소득 수준, 미래 경제상황지수, 시장 이자율 등을 이용하게 되는데, 여기에서 반응변수는 고개들의 투자액이 되고, 연소득 수준 등은 설명변수가 된다. 회귀모형은 모형에 포힘되는 설명변수가 한 개인 경우에는 단순회귀모형(simple regression model)이라고 하고, 두 개 이상의 설명변수가 모형에 포함되는 경우에는 다중회귀모형(multiple regression model)이라고 한다. 1.1 반응변수와 설명변수 사이의 관계 설정 반응변수와 설명변수 사이의 관계는 일반적으로 결정적 관계(deterministic relation)와 확률적 관계(stochastic relation)로 구분해 볼 수 있다. 결정적 관계는 반응변수와 설명변수 사이에 정확한 수학적 함수관계가 성립되는 경우를 의미하는 것으로 \\(Y=f(X)\\)와 같이 표시할 수 있다. 여기에서 함수 \\(f\\) 는 직선, 곡선 혹은 더 복잡한 형태의 함수식이 될 수 있다. 결정적 관계에서는 설명변수의 값이 주어지면, 함수 \\(f\\) 에 의하여 대응되는 반응변수의 값이 유일하게 결정된다. 예를 들어, 제품의 판매단가가 2만원 일 때 판매개수(\\(X\\))와 매출액(\\(Y\\)) 사이의 관계는 \\(Y=2X\\)로 표현되는 결정적 관계가 성립한다. 반응변수와 설명변수의 관계가 결정적인 경우에 함수 \\(f\\) 에 대한 추정은 통계적 접근이 필요하지 않는 분야가 된다. 확률적 관계는 설명변수로 반응변수의 변동을 100% 설명할 수 없는 관계를 의미한다. 예를 들어, 특정 제품의 매출액(\\(Y\\))과 광고비 지출액(\\(X\\)) 사이의 관계를 생각해 보자. 일반적으로 광고비 지출을 증가시키면 매출액이 증가하겠지만, 광고비 지출액만으로 매출액의 변동을 100% 설명하는 것은 불가능하다. 가격수준, 기술수준, 디자인 선호수준, 소득수준 등 매출액의 변동에 영향을 줄 수 있는 다른 많은 변수를 포함시키더라도 매출액 변동을 100% 설명하는 것은 불가능하다고 할 수 있다. 이와 같이 설명할 수 없는 반응변수의 변동이 항상 존재하는 경우에는 반응변수와 설명변수의 관계를 다음과 같이 오차항 \\(\\varepsilon\\) 을 포함시켜서 표현할 수 있다. \\[\\begin{equation} Y = f(X) + \\varepsilon \\tag{1.1} \\end{equation}\\] 식 (1.1)에서 함수 \\(f(X)\\) 는 반응변수와 설명변수 사이에 존재하는 체계적 정보(systematic information)를 표현하며, 오차항 \\(\\varepsilon\\)는 함수 \\(f(X)\\) 로 설명되지 않는 반응변수의 변동을 나타내는 확률변수로서 평균은 \\(0\\) 이며, 설명변수와는 독립이라고 가정한다. 1.2 회귀분석의 목적 회귀분석의 목적은 식 (1.1)의 함수 \\(f\\) 의 추정이라고 할 수 있는데, 함수 \\(f\\) 를 추정하려는 이유는 수집된 데이터에 존재하는 반응변수와 설명변수 사이의 관계를 명확하게 설명하려는 ’자료의 기술’과 새롭게 주어진 설명변수의 값에 대응되는 반응변수의 값에 대한 ’예측’이라고 할 수 있다. 자료의 기술(Data description) 자료들의 특징이나 변수들 사이의 관계 등을 명확하게 밝히는 것이 분석 목적이 되는 경우가 있다. 이런 경우에는 두 변수 사이의 함수형태를 가정하지 않는 비모수적 회귀모형이 효과적인 분석 방식이 될 수 있다. 그림 1.1는 2014년 호주 빅토리아 주에서 측정된 일일 최고 기온과 전기 사용량의 산점도와 두 변수에 대한 비모수적 회귀모형의 적합 결과를 나타낸 그래프이다. 명확한 2차 곡선의 관계가 잘 드러난 예제이다. 그림 1.1: 자료의 기술 목적으로 사용된 회귀모형의 예 예측(Prediction) 새롭게 관측되는 변수 \\(X\\) 의 자료에 대응되는 변수 \\(Y\\) 의 가장 정확한 예측값을 생성하는 것이 분석의 목적이 되는 경우가 많이 있다. 식 (1.1)의 관계에서 변수 \\(Y\\) 의 예측값 \\(\\widehat{Y}\\) 은 다음과 같이 생성된다. \\[\\begin{equation} \\widehat{Y} = \\hat{f}(X) \\end{equation}\\] 변수 \\(Y\\) 는 함수 \\(f\\) 와 오차항 \\(\\varepsilon\\) 의 합으로 구성되어 있지만, 우리가 예측할 수 있는 대상은 두 변수 사이의 체계적 정보를 담고 있는 함수 \\(f\\) 뿐이다. 오차항은 변수 \\(Y\\) 에서 함수 \\(f\\) 로 설명되지 않는 변동을 나타내는 것으로서 확률적 관계에서는 항상 존재하며, 예측이 가능한 대상은 아니다. 예측의 정확도는 변수 \\(Y\\) 와 예측값 \\(\\widehat{Y}\\) 의 차이의 제곱에 대한 평균으로 표시될 수 있다. \\[\\begin{align} E(Y-\\widehat{Y})^{2} &amp; = E\\left(f(X)+\\varepsilon - \\hat{f}(X) \\right)^{2} \\\\ &amp; = \\left(f(X)-\\hat{f}(X) \\right)^{2} + Var(\\varepsilon) \\tag{1.2} \\end{align}\\] 식 (1.2)에서 \\(Var(\\varepsilon)\\) 은 변수 \\(Y\\) 의 고유 특성으로 발생되는 변동이기 때문에 분석자가 감소시킬 수 있는 변량은 아니다. 예측의 정확도를 높이기 위해 분석자가 줄일 수 있는 변량은 \\(\\left(f(X)-\\hat{f}(X) \\right)^{2}\\) 이다. 1.3 회귀모형의 종류 1.3.1 모수적 회귀모형 모형 \\(f(X)\\)에 대해서 직선 혹은 곡선 등의 함수형태를 가정하는 방법이 있다. 이런 접근 방식을 모수적 회귀모형(parametric regression model)이라고 하며, 이 책에서 주로 살펴볼 모형이다. 반응변수와 설명변수의 관계가 설정된 함수형태를 항상 따른다는 것은 매우 강한 제약 조건이 될 수 있으나, 반응변수와 설명변수 사이에 존재하는 참 관계를 잘 나타낼 수 있다면 매우 효과적인 분석 모형이 될 것이다. 실제로 참 관계는 매우 복잡한 형태가 될 수 있으며, 대부분의 경우 정확한 모습은 미리 알 수 없다. 만일 참 관계가 그림 1.2와 같다면, 선형식으로 충분히 근사될 수 있을 것이다. 그림 1.2: 복잡한 관계에 대한 선형회귀 근사 단순선형회귀모형 예를 들어, 특정 제품의 매출액과 광고비 지출액의 관계가 그림 1.2와 같다고 하면, 두 변수의 관계는 다음과 같이 표현할 수 있다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\varepsilon \\tag{1.3} \\end{equation}\\] 반응변수와 설명변수에 대한 회귀모형을 식 (1.3)와 같이 설정하면, 회귀직선은 두 변수 사이의 참 관계(true relation)에 대한 근사(approximation)를 의미한다. 식 (1.3)에서 \\(\\beta_{0}\\) 와 \\(\\beta_{1}\\) 은 두 변수의 관계를 규정하는 ’모집단 회귀직선’의 Y축 절편과 직선의 기울기를 각각 나타내는 있는 모수(parameter)이며, 회귀계수(regression coefficient)라고 불린다. 식 (1.3)를 단순선형회귀모형(simple linear regression model)이라고 부른다. 여기에서 ’단순’은 모형에 설명변수가 하나만 있다는 것을 의미하고, ’선형’은 모형에 포함된 모수 사이의 관계가 선형임을 의미한다. 단순회귀모형의 경우에 두 변수 사이의 관계를 탐색하기 위한 첫 번째 단계는 산점도를 작성하는 것이다. 예를 들어, 함수 \\(f\\) 를 추정하기 위해 두 변수에 대한 \\(n=50\\) 의 자료를 수집하여 작성한 산점도가 그림 1.3와 같다고 하자. 그림 1.3에서 자료들은 직선 위에 위치하지 않고 있는데, 이와 같은 반응변수의 관측값과 직선 (\\(\\beta_{0}+\\beta_{1}X\\))의 차이를 나타내기 위하여 오차항 \\(\\varepsilon\\)이 포함되었다. 오차항은 광고비 지출 이외의 다른 변수들의 영향이나 측정 오차 등을 포함하는 통계적 오차가 된다. 그림 1.3: 직선 관계 함수 함수 \\(f\\) 를 추정하는 데 사용되는 자료를 training data(학습 데이터)라고 한다. Training data의 산점도가 그림 1.3와 같다면 함수 \\(f\\)를 식 (1.3)에 주어진 직선 형태로 가정할 수 있을 것이다. 추정된 \\(\\hat{f}(x)\\) 인 직선이 training data를 잘 설명하고 있음을 알 수 있다. 또한 만일 training data의 산점도가 그림 1.4와 같다면, \\(f\\) 를 2차 함수 형태로 가정할 수 있을 것이다. 추정된 \\(\\hat{f}(x)\\) 인 2차 곡선이 training data를 잘 설명하고 있음을 알 수 있다. 그림 1.4: 이차 관계 함수 회귀분석의 중요한 목적 중 하나는 회귀모형에 포함된 모수를 추정하는 것이다. 이러한 절차를 모형의 적합(fitting)이라고 하며, 2장에서는 회귀분석에서 가장 많이 사용되는 추정 방법인 최고제곱추정 방법이 소개될 것이다. 예를 들어 그림 1.3에 주어진 자료를 이용해서 구한 최소제곱추정에 의한 회귀직선식은 다음과 같이 주어진다. \\[\\begin{equation} \\hat{Y}=2.1 + 4.9X \\end{equation}\\] 여기에서 \\(\\hat{Y}\\)은 광고비 지출액이 \\(X\\)일 때 대응되는 평균 매출액의 추정값이 된다. 일반적으로 추정된 회귀식은 설명변수가 관측된 범위 내에서만 적용하는 것이 바람직하다. 예를 들면, 그림 1.5에서 두 변수 \\(Y\\)와 \\(X\\)에 대한 자료가 \\(x_{1} \\leq X \\leq x_{2}\\)의 범위에서 수집되었다고 하자. 이 범위에서 참 관계는 직선으로 근사될 수 있을 것이다. 그러나 \\(X \\geq x_{2}\\)의 범위에서 반응변수의 값을 선형 근사 관계를 근거로 예측하고자 한다면 심각한 신뢰성 문제가 발생하게 된다. 설명변수가 관측된 범위를 벗어난 구역에 대한 예측은 가능한 시도하지 않는 것이 좋겠지만, 시도를 해야만 하는 상황이라면 매우 조심해서 예측 결과를 적용해야 할 것이다. 그림 1.5: 회귀모형의 유효성 문제 다중회귀모형 설명변수의 개수를 \\(k\\) 개로 늘린 회귀모형은 다음과 같이 설정할 수 있다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k} + \\varepsilon \\tag{1.4} \\end{equation}\\] 식 (1.4)을 다중선형회귀모형(multiple linear regression model)이라고 부른다. 여기에서 ’다중’은 모형에 포함된 설명변수의 개수가 두 개 이상인 경우를 의미한다. 반응변수의 변동을 설명변수 하나만으로 충분히 설명한다는 것은 현실적으로 거의 불가능한 상황이라고 할 수 있다. 따라서 실제 상황에서는 설명변수의 개수를 \\(k\\) 개로 늘린 다중회귀모형이 더 현실적인 모형이라고 할 수 있다. 1.3.2 비모수적 회귀모형 모형 \\(f(X)\\)에 대해 특별한 함수 형태를 가정하지 않는 방법도 있다. 비모수적 회귀모형(non-parametric regression model)이라고 하는데, 자세한 설명은 이 책의 수준을 벗어나기 때문에 생략하겠지만, 자료탐색 수준에서 상당히 많이 사용되는 국소회귀(local regression)가 여기에 속하는 방법이다. 모수적 회귀모형에서는 회귀함수 \\(f\\)의 형태에 대한 가정을 하게 되는데, 만일 이 가정에 오류가 있다면 추정 및 예측 결과에도 문제가 있게 된다. 그림 1.6에서 볼 수 있는 산점도는 비교적 함수의 형태가 명확한 그림 1.3와 그림 1.4의 경우와는 많이 다른 경우가 된다. 그림 1.6: 상대적으로 복잡한 형태의 관계 위쪽에 있는 두 그래프는 모두 모수적 회귀모형을 사용해서 함수 \\(f\\) 를 추정한 결과를 나타내고 있다. 왼쪽 위 그래프는 두 변수의 관계를 직선으로 가정하고 추정한 직선이 표시되어 있고, 오른쪽 위 그래프는 두 변수의 관계를 2차 곡선으로 가정하고 추정한 곡선이 표시되어 있다. 하지만 두 그래프에서 보여준 추정 결과는 모두 만족스럽지 못하다고 하겠다. 아래쪽에 있는 두 그래프는 비모수 회귀모형에 속한 두 모형으로 추정된 곡선이 표시되어 있다. 주어진 자료에 대해 비교적 잘 설명하고 있음을 알 수 있다. 주어진 자료를 잘 설명할 수 있다는 장점은 있지만, 비모수적 회귀모형으로 함수 \\(f\\) 를 효과적으로 추정하기 위해서는 많은 자료가 필요하다는 문제가 있다. 특히 다중회귀모형의 경우에는 모수적 회귀모형에 비해 대단히 많은 자료가 필요하기 때문에, 현실적으로 적용하는 데 한계가 있을 수 있다. 1.4 R의 역할 최종 회귀모형을 얻기 위해서는 많은 분석 단계를 거쳐야 한다. 자료의 특성을 잘 보여줄 수 있는 그래프 작성이 가능해야 하며, 반복적인 처리를 통해서 적합 과정의 문제를 발견할 수 있어야 하고, 효과적인 예측을 실시할 수 있어야 한다. 따라서 성공적인 분석을 실시하기 위해서는 고급통계분석에 가장 적절한 프로그래밍 언어를 선택해야 한다. 이 책에서는 R을 사용하여 회귀분석의 모든 과정을 자세하게 소개할 것이다. 기본적인 R 사용법이나 패키지 dplyr과 ggplot2에 대해서는 책을 읽는 분들이 어느 정도 익숙하다는 가정을 하고 있다. 또한 책에 제공된 R code에는 프롬프트(&gt; 또는 +)를 제거하였고, console 창에 출력되는 결과물은 ##으로 시작되도록 하였다. 이 책을 작성할 때의 R 세션 정보는 다음과 같다. sessionInfo() ## R version 4.3.2 (2023-10-31 ucrt) ## Platform: x86_64-w64-mingw32/x64 (64-bit) ## Running under: Windows 11 x64 (build 22631) ## ## Matrix products: default ## ## ## locale: ## [1] LC_COLLATE=Korean_Korea.utf8 LC_CTYPE=Korean_Korea.utf8 ## [3] LC_MONETARY=Korean_Korea.utf8 LC_NUMERIC=C ## [5] LC_TIME=Korean_Korea.utf8 ## ## time zone: Asia/Seoul ## tzcode source: internal ## ## attached base packages: ## [1] splines stats graphics grDevices utils datasets methods ## [8] base ## ## other attached packages: ## [1] patchwork_1.2.0 expsmooth_2.3 fma_2.5 forecast_8.21.1 ## [5] fpp2_2.5 lubridate_1.9.3 forcats_1.0.0 stringr_1.5.1 ## [9] dplyr_1.1.4 purrr_1.0.2 readr_2.1.5 tidyr_1.3.1 ## [13] tibble_3.2.1 ggplot2_3.4.4 tidyverse_2.0.0 ## ## loaded via a namespace (and not attached): ## [1] gtable_0.3.4 xfun_0.41 bslib_0.6.1 lattice_0.21-9 ## [5] tzdb_0.4.0 quadprog_1.5-8 vctrs_0.6.5 tools_4.3.2 ## [9] generics_0.1.3 curl_5.2.0 parallel_4.3.2 fansi_1.0.6 ## [13] highr_0.10 xts_0.13.2 pkgconfig_2.0.3 Matrix_1.6-5 ## [17] lifecycle_1.0.4 farver_2.1.1 compiler_4.3.2 munsell_0.5.0 ## [21] htmltools_0.5.7 sass_0.4.8 yaml_2.3.8 pillar_1.9.0 ## [25] crayon_1.5.2 jquerylib_0.1.4 cachem_1.0.8 nlme_3.1-164 ## [29] fracdiff_1.5-2 tidyselect_1.2.0 digest_0.6.34 stringi_1.8.3 ## [33] bookdown_0.37 labeling_0.4.3 tseries_0.10-55 fastmap_1.1.1 ## [37] grid_4.3.2 colorspace_2.1-0 cli_3.6.2 magrittr_2.0.3 ## [41] utf8_1.2.4 withr_3.0.0 scales_1.3.0 timechange_0.3.0 ## [45] TTR_0.24.4 rmarkdown_2.25 quantmod_0.4.25 nnet_7.3-19 ## [49] timeDate_4032.109 zoo_1.8-12 hms_1.1.3 urca_1.3-3 ## [53] evaluate_0.23 knitr_1.45 lmtest_0.9-40 mgcv_1.9-1 ## [57] rlang_1.1.3 Rcpp_1.0.12 glue_1.7.0 rstudioapi_0.15.0 ## [61] jsonlite_1.8.8 R6_2.5.1 "],["simple-reg.html", "2 장 단순선형회귀모형 2.1 단순회귀모형의 설정 2.2 회귀계수의 최소제곱추정 2.3 오차분산 \\(\\sigma^{2}\\)의 추정", " 2 장 단순선형회귀모형 선형회귀모형의 가장 기본적인 형태는 한 개의 설명변수로 반응변수의 변동을 설명하려는 단순회귀모형이다. 이 장에서는 특히 두 변수의 관계가 직선인 경우에 대해서만 살펴보겠다. 2.1 단순회귀모형의 설정 설명변수가 하나이고, 반응변수와의 관계가 직선인 경우에 회귀모형은 다음과 같이 설정된다. \\[\\begin{equation} Y_{i} = \\beta_{0} + \\beta_{1}X_{i} + \\varepsilon_{i}, ~~i=1,2,\\ldots,n \\tag{2.1} \\end{equation}\\] 여기에서 \\(Y_{i}\\) 는 반응변수의 \\(i\\) 번째 값, \\(X_{i}\\) 는 설명변수의 \\(i\\) 번째 값을 표시한다. 절편 \\(\\beta_{0}\\) 와 \\(\\beta_{1}\\) 은 모수(parameter)이며, 따라서 알려지지 않은 상수(constant)이다. 오차항 \\(\\varepsilon_{i}\\) 는 반응변수의 변동 중 설명변수로 설명할 수 없는 부분을 나타내는 확률변수인데, \\(n\\) 개의 오차항이 모두 평균이 0, 분산은 \\(\\sigma^{2}\\) 이며, 서로 독립이라고 가정한다. 또한 설명변수의 값 \\(X_{i}\\) 는 분석자에 의해서 값이 통제될 수 있는 변량으로써 확률변수가 아닌 상수라고 가정한다. 이러한 가정에서 단순회귀모형의 반응변수 \\(Y_{i}\\) 의 특성을 다음과 같이 유도할 수 있다. \\(Y_{i}\\) 는 상수인 \\(\\beta_{0}+\\beta_{1}X_{i}\\)와 확률변수인 \\(\\varepsilon_{i}\\)의 합으로 구성되어 있다. 따라서 \\(Y_{i}\\)는 확률변수이다. 설명변수의 값이 \\(X_{i}\\)로 주어졌을 때, 확률변수 \\(Y_{i}\\)의 평균은 다음과 같다. \\[\\begin{align*} E(Y_{i}|X_{i}) &amp; = E(\\beta_{0} + \\beta_{1}X_{i} + \\varepsilon_{i}) \\\\ &amp; = \\beta_{0} + \\beta_{1}X_{i} + E(\\varepsilon_{i}) \\\\ &amp; = \\beta_{0} + \\beta_{1}X_{i} \\end{align*}\\] 위 수식에서 \\(\\beta_{0}+\\beta_{1}X_{i}\\)는 상수이기 때문에 기대값은 동일한 값이 되고, \\(E(\\varepsilon_{i})=0\\) 은 가정에 의한 결과이다. 설명변수의 값이 \\(X_{i}\\)로 주어졌을 때, 확률변수 \\(Y_{i}\\)의 분산은 다음과 같다. \\[\\begin{align*} Var(Y_{i}|X_{i}) &amp; = Var(\\beta_{0} + \\beta_{1}X_{i} + \\varepsilon_{i}) \\\\ &amp; = Var(\\beta_{0} + \\beta_{1}X_{i}) + Var(\\varepsilon_{i}) \\\\ &amp; = \\sigma^{2} \\end{align*}\\] 위 수식에서 \\(\\beta_{0}+\\beta_{1}X_{i}\\)는 상수이기 때문에 분산은 0이고 되고, \\(Var(\\varepsilon_{i}) = \\sigma^{2}\\) 는 가정에 의한 결과이다. 위 결과를 종합하면 설명변수의 값이 \\(X_{i}\\)로 주어졌을 때, 확률변수 \\(Y_{i}\\)는 평균이 \\(\\beta_{0}+\\beta_{1}X_{i}\\)이고 분산이 \\(\\sigma^{2}\\)이 된다. 즉, 반응변수의 평균은 설명변수의 값에 따라 절편이 \\(\\beta_{0}\\)이고 기울기가 \\(\\beta_{1}\\)인 직선을 따라 움직인다는 것을 알 수 있는데, 이 직선을 ’모집단 회귀직선’이라고 한다. 또한 반응변수의 평균은 설명변수의 값에 따라 변하지만, 분산은 항상 일정한 값을 갖는다는 것을 알 수 있다. \\(n\\)개의 오차항 \\(\\varepsilon_{1}, \\ldots, \\varepsilon_{n}\\)들이 서로 독립이라고 가정했기 때문에 반응변수도 서로 독립이 된다. 따라서 \\(Y_{i}\\)와 \\(Y_{j}\\)는 \\(i \\ne j\\) 라면, 서로 독립이기 때문에 두 변수의 공분산은 \\(Cov(Y_{i}, Y_{j})=0\\) 이 된다. 두 확률변수 \\(X\\)와 \\(Y\\)의 공분산 \\(Cov(X,Y)\\)는 다음과 같이 정의된다. \\[\\begin{align} Cov(X,Y) &amp; = E\\left((X-E(X))(Y-E(Y)) \\right) \\\\ &amp; = E(XY) - E(X)E(Y) \\tag{2.2} \\end{align}\\] 만일 확률변수 \\(X\\)와 \\(Y\\)가 서로 독립이면, \\(E(XY)=E(X)E(Y)\\)의 관계가 성립한다. 따라서 서로 독립인 두 확률변수의 공분산은 식 (2.2)에 의해서 0이 됨을 알 수 있다. 공분산 \\(Cov(X,Y)\\) 는 변수 \\(X\\)와 \\(Y\\) 사이에 존재하는 선형 관련성을 표현하고 있다. 식 (2.2)에서 볼 수 있듯이 공분산은 변수 \\((X-E(X))(Y-E(Y)\\)의 평균값을 나타낸다. 만일 \\((X,Y)\\)의 자료 중 \\((X-E(X))&gt;0\\) 이고 \\((Y-E(Y))&gt;0\\) 을 만족하거나 \\((X-E(X))&lt;0\\) 이고 \\((Y-E(Y))&lt;0\\) 인 조건을 만족하는 자료가 많게 되면 변수 \\((X-E(X))(Y-E(Y)\\) 의 값은 대부분 양수의 값이 되며, 따라서 변수 \\((X-E(X))(Y-E(Y)\\) 의 평균값인 두 변수의 공분산은 0보다 큰 값을 갖게 된다. 하지만 \\((X,Y)\\)의 자료 중 \\((X-E(X))\\)와 \\((Y-E(Y))\\)의 부호가 서로 반대가 되는 자료가 많게 되면 변수 \\((X-E(X))(Y-E(Y)\\)의 값은 대부분 음수의 값이 되며, 따라서 변수 \\((X-E(X))(Y-E(Y)\\)의 평균값인 두 변수의 공분산은 0보다 작은 값을 갖게 된다. 그런데 \\((X,Y)\\)의 자료 중 \\((X-E(X))\\)와 \\((Y-E(Y))\\)의 부호가 같은 자료들이 많다는 것은 그림 2.1에서 1구역과 3구역에 대부분의 자료가 있다는 것을 의미하고 있기 때문에 한 변수의 값이 증가하거나 감소하면 다른 변수의 값도 함께 증가하거나 감소하는 ’양의 관계’에 있다는 것을 알 수 있다. 또한 \\((X,Y)\\)의 자료 중 \\((X-E(X))\\)와 \\((Y-E(Y))\\)의 부호가 반대인 자료들이 많다는 것은 그림 2.1에서 2구역과 4구역에 대부분의 자료가 있다는 것을 의미하는 것이고, 따라서 한 변수의 값이 증가하거나 감소하면 다른 변수의 값은 정반대의 방향으로 움직이는 ’음의 관계’에 있다는 것을 알 수 있다. 이러한 관련성으로 두 변수의 공분산이 큰 양수 값이면 두 변수의 관계는 강한 양의 관계가 있음을 알 수 있으며, 큰 음수의 값을 취하고 있으면 강한 음의 관계가 있음을 알 수 있다. 그러나 공분산의 값은 변수가 취하는 값 자체의 절대적 크기에도 영향을 받고 있기 때문에 스케일이 다른 자료에 대한 비교는 의미가 없게 된다. 이 문제는 공분산을 두 변수의 표준편차를 나누는 것으로 해결될 수 있으며, 이것이 상관계수가 된다. 그림 2.1: 두 변수 X와 Y의 공분산 구성요소 2.2 회귀계수의 최소제곱추정 식 (2.1)을 이용해서 \\(Y\\) 의 변동을 설명하고 예측하려면, 값이 알려져 있지 않는 모수 \\(\\beta_{0}\\) 와 \\(\\beta_{1}\\) 을 추정해야 한다. 두 변수 \\(Y\\) 와 \\(X\\) 에 대해 관측된 \\(n\\) 개의 자료를 \\((y_{1},x_{1})\\), \\((y_{2}, x_{2})\\), \\(\\ldots\\), \\((y_{n}, x_{n})\\) 이라고 하자. 두 모수 \\(\\beta_{0}\\) 와 \\(\\beta_{1}\\) 의 추정은 \\(n\\) 개의 자료를 가장 잘 설명할 수 있는 직선을 구하는 과정이라고 할 수 있는데, ’자료를 가장 잘 설명하는 직선’이란 자료와 직선 사이에 간격이 가장 작은 직선이라고 볼 수 있다. 최소제곱추정법은 이런 개념을 활용한 추정법이며, 선형회귀모형에서 가장 널리 사용되는 회귀계수의 추정방법이다. 2.2.1 모수 \\(\\beta_{0}\\)와 \\(\\beta_{1}\\)의 추정 두 모수의 추정값을 \\(\\hat{\\beta}_{0}\\) 과 \\(\\hat{\\beta}_{1}\\) 라고 하면, \\(X\\) 변수의 \\(i\\) 번째 관찰값에 대한 \\(Y\\) 변수의 \\(i\\) 번째 예측값은 \\(\\hat{y}_{i} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i}\\) 가 된다. \\(Y\\) 변수의 관찰값인 \\(y_{i}\\) 와 예측값인 \\(\\hat{y}_{i}\\) 의 차이인 \\(y_{i}-\\hat{y}_{i}\\) 는 자료와 회귀직선 사이의 거리가 되는데, 이것을 잔차(residual)라고 하며, \\(e_{i}\\) 로 표시한다. 따라서 \\(\\hat{\\beta}_{0}\\) 과 \\(\\hat{\\beta}_{1}\\) 은 다음에 주어지는 잔차의 제곱합(Residual sum of squares; RSS)을 최소화시키도록 구해야 한다. \\[\\begin{equation} RSS = \\sum_{i=1}^{n} \\left( y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i} \\right)^{2} \\tag{2.3} \\end{equation}\\] 식 (2.3)의 RSS를 최소화시키는 추정값 \\(\\hat{\\beta}_{0}\\) 과 \\(\\hat{\\beta}_{1}\\) 을 구하기 위해서, RSS를 \\(\\hat{\\beta}_{0}\\) 과 \\(\\hat{\\beta}_{1}\\) 에 대하여 각각 편미분을 실시해서 얻은 두 개의 방정식은 다음과 같다. \\[\\begin{align} \\frac{\\partial RSS}{\\partial \\hat{\\beta}_{0}} &amp; = -2 \\sum_{i=1}^{n}(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i}) = 0\\\\ \\frac{\\partial RSS}{\\partial \\hat{\\beta}_{1}} &amp; = -2 \\sum_{i=1}^{n}x_{i}(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i}) = 0 \\tag{2.4} \\end{align}\\] 식 (2.4)을 정리하면 다음과 같다. \\[\\begin{align} \\sum_{i=1}^{n}y_{i} &amp; = n \\hat{\\beta}_{0} + \\hat{\\beta}_{1} \\sum_{i=1}^{n}x_{i} \\\\ \\sum_{i=1}^{n}x_{i}y_{i} &amp; = \\hat{\\beta}_{0}\\sum_{i=1}^{n}x_{i} + \\hat{\\beta}_{1}\\sum_{i=1}^{n}x_{i}^{2} \\tag{2.5} \\end{align}\\] 식 (2.5)를 \\(\\hat{\\beta}_{0}\\) 과 \\(\\hat{\\beta}_{1}\\) 에 대하여 정리하면 각각 다음의 결과를 얻게 된다. \\[\\begin{align} \\hat{\\beta}_{1} &amp;= \\frac{\\sum_{i=1}^{n}(x_{i}-\\overline{x})(y_{i}-\\overline{y})}{\\sum_{i=1}^{n}(x_{i}-\\overline{x})^{2}} \\\\ \\hat{\\beta}_{0} &amp;= \\overline{y}-\\hat{\\beta}_{1}\\overline{x} \\tag{2.6} \\end{align}\\] 단, \\(\\overline{y}\\) 와 \\(\\overline{x}\\) 는 표본평균이다. 추정된 회귀직선의 기울기인 \\(\\hat{\\beta}_{1}\\)의 의미는 설명변수를 한 단위 증가시켰을 때 반응변수의 평균 변화량의 추정값이 되어서, \\(\\hat{\\beta}_{1}&gt;0\\)이면 설명변수 값의 증가가 반응변수 값의 증가로 연결되는 관계를 의미하고, 반면에 \\(\\hat{\\beta}_{1}&lt;0\\)이면 설명변수 값의 증가가 반응변수 값의 감소로 연결되는 관계를 의미한다. 절편인 \\(\\hat{\\beta}_{0}\\) 은 설명변수의 값이 0일 때, 반응변수의 평균값의 추정값이 되는데, 만일 설명변수의 값 범위에 0이 포함되지 않는다면 특별한 의미를 부여하기는 어렵다. 식 (2.1)에 포함된 오차항 \\(\\varepsilon_{i}\\)는 자료를 통해서 관측될 수 없는 변량이다. 하지만 회귀모형에서는 오차항에 대해 몇 가지 가정을 하고 있으며, 이 가정이 만족되지 않는 자료를 대상으로 식 (2.1)의 회귀모형을 적용시켜 분석하게 되면, 그 결과에 심각한 문제가 발생할 수 있다. 따라서 가정 만족 여부를 확인하는 것은 매우 중요한 분석 과정이 되는데, 이 경우 오차항 대신 사용할 수 있는 것이 잔차이다. 잔차분석에 대해서는 5장에서 살펴보겠다. \\(\\bullet\\) 예제 2.1: 모집단 회귀직선과 추정된 회귀직선 모집단 회귀직선과 오차항의 분포는 일반적으로 알려져 있지 않지만, 회귀모형의 의미를 살펴보기 위해서 \\(E(Y|X)=5+2X\\) 라고 가정하고, 오차항 \\(\\varepsilon\\)은 평균이 0이고 분산이 1인 표준정규분포라고 가정하자. 설명변수의 값이 1, 4, 2, 7, 12, 6, 0, 3, 5, 8, 7 로 주어졌을 때, 표준정규분포에서 발생시킨 오차값을 추가해서 반응변수의 값을 생성해 보자. 단순선형회귀모형을 자료에 적합시켜서 절편과 기울기를 추정하고 모집단 회귀직선과 비교해 보자. 주어진 설명변수의 값에 대해 정규 난수를 추가한 반응변수 값을 생성해서 데이터 프레임으로 만들어 보자. 표준정규분포에서 난수 생성은 함수 rnorm()으로 수행할 수 있다. library(tidyverse) set.seed(12) df2.1 &lt;- tibble(x = c(1, 4, 2, 7, 12, 6, 0, 3, 5, 8, 7), y = 5 + 2*x + rnorm(n = length(x)) ) 생성된 모의자료는 다음과 같다. df2.1 ## # A tibble: 11 × 2 ## x y ## &lt;dbl&gt; &lt;dbl&gt; ## 1 1 5.52 ## 2 4 14.6 ## 3 2 8.04 ## 4 7 18.1 ## 5 12 27.0 ## 6 6 16.7 ## 7 0 4.68 ## 8 3 10.4 ## 9 5 14.9 ## 10 8 21.4 ## 11 7 18.2 선형화귀모형의 적합은 함수 lm()으로 할 수 있으며, 일반적인 사용법은 lm(formula, data, ...)이다. formula는 설정된 회귀모형을 나타내는 R 모형 공식으로써, 단순회귀모형의 경우에는 y ~ x와 같이 물결표(~)의 왼쪽에는 반응변수, 오른쪽에는 설명변수를 두면 된다. R 모형 공식에 대해서는 다중회귀모형에서 더 자세하게 살펴보겠다. data는 회귀분석에 사용될 데이터 프레임을 지정하는 것으로써, 이 예제에서는 위에서 생성한 데이터 프레임 df2.1를 지정하면 된다. 모의자료가 있는 데이터 프레임 df2.1를 대상으로 단순회귀모형을 함수 lm()으로 적합시켜보자. 적합 결과는 \\(\\hat{y} = 4.764 + 1.948x\\) 임을 알 수 있다. fit2.1 &lt;- lm(y ~ x, data = df2.1) fit2.1 ## ## Call: ## lm(formula = y ~ x, data = df2.1) ## ## Coefficients: ## (Intercept) x ## 4.764 1.948 함수 lm()으로 생성된 객체 fit2.1을 단순하게 출력시키면 추정된 회귀계수만 나타나지만, 사실 객체 fit2.1은 다음과 같이 많은 양의 정보가 담겨 있는 리스트 객체이다. names(fit2.1) ## [1] &quot;coefficients&quot; &quot;residuals&quot; &quot;effects&quot; &quot;rank&quot; ## [5] &quot;fitted.values&quot; &quot;assign&quot; &quot;qr&quot; &quot;df.residual&quot; ## [9] &quot;xlevels&quot; &quot;call&quot; &quot;terms&quot; &quot;model&quot; 사용자마다 필요한 정보가 서로 다를 수 있기 때문에 SAS나 SPSS에서와 같이 모든 결과물을 한 번에 출력하는 것은 좋은 방법이 아닐 수 있다. 객체 fit2.1에 담겨 있는 필요한 정보를 획득하기 위해서는 해당하는 함수를 사용해야 하며, 앞으로 차근차근 살펴보겠다. 모집단 회귀직선 및 생성된 모의자료, 그리고 추정된 회귀직선을 함께 나타낸 그래프가 그림 2.2이다. 추정된 회귀직선은 모집단 회귀직선과 매우 비숫하지만 완벽하게 일치하지 않음을 알 수 있다. 그림 2.2: 예제 2.1에서 사용된 모의자료와 회귀직선 \\(\\bullet\\) 예제 2.2: 매출액에 대한 광고효과 분석 피자 전문 체인점 영업부서에서는 매출액에 대한 광고효과를 분석하기 위하여 유사한 인구분포를 갖는 20개 판매지역의 매출액 규모와 광고비 지출에 대한 자료를 수집하였다. 수집된 자료는 파일 ex2-2.csv에 입력되었고, 자료의 단위는 100만원이다. 매출액 규모와 광고비 지출 사이의 산점도를 작성하고, 두 변수 사이의 관계를 살펴보자. 최소제곱법에 의한 단순회귀직선을 추정하고, 추정된 \\(\\hat{\\beta}_{1}\\)의 의미를 해석해 보자. 자료가 콤마로 구분된 CSV 파일을 함수 readr::read_csv()로 불러와서 두 변수의 산점도를 작성해 보자. df2.2 &lt;- readr::read_csv(&quot;Data/ex2-2.csv&quot;) df2.2 ## # A tibble: 20 × 2 ## sales advertisement ## &lt;dbl&gt; &lt;dbl&gt; ## 1 425 23 ## 2 370 21 ## 3 200 16 ## 4 580 34 ## 5 620 32 ## 6 650 36 ## 7 700 40 ## 8 490 37 ## 9 610 35 ## 10 290 20 ## 11 320 20 ## 12 350 21 ## 13 400 23 ## 14 517 21 ## 15 545 30 ## 16 590 32 ## 17 711 39 ## 18 650 37 ## 19 740 41 ## 20 660 38 ggplot(df2.2, aes(x = advertisement, y = sales)) + geom_point(size = 2) + geom_smooth(se = FALSE, aes(color = &quot;비모수적 회귀곡선&quot;)) + geom_smooth(method = &quot;lm&quot;, se = FALSE, aes(color = &quot;회귀직선&quot;)) + labs(color = NULL) 그림 2.3: 예제 2.2 자료의 산점도와 비모수적 회귀곡선 비모수적 회귀곡선은 두 변수의 관계를 가장 잘 나타내는 곡선을 추정하는 기능을 가지고 있다. 그림 2.3에 작성된 비모수적 회귀곡선은 대체로 직선의 형태를 취하고 있음을 알 수 있으며, 함께 표시된 회귀직선과 큰 차이가 없는 것을 알 수 있다. 이것으로 두 변수의 관계를 직선으로 설정하는 데에 큰 무리가 없음을 알 수 있다. 함수 lm()으로 단순회귀직선을 추정해 보자. fit2.2 &lt;- lm(sales ~ advertisement, data = df2.2) fit2.2 ## ## Call: ## lm(formula = sales ~ advertisement, data = df2.2) ## ## Coefficients: ## (Intercept) advertisement ## -6.944 17.713 적합 결과는 \\(\\hat{y} = -6.994 + 17.713x\\) 임을 알 수 있다. 추정된 직선의 기울기 \\(\\hat{\\beta}_{1}=17.713\\) 의 의미는 광고비 지출을 한 단위인 100만원 증가시키면 평균 매출액의 규모가 17.713백만원 증가한다는 것이 된다. 절편 \\(\\hat{\\beta}_{0}=-6.994\\) 의 의미는 광고비 지출이 \\(0\\) 일 때 평균 매출액은 -6.994백만원이라는 것이 된다. 즉, 광고를 하지 않으면 적자를 본다는 것인데, 이러한 해석은 조사된 광고비 자료의 범위에 \\(0\\) 이 포함되어 있지 않기 때문에 적절하지 않다고 하겠다. 2.2.2 최소제곱추정량의 특성 식 (2.1)에서 설정된 단순회귀모형에서 모수 \\(\\beta_{0}\\)와 \\(\\beta_{1}\\)의 최소제곱추정량인 \\(\\hat{\\beta}_{0}\\)과 \\(\\hat{\\beta}_{1}\\)은 몇 가지 중요한 통계적 특성을 갖는다. 첫 번째 특성은 식 (2.6)의 \\(\\hat{\\beta}_{0}\\)과 \\(\\hat{\\beta}_{1}\\)은 반응변수 \\(Y_{i}\\)의 선형결합으로 표시된다는 것이다. 먼저 \\(\\hat{\\beta}_{1}\\)의 경우를 살펴보자. \\[\\begin{align} \\hat{\\beta}_{1} &amp; = \\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X})(Y_{i}-\\overline{Y})}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}} \\\\ &amp; = \\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X})Y_{i}-\\sum_{i=1}^{n}(X_{i}-\\overline{X})\\overline{Y}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}} \\\\ &amp; = \\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X})Y_{i}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}} \\\\ &amp; = \\sum_{i=1}^{n}\\left(\\frac{X_{i}-\\overline{X}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right)Y_{i} \\\\ &amp; = \\sum_{i=1}^{n}c_{i}Y_{i} \\tag{2.7} \\end{align}\\] 단, \\(c_{i} = (X_{i}-\\overline{X})/\\sum(X_{i}-\\overline{X})^{2}\\). \\(\\hat{\\beta}_{0}\\)의 경우에는 \\(\\overline{Y}-\\hat{\\beta}_{1}\\overline{X}\\)의 표현식에서 \\(\\hat{\\beta}_{1}\\)이 \\(Y_{i}\\)의 선형결합이기 때문에 자연스럽게 \\(Y_{i}\\)의 선형결합으로 표시됨을 알 수 있다. 두 번째 특성은 최소제곱추정량이 불편추정량(unbiased estimator)이라는 것이다. 불편추정량이란 추정량의 기대값이 추정하려는 모수와 일치하는 추정량을 의미한다. 먼저 \\(\\hat{\\beta}_{1}\\)의 기대값을 구해보자. \\[\\begin{align} E\\left(\\hat{\\beta}_{1}\\right) &amp; = E\\left(\\sum_{i=1}^{n}c_{i}Y_{i}\\right) \\\\ &amp; = \\sum_{i=1}^{n}c_{i}E(Y_{i}) \\\\ &amp; = \\sum_{i=1}^{n}c_{i}E(\\beta_{0}+\\beta_{i}X_{i}+\\varepsilon_{i}) \\\\ &amp; = \\beta_{0}\\sum_{i=1}^{n}c_{i}+\\beta_{1}\\sum_{i=1}^{n}c_{i}X_{i} \\tag{2.8} \\end{align}\\] \\(\\sum c_{i}\\)의 값은 다음과 같이 구할 수 있다. \\[\\begin{equation} \\sum_{i=1}^{n} c_{i} = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)}{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}} = 0 \\tag{2.9} \\end{equation}\\] 또한 \\(\\sum c_{i}X_{i}\\)의 값은 다음과 같이 구할 수 있다. \\[\\begin{align} \\sum_{i=1}^{n}c_{i}X_{i} &amp; = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)X_{i}}{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}} \\\\ &amp; = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)\\left(X_{i}-\\overline{X}\\right)}{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}} \\\\ &amp; = \\frac{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}}{\\sum_{i=1}^{n}\\left(X_{i}-\\overline{X}\\right)^{2}} \\\\ &amp; = 1 \\tag{2.10} \\end{align}\\] 식 (2.8)의 마지막 식에서 식 (2.9)와 식 (2.10)의 결과를 적용하면 다음의 결과를 얻게 된다. \\[\\begin{equation} E\\left(\\hat{\\beta}_{1}\\right) = \\beta_{1} \\tag{2.11} \\end{equation}\\] 이제 \\(\\hat{\\beta}_{0}\\)의 기대값을 유도해서 불편추정량임을 보이자. \\[\\begin{align} E\\left(\\hat{\\beta}_{0}\\right) &amp; = E\\left(\\overline{Y}-\\hat{\\beta}_{1}\\overline{X}\\right) \\\\ &amp; = \\frac{1}{n}\\sum_{i=1}^{n}E(Y_{i})-\\overline{X}E\\left(\\hat{\\beta}_{1}\\right) \\\\ &amp; = \\frac{1}{n}\\sum_{i=1}^{n}(\\beta_{0}+\\beta_{1}X_{i}) - \\overline{X}\\beta_{1} \\\\ &amp; = \\beta_{0}+\\beta_{1}\\overline{X}-\\overline{X}\\beta_{1} \\\\ &amp; = \\beta_{0} \\tag{2.12} \\end{align}\\] 불편추정량인 \\(\\hat{\\beta}_{0}\\)과 \\(\\hat{\\beta}_{1}\\)의 분산을 유도해 보자. 식 (2.7)으로 \\(\\hat{\\beta}_{1}\\)은 \\(\\sum c_{i}Y_{i}\\)로 표시할 수 있는데, \\(Y_{i}\\)가 서로 독립이기 때문에 \\(\\hat{\\beta}_{1}\\)의 분산은 다음과 같이 유도할 수 있다. \\[\\begin{align} Var\\left(\\hat{\\beta}_{1}\\right) &amp; = \\sum_{i=1}^{n}c_{i}^{2}Var(Y_{i}) \\\\ &amp; = \\sigma^{2}\\sum_{i=1}^{n}c_{i}^{2} \\\\ &amp; = \\sigma^{2}\\sum_{i=1}^{n}\\left(\\frac{(X_{i}-\\overline{X})}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right)^{2} \\\\ &amp; = \\sigma^{2}\\frac{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}{\\left(\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}\\right)^{2}} \\\\ &amp; = \\frac{\\sigma^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}} \\tag{2.13} \\end{align}\\] \\(\\hat{\\beta}_{0}\\)의 분산은 다음과 같이 유도할 수 있다. \\[\\begin{align} Var\\left(\\hat{\\beta}_{0}\\right) &amp; = Var\\left(\\overline{Y}-\\hat{\\beta}_{1}\\overline{X}\\right) \\\\ &amp; = Var\\left(\\overline{Y}\\right) + \\overline{X}^{2}Var\\left(\\hat{\\beta}_{1}\\right)-2\\overline{X}Cov\\left(\\overline{Y},\\hat{\\beta}_{1}\\right) \\\\ &amp; = \\frac{\\sigma^{2}}{n} + \\overline{X}^{2}\\frac{\\sigma^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}} \\\\ &amp; = \\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\overline{X}^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right) \\tag{2.14} \\end{align}\\] \\(\\overline{Y}\\)와 \\(\\hat{\\beta}_{1}\\)의 공분산이 0임을 보이는 것은 연습문제로 남겨두겠다. 지금까지 살펴본 최소제곱추정량의 특성과 관련된 중요한 결과가 바로 가우스-마코프(Gauss-Markov) 정리이다. 가우스-마코프 정리는 최소제곱추정량으로 회귀계수를 추정해서 사용할 수 있는 근거를 제시하고 있다. 정리 2.1 (Gauss-Markov 정리) 오차항의 평균이 0, 분산이 \\(\\sigma^{2}\\)이며 서로 독립을 가정한 회귀모형에서 최소제곱추정량은 최량선형불편추정량(Best Linear Unbiased Estimator: BLUE)이다. 즉, 최소제곱추정량은 \\(Y_{i}\\)의 선형결합으로 표시되는 모든 불편추정량 중에 최소 분산을 갖는다. \\(\\bullet\\) 최소제곱추정량으로 추정된 회귀직선의 특성 최소제곱추정량으로 추정된 회귀직선은 다음과 같은 몇 가지 유용한 특성을 가지고 있다. 절편을 포함한 회귀모형에서 잔차의 합은 항상 0이다. 이 특성은 식 (2.3)의 RSS를 \\(\\hat{\\beta}_{0}\\)으로 편미분한 식 (2.4)의 첫 번째 방정식에서 바로 확인할 수 있다. \\[\\begin{equation} \\sum_{i=1}^{n}(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i}) = \\sum_{i=1}^{n}(y_{i}-\\hat{y}_{i}) = \\sum_{i=1}^{n}e_{i} = 0 \\tag{2.15} \\end{equation}\\] 관측값 \\(y_{i}\\)의 합과 추정값 \\(\\hat{y}_{i}\\)의 합은 동일하다. 이 특성은 식 (2.15)에서 확인할 수 있다. 추정된 회귀직선은 항상 표본평균점 \\((\\overline{x}, \\overline{y})\\)을 항상 통과한다. 이 특성은 다음의 수식으로 확인할 수 있다. 마지막 수식에서 \\(x=\\overline{x}\\) 가 되면, \\(\\hat{y}=\\overline{y}\\) 가 되기 때문에 추정된 회귀직선은 \\((\\overline{x}, \\overline{y})\\)을 항상 통과하게 된다. \\[\\begin{align*} \\hat{y} &amp; = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}x \\\\ &amp; = (\\overline{y}-\\hat{\\beta}_{1}\\overline{x}) + \\hat{\\beta}_{1}x \\\\ &amp; = \\overline{y} + \\hat{\\beta}_{1}(x-\\overline{x}) \\end{align*}\\] 잔차의 \\(x_{i}\\)에 대한 가중합은 0이다. 이 특성은 식 (2.3)의 RSS를 \\(\\hat{\\beta}_{1}\\)으로 편미분한 식 (2.4)의 두 번째 방정식에서 바로 확인할 수 있다. \\[\\begin{align*} \\sum_{i=1}^{n}x_{i}(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{i}) &amp; = \\sum_{i=1}^{n}x_{i}y_{i}-\\sum_{i=1}^{n}x_{i}(\\hat{\\beta}_{0}+\\hat{\\beta}_{1}x_{i}) \\\\ &amp; = \\sum_{i=1}^{n}x_{i}y_{i} - \\sum_{i=1}^{n}x_{i}\\hat{y}_{i} \\\\ &amp; = \\sum_{i=1}^{n}x_{i}(y_{i}-\\hat{y}_{i}) \\\\ &amp; = \\sum_{i=1}^{n}x_{i}e_{i} = 0 \\end{align*}\\] 잔차의 \\(\\hat{y}_{i}\\)에 대한 가중합은 0이다. 이 특성은 다음의 수식으로 확인할 수 있다. \\[\\begin{align*} \\sum_{i=1}^{n}\\hat{y}_{i}e_{i} &amp; = \\sum_{i=1}^{n}(\\hat{\\beta}_{0} + \\hat{\\beta}_{1}x_{i})e_{i} \\\\ &amp; = \\hat{\\beta}_{0}\\sum_{i=1}^{n}e_{i} + \\hat{\\beta}_{1}\\sum_{i=1}^{n}x_{i}e_{i} \\\\ &amp; = 0 \\end{align*}\\] 2.3 오차분산 \\(\\sigma^{2}\\)의 추정 회귀계수 \\(\\beta_{0}\\)과 \\(\\beta_{1}\\)의 구간추정 및 가설검정을 실시하는 경우와 설명변수의 주어진 값에 대한 반응변수 값의 예측구간 추정 등을 실시하는 경우에는 오차항의 분산인 \\(\\sigma^{2}\\)의 추정값이 반드시 필요하다. 식 (1.1)의 회귀모형에서 회귀모형 \\(f(X)\\)의 실제 형태는 일반적으로 알려져 있지 않으며, 주어진 자료를 근거로 식 (1.3)와 같은 함수형태를 가정하게 된다. 이 때 가정한 함수형태의 적절성 여부와 관계 없이 오차항의 분산을 정확하게 추정할 수 있다면 가장 이상적인 상황이 될 것이다. 그러나 이것은 하나의 \\(X\\) 변수의 값에 대하여 여러 개의 \\(Y\\) 변수의 값이 있는 경우에만 가능한 상황이며, 현실적으로는 접하기 어려운 상황이 된다. 따라서 일반적으로 사용하는 방법은 잔차의 분산으로 오차항의 분산을 추정하는 것이다. 잔차의 평균은 0이기 때문에 잔차의 분산은 잔차제곱합(residual sum of squares; RSS)으로 표현된다. \\[\\begin{equation} RSS = \\sum_{i=1}^{n}\\left(y_{i}-\\hat{y}_{i}\\right)^{2} \\tag{2.16} \\end{equation}\\] 회귀모형의 추론 과정에는 잔차제곱합과 같은 형태의 몇 가지 제곱합이 사용된다. 이러한 제곱합은 모두 ’자유도’를 갖게 되는데, 각 제곱합의 자유도는 다음 두 가지 방법 중 적용 가능한 방법으로 구할 수 있다. 변수의 합이 주어진 경우: 예를 들어 \\(x_{1}+x_{2}+x_{3}=0\\) 이라는 조건에서 자유롭게 값을 선택할 수 있는 변수의 개수는 2개가 된다. 그것은 만일 \\(x_{1}\\)과 \\(x_{2}\\)의 값이 선택되면 \\(x_{3}\\)의 값은 자동으로 구해지기 때문이다. 표본분산의 경우 적용되는 제곱합의 형태는 \\(\\sum_{i=1}^{n}(y_{i}-\\overline{y})^{2}\\)이 되는데, \\(\\sum_{i=1}^{n}(y_{i}-\\overline{y})=0\\) 이라는 조건이 있기 때문에 자유도는 \\(n-1\\)이 된다. 추정량이 포함된 경우: 자유도는 통계학에서 일종의 화폐와 같은 역할을 한다. 수집되는 각 자료마다 하나의 자유도가 추가되는 반면에, 모수에 대한 추정에 이루어질 때마다 하나의 자유도가 차감되는 것이다. 식 (2.16)의 잔차제곱합에서는 \\(n\\)개의 \\(y_{i}\\)가 수집되지만 \\(\\hat{y}_{i}\\)를 얻기 위해서는 두 모수 \\(\\beta_{0}\\)와 \\(\\beta_{1}\\)에 대한 추정이 이루어져 2개의 자유도를 잃게 된다. 따라서 잔차제곱합의 자유도는 \\(n-2\\)가 된다. 식 (2.16)의 잔차제곱합 \\(RSS\\)의 기대값은 \\(E(RSS)=(n-2)\\sigma^{2}\\)가 되는데, 이것은 \\(RSS/\\sigma^{2}\\) 가 자유도가 \\(n-2\\) 인 카이제곱 분포를 하고 있으며, 카이제곱 분포를 하는 확률변수의 기대값은 자유도가 된다는 결과에 의해서 \\(E(RSS/\\sigma^{2})=n-2\\) 가 되기 때문이다. 따라서 오차항의 분산인 \\(\\sigma^{2}\\)의 불편추정량은 다음과 같이 정의된다. \\[\\begin{equation} \\hat{\\sigma}^{2} = \\frac{RSS}{n-2} = MSE \\tag{2.17} \\end{equation}\\] \\(\\sigma^{2}\\) 의 추정량인 \\(MSE\\)는 잔차제곱평균이라고 하며, \\(\\sigma\\) 의 추정량인 \\(\\sqrt{MSE}\\)는 회귀 표준오차(standard error of regression)라고 한다. 추정량 \\(MSE\\)는 잔차를 기반으로 오차항의 분산을 추정하는 것이기 때문에 식 (1.3)에서 가정한 함수 형태가 적절해야 하는 것은 필수적이다. 또한 오차항의 가정 사항도 모두 만족되어야 \\(\\sigma^{2}\\)의 좋은 추정량이 될 수 있다. 만일 이러한 가정 사항이 만족되지 않는다면, \\(\\sigma^{2}\\)의 추정량으로서 \\(MSE\\)의 유용성은 매우 떨어진다고 할 수 있다. \\(\\bullet\\) 예제 2.3: 매출액에 대한 광고효과 분석 예제 2.2의 자료 ex2-2.csv에 대하여 다음의 분석을 실시해 보자. 예제 2.2에서 구한 회귀직선으로 주어진 광고비 지출자료에 대한 평균적인 매출액을 추정하고, 잔차를 구하자. \\(\\sigma\\) 의 추정값인 회귀 표준오차를 구해 보자. 예제 2.2에서 이루어진 분석을 다시 실행해 보자. df2.2 &lt;- readr::read_csv(&quot;Data/ex2-2.csv&quot;) fit2.2 &lt;- lm(sales ~ advertisement, data = df2.2) 함수 lm()으로 생성된 리스트 객체 fit2.2에는 \"fitted.values\"와 \"residuals\"라는 이름으로 주어진 설명변수 값에 대한 반응변수의 추정값과 잔차가 입력되어 있다. cbind(df2.2, Yhat = fit2.2$fitted, resid = fit2.2$resid) ## sales advertisement Yhat resid ## 1 425 23 400.4524 24.547619 ## 2 370 21 365.0266 4.973389 ## 3 200 16 276.4622 -76.462185 ## 4 580 34 595.2941 -15.294118 ## 5 620 32 559.8683 60.131653 ## 6 650 36 630.7199 19.280112 ## 7 700 40 701.5714 -1.571429 ## 8 490 37 648.4328 -158.432773 ## 9 610 35 613.0070 -3.007003 ## 10 290 20 347.3137 -57.313725 ## 11 320 20 347.3137 -27.313725 ## 12 350 21 365.0266 -15.026611 ## 13 400 23 400.4524 -0.452381 ## 14 517 21 365.0266 151.973389 ## 15 545 30 524.4426 20.557423 ## 16 590 32 559.8683 30.131653 ## 17 711 39 683.8585 27.141457 ## 18 650 37 648.4328 1.567227 ## 19 740 41 719.2843 20.715686 ## 20 660 38 666.1457 -6.145658 회귀모형의 다양한 추정 결과는 함수 lm()으로 생성된 객체를 함수 summary()에 입력하면 얻을 수 있다. 자세한 사용 방법은 3장에서 살펴볼 것이지만, 여기에서는 함수 summary()의 결과 중 \\(\\sigma\\) 의 추정값만을 출력해 보자. summary(fit2.2)$sigma ## [1] 60.41388 "],["simple-reg-infer.html", "3 장 단순선형회귀모형의 추론 3.1 회귀계수 \\(\\beta_{1}\\)에 대한 추론 3.2 회귀계수 \\(\\beta_{0}\\)에 대한 추론 3.3 반응변수의 평균, \\(E(Y|X_{o})\\)에 대한 신뢰구간 추정 3.4 반응변수의 개별 관측값 예측 3.5 상관계수 : 두 변수 사이의 선형 연관성 측정", " 3 장 단순선형회귀모형의 추론 회귀모형의 추론에서 우리가 살펴볼 내용은 회귀계수에 대한 신뢰구간 추정 및 검정, 그리고 설명변수의 값이 주어졌을 때 반응변수의 조건부 평균에 대한 신뢰구간 추정 등이다. 이번 장에서 살펴볼 회귀모형은 다음의 단순회귀모형이다. \\[\\begin{equation} Y_{i} = \\beta_{0} + \\beta_{1}X_{i} + \\varepsilon_{i}, ~~i = 1, \\ldots, n \\tag{3.1} \\end{equation}\\] 2장에서 회귀계수를 최소제곱추정량으로 추정하기 위해서 필요한 가정은 다음과 같다. \\[\\begin{equation} \\varepsilon_{1}, \\ldots, \\varepsilon_{n} \\text{은 서로 독립이고}~E(\\varepsilon_{i})=0,~Var(\\varepsilon_{i})=\\sigma^{2} \\end{equation}\\] 회귀모형의 추론을 위해서는 위 가정에 정규분포 가정이 추가된 다음의 가정이 필요하게 된다. \\[\\begin{equation} \\varepsilon_{1}, \\ldots, \\varepsilon_{n} \\stackrel{iid}{\\sim} N(0,\\sigma^{2}) \\tag{3.2} \\end{equation}\\] 3.1 회귀계수 \\(\\beta_{1}\\)에 대한 추론 식 (3.1)에서 회귀계수 \\(\\beta_{1}\\)은 회귀직선의 기울기를 나타내는 모수로서, 설명변수 \\(X\\)를 한 단위 증가시켰을 때 반응변수 \\(Y\\)의 평균 변화량을 나타낸다. 회귀계수 \\(\\beta_{1}\\)의 점추정량은 2장에서 살펴본 최소제곱추정량으로 구할 수 있었는데, 이번 절에서는 \\(\\beta_{1}\\)의 신뢰구간 추정과 가설검정에 대해 살펴보고자 한다. 식 (3.1)으로 설정된 회귀모형이 주어진 자료를 적절하게 설명하고 있는지 여부는 점추정 결과만으로는 알 수 없다. 추정 결과의 정확성 또는 적절성 등에 대한 접근은 관련된 회귀계수의 신뢰구간 추정을 통해 확인할 수 있다. 또한 주어진 자료를 근거로 가설 \\(H_{0}:\\beta_{1}=0\\) 에 대한 검정이 필요한데, 만일 가설을 기각할 수 없다면, 반응변수 \\(Y\\)와 설명변수 \\(X\\)의 관계가 식 (3.1)에서 설정한 선형관계로는 설명하기 어렵다는 것을 의미하기 때문이다. 이러한 추론을 실시하기 위해서는 회귀계수 \\(\\beta_{1}\\)의 점추정량인 \\(\\hat{\\beta}_{1}\\)의 표본분포를 반드시 알고 있어야 한다. 3.1.1 \\(\\hat{\\beta}_{1}\\)의 표본분포 식 (3.1)의 회귀모형에 식 (3.2)에 주어진 오차항의 가정 사항을 적용시키면 설명변수 \\(X_{i}\\)가 주어졌을 때 반응변수 \\(Y_{i}\\)에 대한 분포는 다음과 같이 주어진다. \\[\\begin{equation} Y_{i} \\stackrel{iid}{\\sim} N(\\beta_{0}+\\beta_{1}X_{i}, \\sigma^{2}),~~i=1, \\ldots, n \\end{equation}\\] 2장에서 유도한 회귀계수 \\(\\beta_{1}\\)의 최소제곱추정량인 \\(\\hat{\\beta}_{1}\\)은 식 (2.7)에서 다음과 같이 반응변수 \\(Y_{i}\\)의 선형결합으로 표현됨을 보였다. \\[\\begin{equation} \\hat{\\beta}_{1} = \\sum_{i=1}^{n} c_{i}Y_{i},~~c_{i}= \\frac{X_{i}-\\overline{X}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}} \\end{equation}\\] 즉, \\(\\hat{\\beta}_{1}\\)은 정규분포를 하는 확률변수인 \\(Y_{i}\\)의 선형결합으로 표현되기 때문에 \\(\\hat{\\beta}_{1}\\)의 분포도 정규분포를 따르게 되는 것이다. 또한 식 (2.11)와 식 (2.13)에서 유도된 \\(\\hat{\\beta}_{1}\\)의 평균과 분산을 적용시키면 \\(\\hat{\\beta}_{1}\\)의 표본분포는 다음과 같이 주어진다. \\[\\begin{equation} \\hat{\\beta}_{1} \\sim N \\left( \\beta_{1}, \\frac{\\sigma^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}} \\right) \\tag{3.3} \\end{equation}\\] 3.1.2 \\(\\beta_{1}\\)의 신뢰구간 모수 \\(\\beta_{1}\\)에 대한 신뢰구간은 하나의 숫자보다는 모수를 포함할 구간을 제시하는 것이며, 구간의 폭은 추정량의 표준오차에 비례하여 결정된다. 따라서 주어진 자료가 설정된 회귀모형에 의하여 잘 설명된다면 신뢰구간의 폭은 상대적으로 작은 값을 갖게 된다. 모수 \\(\\beta_{1}\\)의 신뢰구간은 모수의 불편추정량인 \\(\\hat{\\beta}_{1}\\)의 표본분포를 활용해서 유도되는데, 우선 식 (3.3)에서 다음의 결과를 얻을 수 있다. \\[\\begin{equation} Z = \\frac{\\hat{\\beta}_{1}-\\beta_{1}}{\\sqrt{\\sigma^{2}/\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}} \\sim N(0,1) \\end{equation}\\] 식 (2.17)에서 \\(\\sigma^{2}\\)의 불편추정량은 \\(MSE\\)임이 확인되었고, 이제 \\(\\sigma^{2}\\)를 \\(MSE\\)로 대체하면 다음의 결과를 얻게 된다. \\[\\begin{equation} t = \\frac{\\hat{\\beta}_{1}-\\beta_{1}}{\\sqrt{MSE/\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}} \\sim t(n-2) \\tag{3.4} \\end{equation}\\] 단, \\(t(n-2)\\)는 자유도가 \\(n-2\\)인 \\(t\\) 분포를 의미하는 것인데, 통계량 \\(t\\)의 자유도는 분모에 사용된 \\(MSE\\)의 자유도가 그대로 적용된다. 모수 \\(\\beta_{1}\\)의 \\(100 \\times (1-\\alpha)\\)% 신뢰구간은 식 (3.4)을 이용해서 다음과 같이 유도된다. \\[\\begin{align} 1-\\alpha &amp; = P \\left( -t_{\\alpha, n-2} &lt; \\frac{\\hat{\\beta}_{1}-\\beta_{1}}{SE(\\hat{\\beta}_{1})} &lt; t_{\\alpha, n-2} \\right) \\\\ &amp; = P \\left( \\hat{\\beta}_{1} - t_{\\alpha, n-2} \\cdot SE(\\hat{\\beta}_{1}) &lt; \\beta_{1} &lt; \\hat{\\beta}_{1} + t_{\\alpha, n-2} \\cdot SE(\\hat{\\beta}_{1}) \\right) \\tag{3.5} \\end{align}\\] 단, \\(SE(\\hat{\\beta}_{1})=\\sqrt{MSE/\\sum(X_{i}-\\overline{X})^{2}}\\) 를 나타내고, \\(t_{\\alpha, n-2}\\) 는 자유도가 \\(n-2\\)인 \\(t\\) 분포에서 상위 \\(100 \\times \\alpha\\) 백분위수를 나타낸다. 모수의 신뢰구간은 점추정량의 표본분포를 근거로 유도된 것이기 때문에, 식 (3.5)의 신뢰구간도 표본분포의 개념을 이용해서 해석해야 한다. 즉, 자료에 주어진 동일한 \\(X\\) 값에 대해서 표본을 반복해서 추출하고, 추출된 각 표본마다 식 (3.5)에 의해 95% 신뢰구간을 계산한다면, 계산된 전체 신뢰구간 중 대략 95%의 신뢰구간에는 미지의 모수인 \\(\\beta_{1}\\)이 포함되지만 나머지 5%의 신뢰구간에는 \\(\\beta_{1}\\)이 포함되지 않을 수 있다는 것이다. 따라서 실제 상황에서 우리가 얻게 되는 하나의 표본을 대상으로 계산된 신뢰구간에는 \\(\\beta_{1}\\)이 포힘되어 있는지 여부를 알 수 없는 것이다. 3.1.3 \\(\\beta_{1}\\)에 대한 가설검정 기울기에 대한 가설검정은 \\(\\beta_{1}\\) 이 특정한 상수인 \\(\\beta_{1o}\\) 와 같은 값을 갖는지 여부에 대한 것이다. \\[\\begin{equation} H_{0}:\\beta_{1} = \\beta_{1o}, ~~~H_{1}:\\beta_{1} \\ne \\beta_{1o} \\tag{3.6} \\end{equation}\\] 만일 \\(\\sigma^{2}\\) 가 알려져 있다면, 검정통계량은 다음과 같이 주어지며, \\[\\begin{equation} Z= \\frac{\\hat{\\beta}_{1}-\\beta_{1o}}{\\sqrt{\\sigma^{2}/\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}} \\tag{3.7} \\end{equation}\\] \\(H_{0}\\)이 사실인 경우, 분포는 \\(N(0,1)\\) 이 된다. 그러나 대부분의 상황에서는 \\(\\sigma^{2}\\) 의 값을 알기 어렵고, 따라서 \\(MSE\\) 로 추정한 결과를 대신 사용하게 된다. 이 경우에 검정통계량은 다음과 같이 주어지며, \\[\\begin{equation} t = \\frac{\\hat{\\beta}_{1}-\\beta_{1o}}{SE(\\hat{\\beta}_{1})} \\tag{3.8} \\end{equation}\\] 단, \\(SE(\\hat{\\beta}_{1}) = \\sqrt{MSE/\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\). \\(H_{0}\\)이 사실인 경우, 검정통계량의 분포는 \\(t(n-2)\\) 가 된다. 식 (3.6)의 가설 중 가장 중요한 경우는 \\(\\beta_{1o}=0\\) 이 되며, 이것을 회귀계수 \\(\\beta_{1}\\)의 유의성 검정이라고 한다. \\[\\begin{equation} H_{0}:\\beta_{1}=0,~~~H_{1}:\\beta_{1} \\ne 0 \\tag{3.9} \\end{equation}\\] \\(\\beta_{1}\\)의 유의성 검정에서 사용되는 검정통계량은 식 (3.8)에서 \\(\\beta_{1o}\\) 를 0으로 두는 형태가 된다. \\[\\begin{equation} t = \\frac{\\hat{\\beta}_{1}}{SE(\\hat{\\beta}_{1})} \\tag{3.10} \\end{equation}\\] 주어진 자료를 근거로 만일 \\(H_{0}\\) 을 기각할 수 없다면, 식 (3.1)에서 설정된 모형이 \\(Y_{i}=\\beta_{0}+\\varepsilon_{i}\\) 로 변경되는 것이며, 따라서 설명변수 \\(X\\)가 반응변수 \\(Y\\)의 변동을 설명하는데 아무런 기여를 할 수 없다는 것을 의미하게 된다. 하지만 이것은 두 변수의 실제 관계가 선형이 아닌 경우에는 틀린 결론이 될 수 있다. 예시 자료를 활용해서 자세히 살펴보자. 그림 3.1은 두 변수 사이에 관계가 없는 경우에 대한 예시로서, 설명변수가 반응변수의 변동을 설명하는데 어떤 역할도 할 수 없는 상황으로 보인다. 파란 직선은 추정된 회귀직선을 나타내고 있는데, \\(H_{0}:\\beta_{1}=0\\) 을 기각하기 어려운 상황으로 보인다. 실제 예시 자료를 근거로 가설 \\(H_{0}:\\beta_{1}=0, ~~H_{1}:\\beta_{1} \\ne 0\\) 에 대한 검정을 실시한 결과 p-값은 0.859으로 계산되어 \\(H_{0}\\)은 기각할 수 없게 된다. 그림 3.1: \\(H_{0}:\\beta_{1}=0\\)을 기각할 수 없는 상황: 두 변수 사이에 관계가 없는 경우 그림 3.2는 두 변수의 관계가 선형이 아닌 2차 함수의 관계에 있는 경우에 대한 예시 자료이다. 추정된 기울기는 0에 가까운 값을 갖고 있고, 가설 \\(H_{0}:\\beta_{1}=0, ~~H_{1}:\\beta_{1} \\ne 0\\) 에 대한 검정 결과 p-값은 0.839으로 계산되어 \\(H_{0}\\) 을 기각하기 어려운 상황으로 보이지만, 두 변수 사이에는 명확한 관계가 있는 것을 알 수 있다. 그림 3.2: \\(H_{0}:\\beta_{1}=0\\)을 기각할 수 없는 상황: 두 변수 사이에 2차 함수 관계가 있는 경우 그림 3.3도 두 변수의 관계가 선형이 아닌 2차 함수의 관계에 있는 경우에 대한 예시 자료이다. 이 경우에는 기울기의 추정 결과가 0.376으로 나오며, 검정 결과에서도 p-값이 0.0112로 계산되어서 유의수준 \\(\\alpha = 0.05\\) 에서 \\(H_{0}\\)을 기각할 수 있는 자료이다. 하지만 \\(X^{2}\\)을 포함시킨 회구모형 \\(Y=\\beta_{0}+\\beta_{1}X+\\beta_{2}X^{2}+\\varepsilon\\) 이 더 적절한 모형으로 보인다. 그림 3.3: \\(H_{0}:\\beta_{1}=0\\)을 기각할 수 있지만, \\(X^{2}\\)을 포함한 2차 회귀모형이 더 좋은 결과를 보이는 경우 3.2 회귀계수 \\(\\beta_{0}\\)에 대한 추론 회귀계수 \\(\\beta_{0}\\) 는 회귀직선의 Y축 절편을 나타내는 모수로서, 설명변수 \\(X\\) 의 값이 0일 때 반응변수 \\(Y\\)의 평균값을 나타낸다. 따라서 설명변수가 갖는 자료의 범위에 0이 포함되지 않는 경우에는 추론의 필요성이 크지 않은 모수가 된다. 또한 가설 \\(H_{0}:\\beta_{0}=0\\) 을 기각할 수 없는 경우에도 일반적으로는 모형에서 \\(\\beta_{0}\\) 를 제외시키지 않는다. 3.2.1 \\(\\hat{\\beta}_{0}\\) 의 표본분포 2장에서 살펴본 \\(\\beta_{0}\\) 의 최소제곱추정량은 식 (2.6)에서 다음과 같이 유도되었다. \\[\\begin{equation} \\hat{\\beta}_{0} = \\overline{Y} - \\hat{\\beta}_{1} \\overline{X} \\end{equation}\\] \\(\\hat{\\beta}_{1}\\) 이 식 (2.7)에서 \\(Y_{i}\\) 의 선형결합으로 표현됨을 알 수 있기 때문에 \\(\\hat{\\beta}_{0}\\) 도 자연스럽게 \\(Y_{i}\\) 의 선형결합으로 표현됨을 알 수 있다. 따라서 \\(\\hat{\\beta}_{0}\\) 도 \\(Y_{i}\\) 와 같은 분포인 정규분포를 따르게 된다. \\(\\hat{\\beta}_{0}\\) 의 평균과 분산은 식 (2.12)과 식 (2.14)에서 다음과 같이 유도되었다. \\[\\begin{align*} E\\left(\\hat{\\beta}_{0}\\right) &amp;= \\beta_{0} \\\\ Var\\left(\\hat{\\beta}_{0}\\right) &amp;= \\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\overline{X}^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right) \\end{align*}\\] 따라서 \\(\\hat{\\beta}_{0}\\) 의 표본분포는 다음과 같다. \\[\\begin{equation} \\hat{\\beta}_{0} \\sim N\\left(\\beta_{0},~ \\sigma^{2}\\left(\\frac{1}{n}+\\frac{\\overline{X}^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right) \\right) \\end{equation}\\] 3.2.2 \\(\\beta_{0}\\)의 신뢰구간 모수 \\(\\beta_{0}\\) 의 신뢰구간은 3.1.2절에서 \\(\\beta_{1}\\)의 신뢰구간을 유도할 때 사용했던 것과 동일한 방식으로 유도할 수 있다. 우선 \\(\\beta_{0}\\) 의 불편추정량인 \\(\\hat{\\beta}_{0}\\) 을 표준화시킨 통계량 \\(t\\) 는 다음과 같이 표현되며, 분포는 \\(\\hat{\\beta}_{1}\\) 의 경우와 동일하게 \\(t(n-2)\\)가 된다. \\[\\begin{equation} t = \\frac{\\hat{\\beta}_{0}-\\beta_{0}}{\\sqrt{MSE\\left(\\frac{1}{n}+\\frac{\\overline{X}^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right)}} \\sim t(n-2) \\tag{3.11} \\end{equation}\\] 모수 \\(\\beta_{0}\\) 의 \\(100 \\times (1-\\alpha)\\)% 신뢰구간은 식 (3.11)을 이용하면 다음과 같이 유도된다. \\[\\begin{align} 1-\\alpha &amp; = P \\left( -t_{\\alpha, n-2} &lt; \\frac{\\hat{\\beta}_{0}-\\beta_{0}}{SE(\\hat{\\beta}_{0})} &lt; t_{\\alpha, n-2} \\right) \\\\ &amp; = P \\left( \\hat{\\beta}_{0} - t_{\\alpha, n-2} \\cdot SE(\\hat{\\beta}_{0}) &lt; \\beta_{0} &lt; \\hat{\\beta}_{0} + t_{\\alpha, n-2} \\cdot SE(\\hat{\\beta}_{0}) \\right) \\tag{3.12} \\end{align}\\] 단, \\(SE(\\hat{\\beta}_{0})=\\sqrt{MSE\\left(\\frac{1}{n}+\\frac{\\overline{X}^{2}}{\\sum(X_{i}-\\overline{X})^{2}}\\right)}\\) 을 나타내며, \\(t_{\\alpha, n-2}\\) 는 자유도가 \\(n-2\\)인 \\(t\\) 분포에서 상위 \\(100 \\times \\alpha\\) 백분위수를 나타낸다. 3.2.3 \\(\\beta_{0}\\)에 대한 가설검정 모수 \\(\\beta_{0}\\) 에 대한 가설검정도 3.1.3절에서 살펴본 \\(\\beta_{1}\\) 의 가설검정의 경우와 동일한 방식으로 진행할 수 있다. 다음의 가설에 대하여 \\[\\begin{equation} H_{0}: \\beta_{0} = 0, ~~H_{1}: \\beta_{0} \\ne 0 \\end{equation}\\] 검정정통계량은 식 (3.11)을 근거로 다음과 같이 주어진다. \\[\\begin{equation} t = \\frac{\\hat{\\beta}_{0}}{SE(\\hat{\\beta}_{0})} \\tag{3.13} \\end{equation}\\] 단, \\(SE(\\hat{\\beta}_{0})=\\sqrt{MSE\\left(\\frac{1}{n}+\\frac{\\overline{X}^{2}}{\\sum(X_{i}-\\overline{X})^{2}}\\right)}\\). \\(H_{0}\\) 이 사실일 때 검정통계량은 분포는 \\(t(n-2)\\)가 된다. 절차는 \\(\\beta_{1}\\) 의 경우와 동일하지만, 검정 결과의 적용 방식에는 큰 차이가 있다. \\(\\beta_{0}\\) 는 설명변수 \\(X\\) 의 값이 0일 때 반응변수 \\(Y\\)의 평균값을 나타내기 때문에, 설명변수가 갖는 자료의 범위에 0이 포함되지 않는 경우에는 추론의 필요성이 크지 않은 모수가 된다. 또한 가설 \\(H_{0}:\\beta_{0}=0\\) 을 기각할 수 없는 경우에도 모형에서 \\(\\beta_{0}\\) 를 제외시키지 않기 때문에, \\(\\beta_{0}\\) 에 대한 가설검정은 일반적으로 무시된다고 할 수 있다. 만일 가설 \\(H_{0}:\\beta_{0}=0\\) 의 검정 결과에 따라 모형에서 \\(\\beta_{0}\\) 를 제외시키게 되면, 회귀직선은 반드시 원점을 지나가야 하는데, 이것은 지나치게 강력한 제한사항이라고 할 수 있다. 예시 자료를 이용해서 이 문제를 살펴보자. 주어진 자료에 대해 식 (3.1)에 설정된 절편이 포함된 일반적인 단순회귀모형으로 추정된 회귀직선과 원점을 제거한 회귀모형인 \\(Y=\\beta_{1}X+\\varepsilon\\) 으로 추정된 회귀직선이 그림 3.4에 표시되어 있다. 주어진 자료에서 가설 \\(H_{0}:\\beta_{0}=0, ~~H_{1}:\\beta_{0} \\ne 0\\) 에 대한 p-값은 0.248으로 계산되어 \\(H_{0}\\) 을 기각할 수 없는 상황이다. 두 회귀직선 사이에는 무시할 수 없는 차이가 있는 것으로 보이는데, 절편이 포함된 회귀직선의 경우에는 잔차제곱합이 \\(RSS=\\) 118.98인 반면에, 절편이 제거되어 원점을 지나는 회귀직선의 경우에는 잔차제곱합이 \\(RSS=\\) 124.89로 계산된다. 따라서 가설검정의 결과에 따라 절편을 제거한 회귀모형의 적합도가 더 떨어지는 것을 볼 수 있다. 그림 3.4: 절편을 제거한 회귀모형의 한계 \\(\\bullet\\) 예제 3.1: 매출액에 대한 광고효과 회귀모형의 회귀계수에 대한 추론 예제 2.2의 자료 ex2-2.csv에 대하여 다음의 분석을 실시해 보자. 광고비 지출이 매출액 증가에 양의 효과가 있는지 검정하라. 기울기 모수인 \\(\\beta_{1}\\)에 대한 95% 신뢰구간을 추정하라. 회귀모형의 적합은 함수 lm()으로 이루어진다. 적합된 회귀모형에 대한 추론을 위해서는 lm()으로 생성된 객체에 다른 함수를 적용시켜 생성된 객체를 대상으로 이루어진다. 추론을 위해 사용되는 가장 대표적인 함수는 summary()이다. 자료 ex2-2.csv를 대상으로 회귀모형을 적합시켜보자. df2.2 &lt;- readr::read_csv(&quot;Data/ex2-2.csv&quot;) fit2.2 &lt;- lm(sales ~ advertisement, data = df2.2) 함수 lm()으로 생성된 객체 fit2.2에 회귀모형의 추론 단계에서 가장 빈번하게 사용되는 함수 summary()를 적용시킨 결과를 확인해 보자. summary(fit2.2) ## ## Call: ## lm(formula = sales ~ advertisement, data = df2.2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -158.433 -15.093 0.557 21.674 151.973 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -6.944 52.004 -0.134 0.895 ## advertisement 17.713 1.685 10.511 4.13e-09 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 60.41 on 18 degrees of freedom ## Multiple R-squared: 0.8599, Adjusted R-squared: 0.8521 ## F-statistic: 110.5 on 1 and 18 DF, p-value: 4.126e-09 함수 summary()의 결과물은 SAS나 SPSS에서 볼 수 있는 결과물과는 형식에서 차이가 있으나, 많은 정보를 매우 효율적으로 보여주는 방식이라고 하겠다. 결과물을 하나씩 살펴보자. 먼저 Residuals:에는 잔차의 분포 형태를 엿볼 수 있는 요약통계가 계산되어 있다. 가정이 만족된다면 평균이 0인 정규분포를 보이게 되는데, 잔차의 요약통계 결과로 대략적인 판단을 할 수 있다. 잔차에 대한 분석은 5장에서 살펴볼 것이다. Coefficients:에는 회귀계수의 추정값이 Estimate에, 표준오차가 Std. Error에 각각 계산되어 있다. 또한 개별 회귀계수의 유의성 검정인 \\(H_{0}:\\beta_{j} = 0, ~~H_{1}: \\beta_{j} \\ne 0\\) 에 대한 검정통계량의 값이 t value에, p-값이 Pr(&gt;|t|)이 각각 계산되어 있다. Residual standard error:는 식 (2.17)에서 정의된 \\(MSE\\) 에 제곱근을 취한 값으로 \\(\\sigma\\) 에 대한 추정 결과이며, 회귀모형의 평가 측도로 사용된다. Multiple R-squared:와 Adjusted R-squared:, 그리고 F-statistic:의 내용은 추후에 살펴보겠다. 이제 예제 문제를 해결해 보자. 광고비 지출이 매출액 증가에 양의 효과가 있는지 여부를 확인하는 것은 식 (3.9)와는 다르게 다음과 같은 단측검정이 된다. \\[\\begin{equation} H_{0}:\\beta_{1}=0, ~~H_{1}:\\beta_{1}&gt;0 \\tag{3.14} \\end{equation}\\] 광고비 지출이 매출액 증가에는 음의 효과가 있을 수 없다는 확신이 있는 경우에만 적용할 수 있는 가설이며, 가설은 반드시 자료를 조사하기 전에 설정되어야 한다. 함수 summary()에서 계산한 p-값은 식 (3.9)의 양측검정에 대한 것으로서, \\(P(t &gt; |t_{0}|)\\) 을 계산한 것이다. 여기에서 \\(t\\) 는 자유도가 \\((n-2)\\) 인 \\(t\\) 분포이고 \\(|t_{0}|\\) 는 자료에서 계산된 검정통계량 값을 나타낸다. 식 (3.14)의 단측검정에 대한 p-값은 \\(P(t&gt;t_{0})\\)가 되는데, 예제 자료에서는 \\(t_{0}=\\) 17.713으로 양수이므로 함수 summary()에서 계산한 p-값을 2로 나누면 된다. 따라서 양의 효과가 있는지 여부를 확인하는 가설의 p-값은 2.063186e-09 이 되어서, \\(H_{0}\\) 을 기각하는데 문제가 없어 보인다. 즉, 광고비 지출이 매출액 증가에 양의 효과가 있는 것으로 볼 수 있다. 회귀계수에 대한 신뢰구간은 함수 lm()으로 생성된 객체를 함수 confint()에 적용시키면 얻을 수 있다. 함수 confint()의 기본적인 사용법은 confint(object, level = 0.95)인데, object는 함수 lm()으로 생성된 객체이고, level은 신뢰수준을 지정하는 것으로서 95%가 디폴트 값이다. 이제 광고비 자료에 대한 회귀모형 객체인 fit2.2를 함수 confint()에 적용시킨 결과를 살펴보자. confint(fit2.2) ## 2.5 % 97.5 % ## (Intercept) -116.20070 102.31274 ## advertisement 14.17241 21.25336 만일 90% 신뢰구간을 계산하고자 한다면, level = 0.9를 추가하면 된다. confint(fit2.2, level = 0.9) ## 5 % 95 % ## (Intercept) -97.12253 83.23457 ## advertisement 14.79064 20.63513 3.3 반응변수의 평균, \\(E(Y|X_{o})\\)에 대한 신뢰구간 추정 새롭게 관측되는 설명변수 자료에 대한 가장 정확한 반응변수의 예측값을 생성하는 것이 회귀분석의 목적이 되는 경우가 많이 있다. 반응변수의 예측값은 대부분의 경우 반응변수의 평균을 의미하는데, 경우에 따라서는 반응변수의 개별 관측값을 예측하고자 하는 경우도 있을 수 있다. 반응변수의 평균 예측에 대한 문제는 이번 절에서 살펴볼 것이고, 반응변수의 개별 관측값 예측에 대한 문제는 다음 절에서 살펴볼 것이다. 식 (3.1)의 회귀모형에 의하면, 반응변수 \\(Y\\) 의 평균값은 설명변수의 값에 따라 다른 값을 갖게 된다. 따라서 반응변수의 평균을 예측하고자 하는 설명변수의 수준을 먼저 지정해야 하는데, 이 값을 \\(X_{o}\\) 라고 하자. 그러면 지정된 설명변수 수준에 대한 반응변수의 평균값은 \\(E(Y|X_{o})\\) 가 되는데, 이 때 설명변수의 수준 \\(X_{o}\\) 는 현재 분석에 사용된 자료의 범위 내에서 지정하는 것이 안전하다. 그 이유는 그림 1.5에서 살펴본 회귀모형의 유의성 문제로 인하여, 현재 수집된 자료 범위를 벗어난 영역에 대해서는 가급적 예측을 실시하지 않는 것이 안전하기 때문이다. \\(E(Y|X_{o})=\\beta_{0}+\\beta_{1}X_{o}\\) 에 대한 불편추정량은 \\(\\beta_{0}\\) 와 \\(\\beta_{1}\\) 에 대한 불편추정량을 각각 적용시켜서 다음과 같이 주어진다. \\[\\begin{equation} \\widehat{E(Y|X_{o})} \\equiv \\widehat{Y}_{o} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_{o} \\tag{3.15} \\end{equation}\\] \\(E(Y|X_{o})\\) 에 대한 신뢰구간을 추정하기 위해서는 \\(E(Y|X_{o})\\) 의 불편추정량인 \\(\\widehat{Y}_{o}\\) 의 표본분포를 유도해야 한다. \\(\\widehat{Y}_{o}\\) 은 \\(\\hat{\\beta}_{0}\\) 와 \\(\\hat{\\beta}_{1}\\) 의 선형결합으로 이루어져 있는데, \\(\\hat{\\beta}_{0}\\) 와 \\(\\hat{\\beta}_{1}\\) 은 2.2.2절에서 살펴보았지만 모두 \\(Y_{i}\\) 의 선형결합으로 표현된다. 따라서 \\(\\widehat{Y}_{o}\\) 은 \\(Y_{i}\\) 와 동일하게 정규분포를 따르게 된다. 또한 \\(\\widehat{Y}_{o}\\) 의 분산은 다음과 같이 유도할 수 있다. \\[\\begin{align} Var(\\widehat{Y}_{o}) &amp;= Var(\\hat{\\beta}_{0}+\\hat{\\beta}_{1}X_{o}) \\\\ &amp;= Var(\\overline{Y}-\\hat{\\beta}_{1}\\overline{X}+\\hat{\\beta}_{1}X_{O}) \\\\ &amp;= Var(\\overline{Y}+\\hat{\\beta}_{1}(X_{o}-\\overline{X})) \\\\ &amp;= \\frac{\\sigma^{2}}{n}+\\frac{(X_{o}-\\overline{X})^{2}\\cdot \\sigma^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}} \\\\ &amp;= \\sigma^{2}\\left(\\frac{1}{n}+\\frac{(X_{o}-\\overline{X})^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right) \\tag{3.16} \\end{align}\\] 식 (3.16)의 세 번째에서 네 번째 단계로 넘어오는 과정에는 \\(Cov(\\overline{Y}, \\hat{\\beta}_{1})=0\\) 이 필요한데, 이것의 증명은 2장 연습문제로 주어졌다. 따라서 \\(\\widehat{Y}_{o}\\) 의 표본분포는 다음과 같다. \\[\\begin{equation} \\widehat{Y}_{o} \\sim N\\left(\\beta_{0}+\\beta_{1}X_{o},~\\sigma^{2}\\left(\\frac{1}{n}+\\frac{(X_{o}-\\overline{X})^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right) \\right) \\end{equation}\\] \\(E(Y|X_{o})\\) 의 불편추정량인 \\(\\widehat{Y}_{o}\\) 을 표준화시킨 통계량 \\(t\\) 는 다음과 같이 표현되며, 분포는 \\(\\hat{\\beta}_{1}\\)의 경우와 동일하게 \\(t(n-2)\\) 가 된다. \\[\\begin{equation} t = \\frac{\\widehat{Y}_{o}-E(Y|X_{o})}{\\sqrt{MSE\\left(\\frac{1}{n}+\\frac{(X_{o}-\\overline{X})^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right)}} \\tag{3.17} \\end{equation}\\] 모수 \\(E(Y|X_{o})\\) 의 \\(100 \\times (1-\\alpha)\\)% 신뢰구간은 식 (3.17)을 이용하면 다음과 같이 유도된다. \\[\\begin{align} 1-\\alpha &amp;= P\\left(-t_{\\alpha, n-2}&lt;\\frac{\\widehat{Y}_{o}-E(Y|X_{o})}{SE(\\widehat{Y}_{o})}&lt;t_{\\alpha, n-2} \\right) \\\\ &amp;= P\\left(\\widehat{Y}_{o}-t_{\\alpha, n-2}\\cdot SE(\\widehat{Y}_{o}) &lt; E(Y|X_{o}) &lt; \\widehat{Y}_{o}+t_{\\alpha, n-2}\\cdot SE(\\widehat{Y}_{o})\\right) \\tag{3.18} \\end{align}\\] 단, \\(SE(\\widehat{Y}_{o})) = \\sqrt{MSE\\left(\\frac{1}{n}+\\frac{(X_{o}-\\overline{X})^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right)}\\) 을 나타내며, \\(t_{\\alpha, n-2}\\) 는 자유도가 \\((n-2)\\) 인 \\(t\\) 분포의 상위 \\(100 \\times \\alpha\\) 백분위수를 나타낸다. 식 (3.18)의 신뢰구간에서 확인할 수 있는 것은 신뢰구간의 폭이 \\(X_{o} = \\overline{X}\\) 일 때 최소가 되며, \\(|X_{o}-\\overline{X}|\\) 의 값이 증가함에 따라 점점 넓어진다는 점이다. 즉, \\(E(Y|X_{o})\\) 에 대한 추정은 \\(X_{o}\\) 의 값이 자료의 평균에 가까우면 좋은 결과를 기대할 수 있지만, 그렇지 않은 경우에는 신뢰도가 많이 떨어질 수 있다는 것을 의미한다. 3.4 반응변수의 개별 관측값 예측 이번 절에서는 주어진 설명변수의 수준에 대한 반응변수의 개별 관측값 예측에 대한 문제를 살펴보겠다. 이번 절에서도 미리 지정되는 설명변수의 수준을 \\(X_{o}\\) 라고 하고, \\(X_{o}\\) 에 해당하는 새로운 관측값을 \\(Y_{o}\\) 라고 하자. 새로운 관측값 \\(Y_{o}\\) 는 식 (3.1)의 회귀모형에 의해서 다음과 같이 주어진다. \\[\\begin{equation} Y_{o} = \\beta_{0} + \\beta_{1}X_{o} + \\varepsilon_{o} \\end{equation}\\] \\(Y_{o}\\) 의 점추정량 \\(\\widehat{Y}_{o}\\) 을 구하기 위해서는 \\(\\beta_{0}\\) 와 \\(\\beta_{1}\\) 을 각각의 불편추정량으로 대체하고, 확률변수인 \\(\\varepsilon_{o}\\) 은 \\(N(0,\\sigma^{2})\\) 에 따른 임의의 값을 갖게 되기 때문에 정확한 값을 예측할 수 없는 대상이어서 평균값 0으로 적용시켜서 다음과 같이 구할 수 있다. \\[\\begin{equation} \\widehat{Y}_{o} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1}X_{o} \\tag{3.19} \\end{equation}\\] 따라서 개별 관측값 \\(Y_{o}\\) 와 평균 관측값 \\(E(Y|X_{o})\\) 는 식 (3.15)와 (3.19)에 주어진 동일한 점추정량 \\(\\widehat{Y}_{o}\\) 을 갖게 된다. \\(Y_{o}\\) 에 대한 예측구간을 추정하기 위해 다음의 확률변수를 고려해 보자. \\[\\begin{equation} \\psi = \\widehat{Y}_{o}-Y_{o} \\end{equation}\\] 확률변수 \\(\\psi\\) 는 \\(Y_{i}\\) 와 동일한 분포를 하기 때문에 정규분포를 따르고 있으며, 평균은 다음과 같고, \\[\\begin{align*} E(\\psi) &amp;= E(\\widehat{Y}_{o}-Y_{o}) \\\\ &amp;= E(\\hat{\\beta}_{o}+\\hat{\\beta}_{1}X_{o}) - E(\\beta_{0} + \\beta_{1}X_{o} + \\varepsilon_{o}) \\\\ &amp;= 0 \\end{align*}\\] 분산은 다음과 같이 유도된다. \\[\\begin{align*} Var(\\psi) &amp;= Var(\\widehat{Y}_{o}-Y_{o}) \\\\ &amp;= Var(\\widehat{Y}_{o}) + Var(Y_{o}) \\\\ &amp;= \\sigma^{2}\\left(\\frac{1}{n}+\\frac{(X_{o}-\\overline{X})^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right) + \\sigma^{2} \\\\ &amp; = \\sigma^{2}\\left(1+\\frac{1}{n}+\\frac{(X_{o}-\\overline{X})^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right) \\end{align*}\\] 확률변수 \\(\\psi\\) 를 표준화시킨 통계량 \\(t\\) 는 다음과 같이 표현되며, 분포는 \\(t(n-2)\\) 가 된다. \\[\\begin{equation} t = \\frac{\\widehat{Y}_{o}-Y_{o}}{SE(\\psi)} \\end{equation}\\] 단, \\(SE(\\psi) = \\sqrt{MSE\\left(1+\\frac{1}{n}+\\frac{(X_{o}-\\overline{X})^{2}}{\\sum_{i=1}^{n}(X_{i}-\\overline{X})^{2}}\\right)}\\). 따라서 미리 지정되는 설명변수의 수준 \\(X_{o}\\) 에 대해 새롭게 관측될 개별 반응변수 값 \\(Y_{o}\\) 의 \\(100 \\times (1-\\alpha)\\)% 예측구간은 다음과 같이 주어진다. \\[\\begin{align*} 1-\\alpha &amp;= P\\left(-t_{\\alpha, n-2}&lt;\\frac{\\widehat{Y}_{o}-Y_{o}}{SE(\\psi)}&lt;t_{\\alpha, n-2} \\right) \\\\ &amp;= P\\left(\\widehat{Y}_{o}-t_{\\alpha, n-2}\\cdot SE(\\psi) &lt; Y_{o} &lt; \\widehat{Y}_{o}+t_{\\alpha, n-2}\\cdot SE(\\psi)\\right) \\tag{3.20} \\end{align*}\\] 지정된 설명변수 수준 \\(X_{o}\\) 에 대한 반응변수의 평균에 대한 신뢰구간과 반응변수의 개별 관측값에 대한 예측구간의 폭을 비교해 보면, 식 (3.20)의 예측구간이 식 (3.18)의 신뢰구간보다 항상 더 넓다는 것을 알 수 있다. 이것은 예측구간이 \\(E(Y|X_{o})\\) 을 \\(\\widehat{Y}_{o}\\) 으로 추정하는 과정에서 발생하는 오차와 새로운 관측값 \\(Y_{o}\\) 에 의해 발생하는 오차를 모두 포함하기 때문이다. \\(\\bullet\\) 예제 3.2: 매출액에 대한 광고효과 회귀모형에서 매출액의 신뢰구간과 예측구간 추정 예제 2.2 자료 ex2-2.csv에 대하여 다음의 분석을 실시해 보자. 모형 적합에 사용된 광고비 자료에 대한 평균 매출액의 95% 신뢰구간과 매출액의 개별 관측값에 대한 95% 예측구간을 구하라. 피자 체인점들이 광고비를 25, 30, 35, 40을 각각 지출하면 얻게 될 것으로 예상되는 매출액의 평균은 어떻게 되는가? 평균 매출액에 대한 95% 신뢰구간을 구하라. 수원에 있는 어느 특정 피자 체인점이 광고비를 25, 30, 35, 40을 각각 지출하면 얻게 될 것으로 예측되는 매출액의 95% 예측구간을 구하라. 적합된 회귀모형의 예측에 사용되는 함수는 predict()이다. 함수 lm()으로 생성된 회귀모형 객체에 대한 predict()의 기본적인 사용법은 다음과 같다. predict(object, newdata, interval = c(&quot;none&quot;, &quot;confidence&quot;, &quot;prediction&quot;)) object는 함수 lm()으로 생성된 객체이고, newdata는 새롭게 주어지는 설명변수의 값으로 반드시 데이터 프레임으로 입력되어야 하며, 생략이 되면 모형적합에 사용된 자료에 대한 예측이 이루어진다. interval은 구간의 종류를 선택하는 것으로서, 평균 반응변수에 대한 신뢰구간은 \"confidence\", 반응변수의 개별 관측값에 대한 예측구간은 \"prediction\"을 선택하면 되고, 신뢰구간을 원하지 않는 경우에는 \"none\"을 선택하면 된다. 디폴트는 \"none\"이다. 자료 ex2-2.csv에 대하여 회귀모형 객체 fit2.2를 생성하자. df2.2 &lt;- read_csv(&quot;Data/ex2-2.csv&quot;) fit2.2 &lt;- lm(sales ~ advertisement, data = df2.2) 예제의 첫 번째 문제는 모형적합에 사용된 광고비 자료에 대한 예측이다. 따라서 함수 predict()에 newdata를 지정하지 않고 interval을 \"confidence\"와 \"prediction\"을 각각 지정해서 평균 반응변수에 대한 신뢰구간과 개별 관측값에 대한 예측구간을 구하면 된다. pred_1 &lt;- predict(fit2.2, interval = &quot;confidence&quot;) pred_2 &lt;- predict(fit2.2, interval = &quot;prediction&quot;) 두 구간의 폭은 그래프로 비교해 보자. library(patchwork) p1 &lt;- cbind(df2.2, pred_1) |&gt; ggplot(aes(x = advertisement, y = sales)) + geom_point() + geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.1, fill = &quot;pink&quot;, color = &quot;red&quot;, linewidth = 1) + ggtitle(&quot;반응변수 평균에 대한 신뢰구간&quot;) p2 &lt;- cbind(df2.2, pred_2) |&gt; ggplot(aes(x = advertisement, y = sales)) + geom_point() + geom_ribbon(aes(ymin = lwr, ymax = upr), alpha = 0.1, fill = &quot;skyblue&quot;, color = &quot;blue&quot;, linewidth = 1) + ggtitle(&quot;반응변수 개별 관측값에 대한 예측구간&quot;) p1 + p2 그림 3.5: 매출액 자료에 대한 신뢰구간과 예측구간 비교 예제의 두 번째와 세 번째 문제는 새롭게 주어진 설명변수 값에 대하여 평균 매출액에 대한 신뢰구간과 개별 관측값에 대한 예측구간을 추정하는 문제이다. 새롭게 주어진 설명변수 값을 데이터 프레임으로 지정하고, 예측을 실시해 보자. new_data &lt;- data.frame(advertisement = c(25, 30, 35, 40)) pred_3 &lt;- predict(fit2.2, newdata = new_data, interval = &quot;confidence&quot;) pred_4 &lt;- predict(fit2.2, newdata = new_data, interval = &quot;prediction&quot;) 신뢰구간의 폭은 새롭게 주어지는 설명변수의 값이 자료의 평균인 29.8에 가까울수록 작아진다. new_data |&gt; cbind(pred_3) |&gt; mutate(width = upr - lwr) ## advertisement fit lwr upr width ## 1 25 435.8782 402.7979 468.9584 66.16044 ## 2 30 524.4426 496.0525 552.8327 56.78018 ## 3 35 613.0070 579.1774 646.8366 67.65917 ## 4 40 701.5714 655.6407 747.5022 91.86146 예측구간은 폭도 신뢰구간의 경우와 동일하게 자료의 평균에 가까울수록 작아지지만, 이 예제의 경우에는 상대적으로 큰 차이가 없는 것을 알 수 있다. new_data |&gt; cbind(pred_4) |&gt; mutate(width = upr - lwr) ## advertisement fit lwr upr width ## 1 25 435.8782 304.7133 567.0430 262.3297 ## 2 30 524.4426 394.3814 654.5038 260.1224 ## 3 35 613.0070 481.6512 744.3628 262.7117 ## 4 40 701.5714 566.5916 836.5512 269.9596 3.5 상관계수 : 두 변수 사이의 선형 연관성 측정 두 변수 사이의 선형 관련성을 측정하는 통계량으로 식 (2.2)에 정의된 공분산이 있다. 공분산의 값이 크면 두 변수 사이에 강한 양의 관계 또는 음의 관계가 있다고 할 수 있지만, 값이 작으면 선형 관계가 명확하지 않다고 할 수 있다. 그러나 공분산은 변수가 취하는 값 자체의 절대적 크기에도 영향을 받는 통계량이기 때문에, ‘값이 크다’ 또는 ’값이 작다’에 대한 절대적 기준을 제시할 수 없는데, 이 문제는 두 변수의 표준편차를 나누어 값을 상대화시킴으로서 해결할 수 있다. 이것이 두 변수의 상관계수 \\(\\rho\\) 가 된다. \\[\\begin{equation} \\rho = \\frac{Cov(Y,X)}{\\sqrt{Var(Y)}\\sqrt{Var(X)}} \\tag{3.21} \\end{equation}\\] 상관계수는 \\(-1 \\leq \\rho \\leq 1\\) 의 범위를 취하고 있는데, \\(1\\)과 \\(-1\\)이 되면 두 변수가 완벽한 선형관계를 갖게 되고, \\(0\\)에 가까울수록 명확한 선형관계가 없는 경우가 된다. 그림 3.6은 상관관계의 값에 따라 두 변수의 산점도에 어떤 변화가 생길 수 있는지를 보여주는 모의자료에 대한 산점도이다. 그림 3.6: 상관계수와 두 변수 산점도 형태 공분산이나 상관계수는 두 변수가 모두 확률변수인 경우에 적용되는 개념이다. 그러나 식 (2.1)에서 설정한 회귀모형에서 설명변수 \\(X\\) 는 분석자가 통제할 수 있는 변량이고, 반응변수만 확률변수라고 가정하였다. 하지만 설명변수 \\(X\\) 의 수준을 분석자가 임의로 조절하는 것이 불가능한 경우가 많이 있으며, 이런 상황에서는 변수 \\(Y\\) 와 \\(X\\) 가 모두 확률변수가 된다. 예를 들어 그림 1.1에서 다루고 있는 일일 최고 기온과 전기 사용량의 관계에서 설명변수인 일일 최고 기온의 수준은 분석자가 임의로 선택할 수 없는 변량이 된다. 또한 예제 2.2에서 다루고 있는 매출액에 대한 광고비 효과 자료에서도 실제로 우리는 피자 체인점을 임의로 선택해서 지출된 광고비와 매출액을 관측하는 것이기 때문에 광고비 수준을 분석자가 미리 설정하는 것은 어려운 상황이라고 할 수 있다. 이와 같이 설명변수도 확률변수인 경우에는 관측된 설명변수의 수준을 조건부로 하여 회귀모형에 대한 추론을 실시하게 된다. 식 (3.21)에 주어진 모수 \\(\\rho\\) 는 다음의 표본 상관계수로 추정할 수 있다. \\[\\begin{equation} r = \\frac{\\sum_{i=1}^{n} (X_{i}-\\overline{X})(Y_{i}-\\overline{Y})}{\\sqrt{\\sum_{i=1}^{n} (X_{i}-\\overline{X})^{2}}\\sqrt{\\sum_{i=1}^{n}(Y_{i}-\\overline{Y})^{2}}} \\tag{3.22} \\end{equation}\\] 상관계수의 유의성 검정은 다음의 가설에 대한 검정이다. \\[\\begin{equation} H_{0}: \\rho = 0,~~H_{1}:\\rho \\ne 0 \\end{equation}\\] 검정통계량 \\(t\\) 는 다음과 같이 주어지며, \\(H_{0}\\) 이 사실인 경우의 분포는 \\(t(n-2)\\) 가 된다. \\[\\begin{equation} t = \\frac{r \\sqrt{n-2}}{\\sqrt{1-r^{2}}} \\end{equation}\\] 상관계수의 유의성 검정으로 \\(H_{0}\\) 이 기각되면, 두 변수 사이에 유의한 선형관계가 있다는 것을 의미한다. 하지만 그렇다고 해서 두 변수 사이에 강한 선형관계가 있다는 것을 의미하는 것은 아니다. 그림 3.6에서 상관계수 값이 -0.29인 자료의 산점도를 보면 두 변수 사이에는 마치 선형관계가 없는 것으로 보인다. 하지만 상관계수의 유의성 검정을 실시하면 조금 다른 결과를 얻게 되는데, 상관계수의 유의성 검정은 함수 cor.test()로 할 수 있다. cor.test(x,y) ## ## Pearson&#39;s product-moment correlation ## ## data: x and y ## t = -2.0846, df = 48, p-value = 0.04245 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## -0.52441468 -0.01062909 ## sample estimates: ## cor ## -0.2881244 검정 결과는 두 변수 사이에 유의한 선형관계가 존재하는 것으로 나왔다. "],["multiple-reg.html", "4 장 다중선형회귀모형 4.1 다중회귀모형의 설정 4.2 다중회귀모형의 추론 4.3 변수선택", " 4 장 다중선형회귀모형 반응변수의 변동을 하나의 설명변수만으로 충분하게 설명하는 것은 대부분의 경우 불가능할 것이다. 따라서 반응변수와 관련이 있을 것으로 보이는 여러 개의 설명변수를 모형에 포함시키는 다중회귀모형이 실제 상황에서 많이 사용되는 모형이 된다. 기본 가정 및 추론 방법에서는 단순회귀모형과 큰 차이가 있는 것은 아니지만, 모형에 여러 개의 설명변수가 포함되기 때문에 단순회귀모형에서는 없었던 문제들이 발생할 수 있다. 4.1 다중회귀모형의 설정 다중선형회귀모형에서는 반응변수 \\(Y\\) 와 설명변수 \\(X_{1}, \\ldots, X_{k}\\) 사이에 다음의 관계가 존재한다고 가정한다. \\[\\begin{equation} Y_{i} = \\beta_{0} + \\beta_{1}X_{1i} + \\cdots + \\beta_{k}X_{ki} + \\varepsilon_{i},~~i=1,\\ldots,n \\tag{4.1} \\end{equation}\\] \\(Y_{i}\\) 는 반응변수의 \\(i\\) 번째 값, \\(X_{ji}\\) 는 설명변수 \\(X_{j}\\) 의 \\(i\\) 번째 값을 나타내며, 모수 \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k}\\) 는 \\((k+1)\\) 차원의 공간에서 정의되는 회귀평면을 구성하고 있다. 오차항 \\(\\varepsilon_{i}\\) 은 단순회귀모형에서와 동일한 의미를 갖고 있는 확률변수로서, \\(\\varepsilon_{i} \\stackrel{iid}{\\sim} N(0,\\sigma^{2}),~i=1,\\ldots,n\\) 의 가정을 하고 있다. 식 (4.1)의 관계식은 다음과 같이 풀어서 표현할 수 있으며, \\[\\begin{align*} Y_{1} &amp;= \\beta_{0} + \\beta_{1}X_{11} + \\cdots + \\beta_{k}X_{k1} + \\varepsilon_{1} \\\\ Y_{2} &amp;= \\beta_{0} + \\beta_{1}X_{12} + \\cdots + \\beta_{k}X_{k2} + \\varepsilon_{2} \\\\ \\vdots \\\\ Y_{n} &amp;= \\beta_{0} + \\beta_{1}X_{1n} + \\cdots + \\beta_{k}X_{kn} + \\varepsilon_{n} \\end{align*}\\] 이것은 다시 다음과 같이 행렬의 형태로 정리할 수 있다. \\[ \\begin{pmatrix} Y_{1} \\\\ Y_{2} \\\\ \\vdots \\\\ Y_{n} \\end{pmatrix} = \\begin{pmatrix} 1 &amp; X_{11} &amp; \\cdots &amp; X_{k1} \\\\ 1 &amp; X_{12} &amp; \\cdots &amp; X_{k2} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 1 &amp; X_{1n} &amp; \\cdots &amp; X_{kn} \\end{pmatrix} \\begin{pmatrix} \\beta_{0} \\\\ \\beta_{1} \\\\ \\vdots \\\\ \\beta_{k} \\end{pmatrix} + \\begin{pmatrix} \\varepsilon_{1} \\\\ \\varepsilon_{2} \\\\ \\vdots \\\\ \\varepsilon_{n} \\end{pmatrix} \\tag{4.2} \\] 또는 다음의 행렬로 표현할 수 있다. \\[\\begin{equation} \\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\tag{4.3} \\end{equation}\\] \\(\\mathbf{Y}\\) 는 반응변수의 관찰값 벡터이고, \\(\\mathbf{X}\\) 는 설명변수의 관찰값 행렬로써 design matrix라고 하며, \\(\\boldsymbol{\\beta}\\) 는 모수 벡터, \\(\\boldsymbol{\\varepsilon}\\) 은 오차항 벡터이다. 4.1.1 회귀계수의 추정 회귀계수 \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k}\\) 의 추정은 단순회귀모형의 경우와 동일한 방식으로 이루어진다. 반응변수 \\(Y\\) 와 설명변수 \\(X_{1}, \\ldots, X_{k}\\) 에 대해 관측된 \\(n\\) 개의 자료를 \\((y_{i}, x_{1i}, \\ldots, x_{ki}), ~i=1,\\ldots,n\\) 이라고 하면, 다음의 \\(RSS\\) 를 최소화하는 \\(\\hat{\\boldsymbol{\\beta}} = (\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\ldots, \\hat{\\beta}_{k})\\) 를 선택한다. \\[\\begin{equation} RSS = \\sum_{i=1}^{n} \\left(y_{i} - \\hat{\\beta}_{0} - \\hat{\\beta}_{1}x_{1i} - \\ldots - \\hat{\\beta}_{k}x_{ki} \\right)^{2} \\tag{4.4} \\end{equation}\\] 식 (4.4)의 \\(RSS\\) 를 최소화시키는 추정값 \\(\\hat{\\beta}_{0}, \\ldots, \\hat{\\beta}_{k}\\) 를 구하기 위해서 \\(RSS\\) 를 \\(\\hat{\\beta}_{j}, ~j = 0, \\ldots, k\\) 에 대하여 각각 편미분을 실시해서 다음의 \\((k+1)\\) 개의 방정식을 얻는다. \\[\\begin{align} \\frac{\\partial RSS}{\\partial \\hat{\\beta}_{0}} &amp; = -2 \\sum_{i=1}^{n}(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{1i}-\\cdots-\\hat{\\beta}_{k}x_{ki}) = 0\\\\ \\frac{\\partial RSS}{\\partial \\hat{\\beta}_{1}} &amp; = -2 \\sum_{i=1}^{n}x_{1i}(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{1i}-\\cdots-\\hat{\\beta}_{k}x_{ki}) = 0 \\\\ &amp;\\vdots \\\\ \\frac{\\partial RSS}{\\partial \\hat{\\beta}_{k}} &amp; = -2 \\sum_{i=1}^{n}x_{ki}(y_{i}-\\hat{\\beta}_{0}-\\hat{\\beta}_{1}x_{1i}-\\cdots-\\hat{\\beta}_{k}x_{ki}) = 0 \\tag{4.5} \\end{align}\\] 식 (4.5)을 정리하면 다음과 같이 표현이 되며, \\[\\begin{align} n\\hat{\\beta}_{0}+\\hat{\\beta}_{1}\\sum_{i=1}^{n} x_{1i} + \\cdots + \\hat{\\beta}_{k}\\sum_{i=1}^{n} x_{ki} &amp;= \\sum_{i=1}^{n} y_{i} \\\\ \\hat{\\beta}_{0}\\sum_{i=1}^{n} x_{1i} + \\hat{\\beta}_{1}\\sum_{i=1}^{n} x_{1i}^{2} + \\cdots + \\hat{\\beta}_{k} \\sum_{i=1}^{n} x_{1i}x_{ki} &amp;= \\sum_{i=1}^{n} x_{1i}y_{i} \\\\ &amp; \\vdots \\\\ \\hat{\\beta}_{0}\\sum_{i=1}^{n} x_{ki} + \\hat{\\beta}_{1}\\sum_{i=1}^{n} x_{ki}x_{1i} + \\cdots + \\hat{\\beta}_{k}\\sum_{i=1}^{n} x_{ki}^{2} &amp;= \\sum_{i=1}^{n} x_{ki}y_{i} \\tag{4.6} \\end{align}\\] 행렬의 형태로 표현하면 다음과 같이 된다. \\[ \\begin{pmatrix} n &amp; \\sum x_{1i} &amp; \\cdots &amp; \\sum x_{ki} \\\\ \\sum x_{1i} &amp; \\sum x_{1i}^{2} &amp; \\cdots &amp; \\sum x_{1i}x_{ki} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sum x_{ki} &amp; \\sum x_{ki}x_{1i} &amp; \\cdots &amp; \\sum x_{ki}^{2} \\end{pmatrix} \\begin{pmatrix} \\hat{\\beta}_{0} \\\\ \\hat{\\beta}_{1} \\\\ \\vdots \\\\ \\hat{\\beta}_{k} \\end{pmatrix}= \\begin{pmatrix} 1 &amp; 1 &amp; \\cdots &amp; 1 \\\\ x_{11} &amp; x_{12} &amp; \\cdots &amp; x_{1n} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ x_{k1} &amp; x_{k2} &amp; \\cdots &amp; x_{kn} \\end{pmatrix} \\begin{pmatrix} y_{1} \\\\ y_{2} \\\\ \\vdots \\\\ y_{n} \\end{pmatrix} \\tag{4.7} \\] 식 (4.7)의 좌변 첫 번째 행렬은 식 (4.3)에서 정의된 행렬 \\(\\mathbf{X}\\) 로 다음과 같이 표현된다. \\[ \\begin{pmatrix} n &amp; \\sum x_{1i} &amp; \\cdots &amp; \\sum x_{ki} \\\\ \\sum x_{1i} &amp; \\sum x_{1i}^{2} &amp; \\cdots &amp; \\sum x_{1i}x_{ki} \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ \\sum x_{ki} &amp; \\sum x_{ki}x_{1i} &amp; \\cdots &amp; \\sum x_{ki}^{2} \\end{pmatrix}= \\mathbf{X}^{T}\\mathbf{X} \\tag{4.8} \\] 단, \\(\\mathbf{X}^{T}\\) 는 행렬 \\(\\mathbf{X}\\) 의 전치행렬이다. 따라서 다중회귀모형의 회귀계수에 대한 최소제곱추정량을 구하기 위한 방정식은 다음과 같이 주어진다. \\[\\begin{equation} \\mathbf{X}^{T}\\mathbf{X}\\hat{\\boldsymbol{\\beta}} = \\mathbf{X}^{T}\\mathbf{Y} \\tag{4.9} \\end{equation}\\] 만일 \\(\\mathbf{X}^{T}\\mathbf{X}\\) 의 역행렬이 존재한다면, 최소제곱추정량은 다음과 같이 표현된다. \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{Y} \\tag{4.10} \\end{equation}\\] \\(\\bullet\\) 예제: 다중회귀모형의 회귀계수 추정 데이터 프레임 mtcars는 1974년 \\(\\textit{Motor Trend US}\\) 에 실린 32 종류 자동차 모델의 연비와 관련된 자료가 입력되어 있다. str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... 연비를 나타내는 mpg를 반응변수로 하고, 설명변수에는 차량의 무게를 나타내는 wt와 0.25 마일까지 도달하는 데 걸리는 시간을 나타내는 qsec을 선택해서 다중회귀모형을 설정하고 회귀계수를 추정해 보자. 단순회귀모형의 경우와 동일하게 다중회귀모형에서도 함수 lm()을 사용해서 모형적합을 실시할 수 있다. 하지만 단순회귀모형의 경우와 다르게 설명변수의 개수가 2개 이상이 되며, 복잡한 형태의 모형이 설정되는 경우도 많이 있기 때문에 다중회귀모형의 경우에는 함수 lm()에 지정되는 R 모형공식에 반응변수와 설명변수를 구분하는 ~ 기호 외에도 많은 기호가 사용된다. R 모형공식에서 사용되는 기호 기호 사용법 물결표 (~) 반응변수와 설명변수의 구분. 물결표의 왼쪽에는 반응변수, 오른쪽에는 설명변수를 둔다. 플러스 (+) 모형에 포함된 설명변수의 구분. 반응변수 y와 설명변수 x1, x2, x3의 회귀모형은 y ~ x1 + x2 + x3로 표현된다. 콜론 (:) 설명변수 사이의 상호작용 표현. 반응변수 y와 설명변수 x1, x2 그리고 x1과 x2의 상호작용이 포함된 모형은 y ~ x1 + x2 + x1:x2로 표현된다. 별표 (*) 주효과와 상호작용 효과를 포함한 모든 효과 표현. y ~ x1 * x2 * x3는 y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3 + x1:x2:x3를 의미한다. 윗꺽쇠 (^) 지정된 차수까지의 상호작용 표현. y ~ (x1 + x2 + x3)^2는 y ~ x1 + x2 + x3 + x1:x2 + x1:x3 + x2:x3을 의미한다. 점 (.) 반응변수를 제외한 데이터 프레임에 있는 모든 변수. 만일 데이터 프레임에 y, x1, x2, x3가 있다면 y ~ . 은 y ~ x1 + x2 + x3을 의미한다. 마이너스(-) 회귀모형에서 제외되는 변수. y ~ (x1 + x2 + x3)^2 – x2:x3는 y ~ x1 + x2 + x3 + x1:x2 + x1:x3을 의미한다. - 1, + 0 절편 제거. y ~ x – 1 혹은 y ~ x + 0은 원점을 지나는 회귀모형을 의미한다. I() 괄호 안의 연산자를 수학 연산자로 인식. y ~ x1 + I(x2+x3)는 \\(y=\\beta_{0}+\\beta_{1}x_{1}+\\beta_{2}(x_{2}+x_{3})+\\varepsilon\\) 모형을 의미한다. 이제 mpg를 반응변수로, wt와 qsec을 설명변수로 하는 다중회귀모형을 적합해 보자. fit_m &lt;- lm(mpg ~ wt + qsec, data = mtcars) fit_m ## ## Call: ## lm(formula = mpg ~ wt + qsec, data = mtcars) ## ## Coefficients: ## (Intercept) wt qsec ## 19.7462 -5.0480 0.9292 적합된 모형식은 \\(\\widehat{\\mbox{mpg}} = 19.74 - 5.047\\mbox{ wt} + 0.929\\mbox{ qsec}\\) 임을 알 수 있다. 추정된 회귀계수에 대한 해석은 단순회귀모형의 경우와 비슷하지만 제한 사항이 추가된다. 단순회귀모형에서는 하나의 설명변수만 있기 때문에 추정된 모형은 직선으로 표현되고, 따라서 해당 설명변수가 한 단위 증가했을 때 반응변수의 평균 변화량으로 회귀계수를 해석할 수 있었다. 하지만 다중회귀모형에서는 추정된 회귀모형이 직선이 아닌 2차원 이상에서 정의되는 평면이 되기 때문에 조금 더 복잡한 상황이 되는데, 그것은 어느 한 설명변수의 효과를 측정하기 위해서는 회귀평면을 구성하고 있는 다른 설명변수의 값이 고정되어야 하기 때문이다. 그림 4.1에서는 mpg를 반응변수로, wt와 qsec을 설명변수로 하는 다중회귀모형으로 적합된 회귀평면이 작성되어 있다. 변수 wt의 추정된 회귀계수 \\(-5.047\\) 은 qsec을 일정한 수준으로 고정한 상태에서 wt가 한 단위 증가했을 때 mpg의 평균 변화량을 나타내는 것인데, 그림에서 회귀평면이 wt 축의 양의 방향으로 급격하게 기우는 모습을 볼 수 있다. 또한 변수 qsec에 대한 추정된 회귀계수 \\(0.929\\) 은 wt를 일정한 수준으로 고정한 상태에서 qsec이 한 단위 증가했을 때 mpg의 평균 변화량을 나타내는 것이며, 그림에서 회귀평면이 qsec의 양의 방향으로 완만하게 증가하는 모습을 볼 수 있다. 그림 4.1: 설명변수가 2개인 다중회귀모형에서 추정된 회귀평면 \\(\\bullet\\) 예제: 행렬 state.x77 행렬 state.x77은 미국 50개 주와 관련된 8개 변수로 구성되었다. 변수 Life Exp와 HS Grad는 이름 중간에 빈칸이 있으며, 각 주의 이름이 행렬의 row name으로 입력되어 있음을 알 수 있다. state.x77[1:5,] ## Population Income Illiteracy Life Exp Murder HS Grad Frost Area ## Alabama 3615 3624 2.1 69.05 15.1 41.3 20 50708 ## Alaska 365 6315 1.5 69.31 11.3 66.7 152 566432 ## Arizona 2212 4530 1.8 70.55 7.8 58.1 15 113417 ## Arkansas 2110 3378 1.9 70.66 10.1 39.9 65 51945 ## California 21198 5114 1.1 71.71 10.3 62.6 20 156361 변수 Murder를 반응변수로, 나머지 7개 변수를 설명변수로 하는 다중회귀모형을 설정해 보자. 회귀분석을 원활하게 수행하기 위해서는 자료가 행렬이 아닌 데이터 프레임의 형태로 입력되어 있어야 한다. 행렬 state.x77을 데이터 프레임으로 변환하면서, 빈칸이 있는 변수 이름을 수정하고 반응변수를 마지막 변수로 이동시키자. 반응변수의 위치를 마지막으로 이동시킨 이유는 산점도 행렬을 작성하고 결과를 해석할 때 더 편리하기 때문이다. states &lt;- as.data.frame(state.x77) |&gt; rename(Life_Exp = `Life Exp`, HS_Grad = `HS Grad`) |&gt; relocate(Murder, .after = last_col()) str(states) ## &#39;data.frame&#39;: 50 obs. of 8 variables: ## $ Population: num 3615 365 2212 2110 21198 ... ## $ Income : num 3624 6315 4530 3378 5114 ... ## $ Illiteracy: num 2.1 1.5 1.8 1.9 1.1 0.7 1.1 0.9 1.3 2 ... ## $ Life_Exp : num 69 69.3 70.5 70.7 71.7 ... ## $ HS_Grad : num 41.3 66.7 58.1 39.9 62.6 63.9 56 54.6 52.6 40.6 ... ## $ Frost : num 20 152 15 65 20 166 139 103 11 60 ... ## $ Area : num 50708 566432 113417 51945 156361 ... ## $ Murder : num 15.1 11.3 7.8 10.1 10.3 6.8 3.1 6.2 10.7 13.9 ... 다중회귀모형에 의한 분석을 진행하기 전에 반드시 거쳐야 할 단계가 있는데, 그것은 모형에 포함될 변수들의 관계를 탐색하는 것이다. 변수들 사이의 관계 탐색에 많이 사용되는 방법은 상관계수와 산점도행렬이다. 산점도행렬과 같은 그래프에 의한 탐색은 필수적이라 할 수 있으며, 연속형 변수의 경우에는 두 변수끼리 짝을 지어 상관계수를 살펴보는 것도 필요하다. 상관계수는 변수들 사이의 선형관계 정도를 확인할 때 사용할 수 있는데, 함수 cor()로 계산할 수 있다. 사용법은 다음과 같다. cor(x, y = NULL, use = \"everything\", method = c(\"pearson\", \"kendall\", \"spearman\")) x와 y 모두 벡터, 행렬 혹은 데이터 프레임이 가능한데, x만 주어지면 x에 포함된 모든 변수들 사이의 상관계수를 계산하게 되고, y도 주어지면 x에 속한 변수와 y에 속한 변수들을 하나씩 짝을 지어 상관계수를 계산하게 된다. use는 결측값의 처리방식에 대한 것으로 선택할 수 있는 것은 디폴트인 \"everything\"과 \"all.obs\", \"complete.obs\", \"pairwise.complete.obs\"이 있으며, 해당 문자의 약칭으로도 사용이 가능하다. 결측값이 존재하면 use = \"everything\"인 경우에는 NA가 계산결과로 출력되고, use = \"all\"인 경우에는 오류가 발생한다. 또한 use = \"complete\"의 경우에는 NA가 있는 행은 모두 제거된 상태에서 상관계수가 계산되고, use = \"pairwise\"의 경우에는 상관계수가 계산되는 변수들만을 대상으로 NA가 있는 행을 제거하고 상관계수를 계산한다. method는 계산하는 상관계수의 종류를 선택한다. 디폴트인 \"pearson\"은 Pearson의 상관계수를 지정하는 것으로 두 변수 사이의 선형관계 정도를 표현하는 가장 일반적으로 많이 사용되는 상관계수를 계산한다. 두 번째 방법인 \"kendall\"은 Kendall의 순위상관계수 혹은 Kendall의 \\(\\tau\\) 를 지정하는 것으로서 concordant pair와 discordant pair를 이용하여 정의되는 비모수 상관계수를 계산하며, 순서형 자료에 주로 적용되는 방법이다. 세 번째 방법인 \"spearman\"은 Spearman의 순위상관계수 혹은 Spearman의 \\(\\rho\\) 를 지정하는 것으로서 두 변수 사이의 관계가 단조증가 혹은 단조감소 함수로 얼마나 잘 설명될 수 있는지를 표현하는 비모수 상관계수를 계산한다. 정규성 가정이 어긋나는 경우에 사용할 수 있다. 데이터 프레임 states를 구성하고 있는 변수들의 상관계수를 구해보자. cor(states) ## Population Income Illiteracy Life_Exp HS_Grad ## Population 1.00000000 0.2082276 0.10762237 -0.06805195 -0.09848975 ## Income 0.20822756 1.0000000 -0.43707519 0.34025534 0.61993232 ## Illiteracy 0.10762237 -0.4370752 1.00000000 -0.58847793 -0.65718861 ## Life_Exp -0.06805195 0.3402553 -0.58847793 1.00000000 0.58221620 ## HS_Grad -0.09848975 0.6199323 -0.65718861 0.58221620 1.00000000 ## Frost -0.33215245 0.2262822 -0.67194697 0.26206801 0.36677970 ## Area 0.02254384 0.3633154 0.07726113 -0.10733194 0.33354187 ## Murder 0.34364275 -0.2300776 0.70297520 -0.78084575 -0.48797102 ## Frost Area Murder ## Population -0.3321525 0.02254384 0.3436428 ## Income 0.2262822 0.36331544 -0.2300776 ## Illiteracy -0.6719470 0.07726113 0.7029752 ## Life_Exp 0.2620680 -0.10733194 -0.7808458 ## HS_Grad 0.3667797 0.33354187 -0.4879710 ## Frost 1.0000000 0.05922910 -0.5388834 ## Area 0.0592291 1.00000000 0.2283902 ## Murder -0.5388834 0.22839021 1.0000000 함수 cor()에 데이터 프레임을 입력하면 모든 변수들 사이의 상관계수가 행렬 형태로 계산되어 출력된다. 상관계수행렬은 변수의 개수가 많아지면 변수 사이의 관계 파악이 어려워지는 문제가 있다. 이런 경우에는 그래프로 표현하는 것이 더 효과적인 방법이 될 것이다. 패키지 GGally의 함수 ggcorr()은 상관계수행렬을 그래프로 표현할 때 많이 사용되는 함수이며, 사용법은 다음과 같다. ggcorr(data, method = c(\"pairwise\", \"pearson\"), label = FALSE, label_round = 1, ...) method는 결측값 처리 방식과 계산되는 상관계수의 종류를 차례로 지정하는데, 디폴트는 \"pairwise\"와 \"pearson\"이다. label은 그래프에 상관계수를 표시할 것인지 여부를 결정하는 것이고, label_round는 표시되는 상관계수의 반올림 자릿수를 지정한다. 데이터 프레임 states를 구성하고 하는 변수들의 상관계수를 그래프로 나타내 보자. 숫자로만 구성되어 있는 상관계수행렬보다 훨씬 간편하게 변수 사이의 상관관계를 파악할 수 있다. library(GGally) ggcorr(states, label = TRUE, label_round = 2) 그림 4.2: 함수 ggcorr()에 의한 상관계수 그래프 반응변수를 마지막 변수로 이동시켰기 때문에 반응변수와 다른 설명변수 사이의 상관계수를 편하게 확인할 수 있다. 반응변수 Murder가 설명변수 Illiteracy와는 비교적 높은 양의 상관관계를, Life_Exp와는 비교적 높은 음의 상관관계를 보이고 있다. 상관계수는 두 변수 사이의 선형관계 정도만을 측정하는 측도이다. 변수 사이에 존재하는 ’있는 그대로’의 관계를 확인하는 가장 좋은 방법은 산점도행렬이다. 함수 GGally::ggpairs()로 작성해 보자. ggpairs(states, lower = list(continuous = &quot;smooth&quot;)) 그림 4.3: 함수 ggpairs()에 의한 산점도행렬 반응변수를 마지막 변수로 위치를 이동시켰기 때문에 산점도행렬의 마지막 행을 이루고 있는 모든 패널에서 Murder가 Y축 변수로 배치되고, 설명변수들이 각각 X축 변수로 배치되었다. 이제 states 자료에 대한 회귀모형을 함수 lm()으로 적합해 보자. fit_s &lt;- lm(Murder ~ ., data = states) fit_s ## ## Call: ## lm(formula = Murder ~ ., data = states) ## ## Coefficients: ## (Intercept) Population Income Illiteracy Life_Exp HS_Grad ## 1.222e+02 1.880e-04 -1.592e-04 1.373e+00 -1.655e+00 3.234e-02 ## Frost Area ## -1.288e-02 5.967e-06 함수 lm()으로 생성된 객체 fit_s을 단순하게 출력시키면 추정된 회귀계수만 나타나지만, 사실 객체 fit_s는 많은 양의 정보가 담겨 있는 리스트이며, 이것은 획득하기 위해서는 몇 가지 함수를 사용해야 한다. 함수 목록은 아래 표에 정리되어 있으며, 사용하는 방법은 앞으로 살펴보겠다. 함수 lm() 으로 생성된 객체에서 필요한 결과를 얻기 위해 유용하게 사용되는 함수 함수 산출 결과 anova() 추정된 회귀모형의 분산분석표 혹은 두 개 이상의 추정된 모형을 비교하기 위한 분산분석표 coefficients() 추정된 회귀계수. coef()도 가능. confint() 회귀계수의 신뢰구간. 95% 신뢰구간이 디폴트. deviance() 잔차제곱합(residual sum of squares; RSS), \\(\\sum (y_{i}-\\hat{y}_{i})^{2}\\) fitted() 반응변수의 적합값, \\(\\hat{y}_{i}\\) residuals() 회귀모형의 잔차, \\(e_{i}=y_{i}-\\hat{y}_{i}\\) . resid()도 가능. summary() 회귀모형의 다양한 적합 결과 4.1.2 다항회귀모형 식 (4.1)에 정의된 다중회귀모형에서는 반응변수와 설명변수 사이의 관계가 선형이라고 가정하고 있다. 하지만 두 변수의 관계가 그림 1.4와 같이 명확한 2차 함수 관계가 있을 경우에는 설명변수의 제곱항을 모형에 추가하는 다항회귀모형이 더 적절한 대안이 될 수 있다. 단순회귀모형에 대한 \\(p\\) 차 다항회귀모형은 다음과 같이 설정된다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}X^{2} + \\cdots + \\beta_{p}X^{p} + \\varepsilon \\tag{4.11} \\end{equation}\\] 차수 \\(p\\) 는 가능한 낮게 잡는 것이 좋은데, 그것은 너무 높은 차수의 항을 모형에 포함시키면 \\(\\left( \\mathbf{X}^{T} \\mathbf{X} \\right)^{-1}\\) 가 불안정해질 수 있고, 따라서 회귀계수 추정에 문제가 생길 수 있기 때문이다. 이 문제는 ’다중공선성’을 소개할 때 다시 살펴보겠다. 일단 차수가 선택되면, 선택된 차수 이하의 모든 차수는 반드시 모형에 포함되어야 한다. 예를 들어 \\(Y=\\beta_{0}+\\beta_{1}X+\\beta_{2}X^{2}+\\varepsilon\\) 모형에서 어떠한 이유에서 \\(X\\) 의 1차항을 제거한다면, 모형은 \\(Y=\\beta_{0}+\\beta_{2}X^{2}+\\varepsilon\\) 가 되는데, 이 모형은 Y축을 중심으로 좌우대칭을 이루어야 하는 지나치게 강한 가정을 갖게 된다. 또한 만일 변수 \\(X\\) 의 값을 \\(a\\) 만큼 평행 이동시켜야 한다면, \\(X\\) 가 \\(X+a\\) 로 변경되어 다음과 같이 모형에 다시 1차항이 나타나게 된다. \\[\\begin{align*} Y &amp;= \\beta_{0} + \\beta_{2}(X+a)^{2} + \\varepsilon \\\\ &amp;= \\beta_{0} + \\beta_{2}(X^{2} + 2aX + a^{2}) + \\varepsilon \\end{align*}\\] 설명변수가 \\(X_{1}\\) 과 \\(X_{2}\\) 인 다중회귀모형에서 두 변수가 모두 반응변수와 2차 함수 관계가 있다면, 다음과 같은 다항회귀모형이 적절한 회귀모형이 될 수 있다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X_{1} + \\beta_{2}X_{2} + \\beta_{3}X_{1}^{2} + \\beta_{4}X_{2}^{2} + \\varepsilon \\tag{4.12} \\end{equation}\\] \\(\\bullet\\) 예제: women 데이터 프레임 women에는 미국 30대 여성의 키(height)와 몸무게(weight)가 입력되어 있다. 두 변수 height와 weight의 회귀모형을 설정해 보자. 먼저 두 변수의 관계가 선형인지 여부를 그래프로 확인해 보자. ggplot(women, aes(x = height, y = weight)) + geom_point(size = 3) + geom_smooth(aes(color = &quot;loess&quot;), se = FALSE) + geom_smooth(aes(color = &quot;linear&quot;), method=&quot;lm&quot;, se = FALSE) + labs(color = NULL) 그림 4.4: 데이터 프레임 women의 변수 height와 weight의 산점도와 선형회귀직선 및 국소회귀곡선 두 변수의 관계가 선형보다는 2차 곡선이 더 적합한 것으로 보인다. 다항회귀모형은 함수 poly()를 이용하거나 또는 함수 I()를 이용해서 적합할 수 있다. 함수 poly()는 함수 lm()과 함께 lm(y ~ poly(x, degree = 1, raw = FALSE), data)와 같이 사용할 수 있다. degree는 다항회귀모형의 차수를 지정하는 것으로 디폴트 값은 1차이고, raw는 직교다항회귀(orthogonal polynomial regression)의 사용 여부를 선택하는 것으로서 디폴트 값인 FALSE는 직교다항회귀에 의한 적합이 된다. 따라서 일반적인 다항회귀모형을 사용하고자 한다면 반드시 raw에 TRUE를 지정해야 한다. 차수가 높지 않은 일반적인 다항회귀모형을 적합하는 경우에는 함수 I()를 사용하는 것이 더 간편할 수 있다. 함수 lm() 안에서 lm(y ~ x + I(x^2), data)와 같이 입력하면 2차 다항회귀모형을 적합하게 된다. 반응변수 weight에 대한 설명변수 height의 2차 다항회귀모형을 적합해 보자. fit_w &lt;- lm(weight ~ height + I(height^2), women) fit_w ## ## Call: ## lm(formula = weight ~ height + I(height^2), data = women) ## ## Coefficients: ## (Intercept) height I(height^2) ## 261.87818 -7.34832 0.08306 적합된 회귀모형식은 \\(\\hat{y}_{i}=261.87 - 7.348x_{i} + 0.08x_{i}^{2}\\) 임을 알 수 있다. 4.1.3 가변수 회귀모형 선형회귀모형에서 반응변수는 유형이 반드시 연속형이어야 하며, 정규분포의 가정이 필요하다. 반면에 설명변수는 연속형 변수와 범주형 변수가 모두 가능한데, 연속형인 경우에는 정규분포의 가정은 필요 없으나 가능한 좌우대칭에 가까운 분포를 하는 것이 좋다. 범주형 변수 중 순서형 변수인 경우에는 연속형 변수처럼 변수의 값을 그대로 사용하는 것이 가능하지만, 명목형 변수의 경우에는 변수의 값을 그대로 사용하면 절대로 안 되고, 반드시 가변수(dummy variable)를 대신 사용해야 한다. 가변수를 설정하는 방식은 몇 가지가 있는데, 그 중 회귀계수 \\(\\beta_{0}\\) 를 유지하는 방식을 살펴보자. 이 경우, 가변수는 0 또는 1의 값을 갖게 되며, 범주의 개수보다 하나 작은 개수의 가변수를 사용하게 된다. 예를 들어, “Yes”, “No”와 같은 2개의 범주를 갖는 범주형 변수와 연속형 변수 \\(X\\) 를 설명변수로 하는 회귀모형은 다음과 같이 하나의 가변수를 갖게 된다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}D + \\varepsilon \\tag{4.13} \\end{equation}\\] 가변수 \\(D\\) 가 “Yes” 범주이면 1, “No” 범주이면 0을 값으로 갖는다면, 모집단 회귀직선은 다음과 같이 주어진다. \\[ E(Y) = \\begin{cases} \\beta_{0} + \\beta_{1}X &amp; \\quad \\text{if } D = 0 \\\\ \\beta_{0} + \\beta_{2} + \\beta_{1}X &amp; \\quad \\text{if } D = 1 \\end{cases} \\] \\(D=0\\) 이 되는 범주를 ’기준범주’라고 하는데, 절편 \\(\\beta_{0}\\) 가 기준범주의 효과를 나타내고 있고, 가변수의 회귀계수 \\(\\beta_{2}\\) 는 기준범주와 “Yes” 범주의 효과 차이를 나타내고 있다. 식 (4.13)의 회귀모형에서는 반응변수 \\(Y\\) 와 설병변수 \\(X\\) 가 “Yes”와 “No” 범주에서 동일한 관계를 갖고 있다고 가정하고 있는데, 만일 각 범주에서 두 변수의 관계가 다를 수 있다면 기울기도 다르게 설정할 필요가 있다. 만일 기울기가 그룹별로 다르다고 가정한다면 가변수와 다른 설명변수의 상호작용 효과를 추가해야 한다. 식 (4.13)의 회귀모형에 가변수와 연속형 변수의 상호작용 효과가 추가된 모형은 다음과 같이 표현된다. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}D + \\beta_{3}DX + \\varepsilon \\tag{4.14} \\end{equation}\\] 이 경우에 모집단 회귀직선은 다음과 같다. \\[ E(Y) = \\begin{cases} \\beta_{0} + \\beta_{1}X &amp; \\quad \\text{if } D = 0 \\\\ (\\beta_{0} + \\beta_{2}) + (\\beta_{1}+\\beta_{3})X &amp; \\quad \\text{if } D = 1 \\end{cases} \\] 연속형 설명변수와 가변수의 상호작용 효과를 모형에 추가하는 것이 더 포괄적인 모형을 구축하는 방법이지만, 설명변수가 많은 경우에는 지나치게 많은 변수가 모형에 추가될 수 있어서 분석에 큰 부담이 될 수 있다. 반드시 필요한 상호작용 효과를 선별해서 추가하는 것이 바람직하다. 범주형 변수가 3개의 범주를 갖는 경우에는 2개의 가변수가 필요하게 된다. 예를 들어 “high”, “medium”, “low”의 3가지 범주를 갖는 범주형 변수와 연속형 변수 \\(X\\) 를 설명변수로 하는 회귀모형은 다음과 같은 형태를 갖게 된다. 반응변수 \\(Y\\) 와 설병변수 \\(X\\) 가 세 범주에 동일한 관계를 갖고 있다고 가정하자. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X + \\beta_{2}D_{1} + \\beta_{3}D_{2} + \\varepsilon \\end{equation}\\] 단, \\(D_{1}\\) 은 범주가 “high”이면 1, 아니면 0이고, \\(D_{2}\\) 는 범주가 “medium”이면 1, 아니면 0이 되는 가변수이다. 이 경우, 기준범주는 “low”가 되며, \\(\\beta_{2}\\) 는 기준범주와 “high” 범주의 효과 차이를, \\(\\beta_{3}\\) 은 기준범주와 “medium” 범주의 효과 차이를 나타내고 있다. \\(\\bullet\\) 예제: mtcars 반응변수는 자동차 연비인 mpg로 하고, 설명변수로는 1/4 마일 도착 시간인 qsec, 변속기 종류를 나타내는 am과 실린더 개수를 나타내는 cyl을 사용하자. 변수 am과 cyl은 범주형 변수이지만 숫자로 입력된 상태이다. str(mtcars) ## &#39;data.frame&#39;: 32 obs. of 11 variables: ## $ mpg : num 21 21 22.8 21.4 18.7 18.1 14.3 24.4 22.8 19.2 ... ## $ cyl : num 6 6 4 6 8 6 8 4 4 6 ... ## $ disp: num 160 160 108 258 360 ... ## $ hp : num 110 110 93 110 175 105 245 62 95 123 ... ## $ drat: num 3.9 3.9 3.85 3.08 3.15 2.76 3.21 3.69 3.92 3.92 ... ## $ wt : num 2.62 2.88 2.32 3.21 3.44 ... ## $ qsec: num 16.5 17 18.6 19.4 17 ... ## $ vs : num 0 0 1 1 0 1 0 1 1 1 ... ## $ am : num 1 1 1 0 0 0 0 0 0 0 ... ## $ gear: num 4 4 4 3 3 3 3 4 4 4 ... ## $ carb: num 4 4 1 1 2 1 4 2 2 4 ... 함수 lm()은 요인으로 설명변수를 입력하면 자동으로 필요한 개수의 가변수를 생성한다. 따라서 변수 am과 cyl은 요인으로 유형을 변경하는 것이 필요하다. mtcars_3 &lt;- mtcars |&gt; mutate(am = factor(am), cyl = factor(cyl)) 요인으로 변경된 두 변수의 범주별 도수를 확인해 보자. mtcars_3 |&gt; count(am) ## am n ## 1 0 19 ## 2 1 13 mtcars_3 |&gt; count(cyl) ## cyl n ## 1 4 11 ## 2 6 7 ## 3 8 14 두 개의 범주를 갖고 있는 am과 숫자형 변수 qsec을 설명변수로 하는 회귀모형을 적합해 보자. fit_m1 &lt;- lm(mpg ~ qsec + am, data = mtcars_3) 적합 결과는 다음과 같다. fit_m1 ## ## Call: ## lm(formula = mpg ~ qsec + am, data = mtcars_3) ## ## Coefficients: ## (Intercept) qsec am1 ## -18.889 1.982 8.876 적합된 회귀모형을 그래프로 표현하면 그림 4.5과 같이 두 개의 평행한 회귀직선으로 표현된다. 그림 4.5: 모형 fit_m1의 적합 결과 변수 am의 두 범주 중 기준 범주는 am = 0인 범주이며, 모형 fit_m1에서 사용된 가변수 am1은 두 범주의 효과 차이를 나타내고 있는데, 회귀계수가 8.876으로 계산된 것은 am = 0 범주보다 am = 1 범주에서 반응변수의 평균이 8.876 높다고 추정된 것이다. 또한 적합된 두 직선이 평행한 것은 변수 qsec와 mpg의 관계가 am = 0 범주와 am = 1 범주에서 동일하다고 가정했기 때문이다. 동일한 관계를 유지한다는 가정을 하지 않은 상태에서 회귀모형을 적합한 결과는 그림 4.6와 같다. 어떤 모형을 사용하는 것이 더 적절하다고 생각하는가? 이 문제는 회귀모형의 추론을 통해 결정해야 하는 문제가 된다. 그림 4.6에 표시된 회귀모형에는 변수 qsec과 am의 상호작용 항이 포함되었으며, 자세한 설명은 4.2절에서 살펴보겠다. 그림 4.6: 변수 qsec과 am의 상호작용 항이 포함된 회귀모형의 적합 결과 이번에는 세 개의 범주를 갖고 있는 cyl과 숫자형 변수 qsec을 설명변수로 하는 회귀모형을 적합해 보자. fit_m2 &lt;- lm(mpg ~ qsec + cyl, data = mtcars_3) fit_m2 ## ## Call: ## lm(formula = mpg ~ qsec + cyl, data = mtcars_3) ## ## Coefficients: ## (Intercept) qsec cyl6 cyl8 ## 35.0724 -0.4394 -7.4305 -12.6029 적합된 모형은 다음과 같이 그래프로 표현된다. 그림 4.7: 모형 fit_m2의 적합 결과 회귀모형 fit_m2에서 cyl의 기준범주는 cyl = 4인 범주이며, 회귀계수의 추정 결과로 기준범주에서 반응변수의 평균이 cyl = 6인 범주보다 7.43 높고, cyl = 8인 범주보다 12.6 높음을 알 수 있다. 4.2 다중회귀모형의 추론 4.1절에서 우리는 다항회귀모형과 가변수 회귀모형 등 몇 가지 형태의 다중회귀모형을 살펴보았고, 최소제곱추정량에 의한 회귀계수의 추정 방법도 살펴보았다. 지금부터는 회귀계수의 추론에 대해 살펴보도록 하자. 반응변수와 설명변수에 대해 자료가 관측되면 회귀모형에 대한 다양한 추론을 할 수 있는데, 이러한 추론을 진행하기 위해서는 회귀계수의 추정량에 대한 표본분포를 알아야 한다. 4.2.1 회귀계수 추정량의 표본분포 다중회귀모형은 다음과 같이 행렬의 형태로 나타내는 것이 일반적인 표현 방식이 된다. \\[\\begin{equation} \\mathbf{Y} = \\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon} \\end{equation}\\] 반응변수 벡터 \\(\\mathbf{Y}\\) 와 설명변수 관찰값 행렬 \\(\\mathbf{X}\\), 모수 벡터 \\(\\boldsymbol{\\beta}\\), 오차항 벡터 \\(\\boldsymbol{\\varepsilon}\\) 의 정의는 식 (4.2) 에서 볼 수 있다. 또한 오차항 \\(\\varepsilon_{i}, ~i=1,\\ldots,n\\) 이 모두 평균이 \\(0\\), 분산이 \\(\\sigma^{2}\\) 이며, 서로 독립이라는 가정도 다음과 같이 행렬로 표현할 수 있다. \\[ E(\\boldsymbol{\\varepsilon}) = \\begin{pmatrix} 0 \\\\ 0 \\\\ \\vdots \\\\ 0 \\end{pmatrix} \\] \\[ Var(\\boldsymbol{\\varepsilon}) = \\begin{pmatrix} \\sigma^{2} &amp; 0 &amp; \\cdots &amp; 0 \\\\ 0 &amp; \\sigma^{2} &amp; \\cdots &amp; 0 \\\\ \\vdots &amp; \\vdots &amp; \\ddots &amp; \\vdots \\\\ 0 &amp; 0 &amp; \\cdots &amp; \\sigma^{2} \\end{pmatrix}= \\sigma^{2}~\\mathbf{I} \\] 단, \\(\\mathbf{I}\\) 는 \\(n\\) 차 단위행렬이다. 오차항 벡터의 가정을 기반으로 반응변수 벡터 \\(\\mathbf{Y}\\) 의 평균과 분산은 다음과 같이 구할 수 있으며, \\[\\begin{equation} E(\\mathbf{Y}) = E(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = E(\\mathbf{X}\\boldsymbol{\\beta}) = \\mathbf{X}\\boldsymbol{\\beta} \\end{equation}\\] \\[\\begin{equation} Var(\\mathbf{Y}) = Var(\\mathbf{X}\\boldsymbol{\\beta} + \\boldsymbol{\\varepsilon}) = Var(\\boldsymbol{\\varepsilon}) = \\sigma^{2}~\\mathbf{I} \\end{equation}\\] 오차항이 정규분포한다는 가정을 추가하면 반응변수 벡터 \\(\\mathbf{Y}\\) 의 분포는 다음과 같다. \\[\\begin{equation} \\mathbf{Y} \\sim N(\\boldsymbol{\\beta},~\\sigma^{2}\\mathbf{I}) \\end{equation}\\] 이제 모수 벡터 \\(\\boldsymbol{\\beta}\\) 의 최소제곱추정량 \\(\\hat{\\boldsymbol{\\beta}} = (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{Y}\\) 의 표본분포를 알아보자. 먼저 추정량 벡터의 평균은 다음과 같이 구할 수 있다. \\[\\begin{align*} E(\\hat{\\boldsymbol{\\beta}}) &amp;= E((\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{Y}) \\\\ &amp;= (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}E(\\mathbf{Y}) \\\\ &amp;= (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{X}\\boldsymbol{\\beta} \\\\ &amp;= \\boldsymbol{\\beta} \\end{align*}\\] 추정량 벡터의 분산은 다음과 같이 구할 수 있다. \\[\\begin{align*} Var(\\hat{\\boldsymbol{\\beta}}) &amp;= Var((\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}\\mathbf{Y}) \\\\ &amp;= (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}~Var(\\mathbf{Y})~ ((\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T})^{T} \\\\ &amp;= (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T}~\\sigma^{2}\\mathbf{I}~ ((\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T})^{T} \\\\ &amp;= \\sigma^{2} (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\mathbf{X}^{T} \\mathbf{X} (\\mathbf{X}^{T}\\mathbf{X})^{-1} \\\\ &amp;= \\sigma^{2} (\\mathbf{X}^{T}\\mathbf{X})^{-1} \\end{align*}\\] 또한 \\(\\hat{\\boldsymbol{\\beta}}\\) 은 반응변수 벡터 \\(\\mathbf{Y}\\) 의 선형결합으로 표시되기 때문에 \\(\\mathbf{Y}\\) 와 같은 정규분포를 하게 된다. 따라서 추정량 벡터의 표본분포는 다음과 같이 주어진다. \\[\\begin{equation} \\hat{\\boldsymbol{\\beta}} \\sim N\\left(\\boldsymbol{\\beta},~\\sigma^{2} (\\mathbf{X}^{T}\\mathbf{X})^{-1}\\right) \\tag{4.15} \\end{equation}\\] 또한 개별 추정량의 분포는 다음과 같이 주어진다. \\[\\begin{equation} \\hat{\\beta}_{j} \\sim N(\\beta_{j},~c_{j+1,j+1}~\\sigma^{2}),~~j=0,1,\\ldots,k \\tag{4.16} \\end{equation}\\] 단, \\(c_{i, j}\\)은 \\((\\mathbf{X}^{T}\\mathbf{X})^{-1}\\) 행렬의 \\(i\\) 번째 행, \\(j\\) 번째 열 원소를 나타낸다. 식 (4.15)에 주어진 추정량 벡터의 표본분포를 이용해서 추론을 실시하기 위해서는 오차항 분산인 \\(\\sigma^{2}\\) 에 대한 추정값이 반드시 필요하다. 2.3절에서 단순회귀모형의 경우에 적용되는 오차항 분산의 추정량 \\(MSE\\) 가 식 (2.17)으로 유도되는 것을 살펴보았다. 단순회귀모형의 경우 잔차제곱합 \\(RSS\\)의 자유도가 \\((n-2)\\) 가 된 이유는 \\(\\widehat{Y}_{i}\\) 을 얻기 위해 추정해야할 모수의 개수가 2개이기 때문이다. 다중회귀모형에서는 추정해야 할 모수가 \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k}\\) 의 \\((k+1)\\) 개가 있고, 따라서 \\(RSS\\)의 자유도는 \\((n-k-1)\\) 이 되어서 \\(\\sigma^{2}\\) 의 불편추정량 \\(MSE\\) 는 다음과 같이 주어진다. \\[\\begin{equation} \\hat{\\sigma}^{2} = \\frac{RSS}{n-k-1} = MSE \\tag{4.17} \\end{equation}\\] 식 (4.17)의 \\(MSE\\) 는 \\(k=1\\) 인 단순회귀모형의 경우도 포괄하는 일반적인 정의가 된다. 4.2.2 회귀모형의 유의성 검정 식 (4.1)의 회귀모형을 반응변수의 변동을 설명할 수 있을 것으로 생각한 모든 설명변수로 구성한 첫 번째 모형이라고 해 보자. 그러면 우리가 가장 먼저 실행할 수 있는 검정은 식 (4.1)의 회귀모형에 포함된 설명변수 중 반응변수의 변동을 설명하는데 유의적인 변수가 적어도 하나라도 있는지 알아보는 것이 될 것이며, 이것을 회귀모형의 유의성 검정이라고 한다. 귀무가설은 다음과 같고, \\[\\begin{equation} H_{0}:\\beta_{1}=\\beta_{2}=\\cdots=\\beta_{k}=0 \\tag{4.18} \\end{equation}\\] 대립가설은 다음과 같다. \\[\\begin{equation} H_{1}: \\text{at least one of }\\beta_{j} \\ne 0 \\end{equation}\\] 가설에 대한 검정통계량은 다음과 같다. \\[\\begin{equation} F = \\frac{(TSS-RSS)/k}{RSS/(n-k-1)} \\end{equation}\\] 귀무가설이 사실일 때 검정통게량의 분포는 \\(F_{k,~n-k-1}\\) 이다. 검정통계량의 구성을 살펴보자. \\(TSS=\\sum(y_{i}-\\overline{y})^{2}\\) 는 반응변수의 총변량으로서 설명변수와는 관련이 없는 변량이다. \\(RSS = \\sum(y_{i}-\\hat{y}_{i})^{2}\\) 는 잔차제곱합으로서 \\(y_{i}\\) 를 \\(\\hat{y}_{i}\\) 으로 예측함으로써 발생한 오차이며, 회귀모형으로 설명이 안 된 변량이라고 할 수 있다. 따라서 총변량에서 설명이 안 된 변량을 제외한 \\((TSS-RSS)\\) 는 회귀모형으로 설명된 변량이라고 할 수 있다. 따라서 검정통계량은 회귀모형에 의해서 설명이 가능한 변량과 설명이 안 되는 변량의 상대적 크기라고 할 수 있으며, 검정통계량의 값이 충분히 크다면 회귀모형으로 반응변수의 변량을 설명할 수 있다는 것을 의미하기 때문에 귀무가설을 기각할 수 있게 된다. 검정통계량의 분모와 분자의 특성을 조금 더 자세하게 살펴보자. 검정통계량의 분모는 \\(RSS\\)를 자신의 자유도 \\((n-k-1)\\) 로 나눈 것으로서, 이 통계량은 회귀모형의 가정이 만족이 되면 귀무가설의 사실 여부와 관계 없이 오차항의 분산인 \\(\\sigma^{2}\\)의 불편추정량이 된다. 즉, 가설의 사실 여부와 관계 없이 다음은 성립된다. \\[\\begin{equation} E(RSS/(n-k-1))=\\sigma^{2} \\tag{4.19} \\end{equation}\\] 이것은 \\(RSS/\\sigma^{2}\\) 이 자유도가 \\((n-k-1)\\) 인 \\(\\chi^{2}\\) 분포를 하기 때문에 성립된다. 반면에, 검정통계량의 분자는 \\((TSS-RSS)\\) 을 자신의 자유도인 \\(k\\) 로 나눈 것으로서, 이 통계량은 \\(H_{0}\\) 이 사실인 경우에는 \\(\\sigma^{2}\\) 의 불편추정량이 되는데, 이것은 \\(H_{0}\\)이 사실인 경우에 \\((TSS-RSS)/\\sigma^{2}\\) 이 자유도가 \\(k\\) 인 \\(\\chi^{2}\\) 분포를 하기 때문이다. 그러나 \\(H_{0}\\) 이 사실이 아닌 경우에는 \\(\\sigma^{2}\\) 을 과대 추정하는 특성이 있다. 따라서 \\(H_{0}\\)이 사실이 아니면 다음과 같이 된다. \\[\\begin{equation} E((TSS-RSS)/k) &gt; \\sigma^{2} \\tag{4.20} \\end{equation}\\] 식 (4.19)와 식 (4.20)에 의해서 만일 검정통계량이 1보다 상당히 큰 값이 되면 \\(H_{0}\\)이 사실이 아닐 가능성이 높게 되는 것을 알 수 있다. \\(H_{0}\\)이 사실인 경우 검정통계량은 \\(\\chi^{2}\\) 분포를 하는 두 통계량의 비율이 되기 때문에 \\(F\\) 분포를 하게 되며, 자유도는 \\((k, ~n-k-1)\\) 를 갖게 된다. 가설 (4.18)이 기각되면 모형에 포함된 설명변수 중 유의한 변수가 있다는 것이므로, 개별 회귀계수에 대한 유의성 검정을 진행할 수 있다. 그러나 만일 가설을 기각할 수 없다면 식 (4.1)의 회귀모형으로는 반응변수의 변동을 제대로 설명할 수 없다는 것이 된다. 이런 결과가 나오면, 다른 설명변수의 조합을 찾아 보거나, 또는 다른 형태의 회귀모형을 시도해 봐야 한다. \\(\\bullet\\) 자유도 2.3절에서 살펴본 제곱합의 자유도를 구하는 두 가지 방법을 적용해서 \\(RSS\\) 와 \\((TSS-RSS)\\) 의 자유도를 구해보자. 먼저 \\(RSS=\\sum(y_{i}-\\widehat{y}_{i})^{2}\\) 의 경우에는 \\(n\\) 개의 \\(y_{i}\\) 가 수집되지만 \\(\\widehat{y}_{i}\\) 를 얻기 위해 \\((k+1)\\) 개 모수 \\(\\beta_{0}, \\beta_{1}, \\ldots, \\beta_{k}\\) 에 대한 추정이 이루어져 \\((k+1)\\) 개의 자유도를 잃게 되어서, 자유도는 \\((n-k-1)\\) 이 된다. \\((TSS-RSS)\\) 의 경우에는 제곱합을 수식으로 표현해야 자유도를 구할 수 있는데, 먼저 \\(TSS=\\sum(y_{i}-\\overline{y})^{2}\\) 를 다음과 같이 분할해 보자. \\[\\begin{align} \\sum_{i=1}^{n}(y_{i}-\\overline{y}_{i})^{2} &amp;= \\sum_{i=1}^{n}(y_{i}-\\widehat{y}_{i}+\\widehat{y}_{i}-\\overline{y})^{2} \\\\ &amp;= \\sum_{i=1}^{n}(\\widehat{y}_{i}-\\overline{y})^{2} + \\sum_{i=1}^{n}(y_{i}-\\widehat{y}_{i})^{2} +2\\sum_{i=1}^{n}(\\widehat{y}_{i}-\\overline{y})(y_{i}-\\widehat{y}_{i}) \\\\ &amp;= \\sum_{i=1}^{n}(\\widehat{y}_{i}-\\overline{y})^{2} + \\sum_{i=1}^{n}(y_{i}-\\widehat{y}_{i})^{2} \\tag{4.21} \\end{align}\\] 마지막 수식은 \\(\\sum_{i=1}^{n}(\\widehat{y}_{i}-\\overline{y})(y_{i}-\\widehat{y}_{i})=0\\) 이기 때문에 성립된다. 식 (4.21)의 결과에서 \\((TSS-RSS)\\) 는 다음과 같이 표현된다. \\[\\begin{equation} TSS-RSS = \\sum_{i=1}^{n}(\\widehat{y}_{i}-\\overline{y})^{2} \\end{equation}\\] 이제 수식으로 표현된 제곱합의 자유도를 구해보자. \\((TSS-RSS)\\) 에는 \\(n\\) 개의 \\(\\widehat{y}_{i}\\) 이 있지만, \\(y_{i}\\) 의 경우와는 다르게 \\(\\widehat{y}_{i}\\) 은 개별적으로 관측되는 것이 아니라 \\((k+1)\\) 개의 회귀계수 추정값인 \\(\\hat{\\beta}_{0}, \\hat{\\beta}_{1}, \\ldots, \\hat{\\beta}_{k}\\) 에 의해서 결정되는 것이 때문에 \\((k+1)\\) 개의 자유도로 시작한다. 또한 \\(\\sum(\\widehat{y}_{i}-\\overline{y})=0\\) 이 성립하기 때문에 하나의 제약조건이 존재하게 되어서, 자유도는 \\((k+1)-1=k\\) 이 된다. 4.2.3 개별 회귀계수 유의성 검정 회귀모형의 유의성 검정에서 적어도 하나의 유의적인 변수가 모형에 있다는 결론이 나면, 어떤 변수가 유의적 변수인지 확인해야 할 것이다. 개별회귀계수 유의성 검정은 식 (4.1)에 정의된 회귀모형을 구성하고 있는 각 설명변수의 유의성을 검정하는 절차이다. 가설은 다음과 같다. \\[\\begin{equation} H_{0}: \\beta_{j} = 0, \\quad H_{1}: \\beta_{j} \\ne 0, \\quad j = 1, \\ldots, k \\tag{4.22} \\end{equation}\\] 검정통계량은 개별 회귀계수의 추정량 \\(\\hat{\\beta}_{j}\\) 를 추정량의 표준오차로 나눈 것이다. \\[\\begin{equation} t = \\frac{\\hat{\\beta}_{j}}{SE(\\hat{\\beta}_{j})} \\end{equation}\\] 단, \\(SE(\\hat{\\beta}_{j}) = \\sqrt{c_{j+1,j+1}~MSE}\\) 이며, \\(c_{j+1, j+1}\\) 은 (4.16)에 정의되어 있다. 귀무가설이 사실일 때 검정통계량의 분포는 \\(t_{n-k-1}\\) 이다. 개별 회귀계수에 대한 검정은 양측검정이 되며, 따라서 회귀계수의 신뢰구간과 밀접한 관련이 있다. 개별 회귀계수에 대한 95% 신뢰구간은 회귀계수와 추정량과 추정량의 표준오차를 이용하여 대략적으로 다음과 같이 표시된다. \\[\\begin{equation} \\hat{\\beta}_{j} \\pm 2 \\cdot SE(\\hat{\\beta}_{j}) \\end{equation}\\] 만일 주어진 자료를 근거로 계산된 \\(\\beta_{j}\\) 의 95% 신뢰구간에 0이 포함되어 있다면, 같은 자료로 계산한 검정통계량의 값은 5% 유의수준으로 구성된 기각역에 들어갈 수 없게 되어서 귀무가설을 기각할 수 없게 된다. 4.2.4 두 회귀모형의 비교 반응변수의 변동을 설명하는데 식 (4.1) 회귀모형의 설명변수를 모두 사용하는 것이 좋은지 아니면 일부분만을 사용하는 것이 더 좋은지 비교하는 절차이다. 비교하는 두 모형은 확장모형( \\(\\Omega\\) )인 모형 (4.1)과 특정 \\(q\\) 개의 회귀계수에 대한 다음의 가설이 사실인 축소모형( \\(\\omega\\) )이다. \\[\\begin{equation} H_{0}: \\beta_{k-q+1}=\\beta_{k-q+2}=\\cdots=\\beta_{k}=0 \\tag{4.23} \\end{equation}\\] 두 모형의 설명력 차이가 크지 않다면 ’모수절약의 원칙’에 의하여 축소모형을 선택하는 것이 더 좋을 것이다. 모형의 설명력 비교는 잔차제곱합을 이용할 수 있는데, 만일 확장모형의 잔차제곱합, \\(RSS_{\\Omega}\\) 와 축소모형의 잔차제곱합, \\(RSS_{\\omega}\\) 의 차이가 크지 않다면, 축소모형의 설명력이 확장모형 만큼 좋다는 의미가 된다. 검정통계량은 다음과 같이 구성된다. \\[\\begin{equation} F=\\frac{(RSS_{\\omega}-RSS_{\\Omega})/q}{RSS_{\\Omega}/(n-k-1)} \\end{equation}\\] 귀무가설이 사실일 때 검정통게량의 분포는 \\(F_{q,~n-k-1}\\) 이다. \\(\\bullet\\) 예제: mtcars 4.1.1절에서 다루었던 mtcars 자료에 대한 분석을 다시 실행해 보자. 반응변수는 mpg이고, 설명변수로는 gear와 carb를 제외한 나머지 변수를 사용하며, cyl, vs, am은 요인으로 변경하자. mtcars_4 &lt;- mtcars |&gt; select(!c(gear, carb)) |&gt; mutate(across(c(cyl, vs, am), as.factor)) mtcars_4를 사용해서 회귀모형 fit_m3를 적합해 보자. fit_m3 &lt;- lm(mpg ~ ., mtcars_4) 이제 lm()으로 생성된 객체 fit_m3를 대상으로 회귀모형 추론을 위한 몇 가지 함수를 적용해 보자. 먼저 가장 빈번하게 사용되는 함수인 summary()의 결과를 확인해 보자. summary(fit_m3) ## ## Call: ## lm(formula = mpg ~ ., data = mtcars_4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.9978 -1.3551 -0.3108 1.1992 4.1102 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.865583 14.801183 1.342 0.1932 ## cyl6 -1.458247 1.983190 -0.735 0.4699 ## cyl8 0.484450 3.910064 0.124 0.9025 ## disp 0.006688 0.013512 0.495 0.6255 ## hp -0.029141 0.017182 -1.696 0.1040 ## drat 0.588059 1.503111 0.391 0.6994 ## wt -3.155246 1.420235 -2.222 0.0369 * ## qsec 0.523235 0.690130 0.758 0.4564 ## vs1 1.237800 2.106056 0.588 0.5627 ## am1 3.000910 1.853400 1.619 0.1197 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.514 on 22 degrees of freedom ## Multiple R-squared: 0.8765, Adjusted R-squared: 0.826 ## F-statistic: 17.35 on 9 and 22 DF, p-value: 4.814e-08 함수 lm()으로 생성된 객체에 함수 summary()를 적용시켜 얻은 결과물은 SAS나 SPSS에서 볼 수 있는 결과물과는 형식에서 차이가 있으나 많은 정보를 매우 효과적으로 보여주는 방식이라고 할 수 있다. 결과물을 하나씩 살펴보자. 먼저 Residuals:에는 잔차의 분포를 엿볼 수 있는 요약통계가 계산되어 있다. 가정이 만족된다면 잔차는 평균이 0인 정규분포를 보이게 되는데, 잔차의 요약 통계 결과 값으로 대략적인 판단을 할 수 있다. Coefficients:에는 회귀계수의 추정값과 표준오차가 계산되어 있다. 또한 개별 회귀계수의 유의성 검정인 \\(H_{0}:\\beta_{j} = 0, \\quad H_{1}: \\beta_{j} \\ne 0\\) 에 대한 검정통계량의 값과 p-값이 계산되어 있다 Residual standard error:는 \\(\\sqrt{MSE}\\) 이며, Multiple R-squared:는 결정계수 \\(R^{2}\\) 이고, Adjusted R-squared:는 수정결정계수의 값으로서, 모두 회귀모형의 평가 측도로 사용된다. 회귀모형의 평가측도에 대해서는 4.3절에서 살펴보겠다. F-statistic:은 모든 회귀계수가 0이라는 가설, 즉 \\(H_{0}:\\beta_{1}=\\beta_{2}=\\cdots=\\beta_{k}=0\\) 에 대한 검정통계량의 값과 자유도, 그리고 p-값이 계산되어 있다. 함수 summary()로 얻어지는 결과물로 회귀모형에 대한 중요한 추론이 가능함을 알 수 있다. 한 가지 SAS나 SPSS에 익숙한 사용자들에게 아쉬울 수 있는 점은 아마도 회귀모형의 분산분석표가 출력되지 않는다는 것일 텐데, 이것은 함수 anova()를 사용함으로써 해결할 수 있다. anova(fit_m3) ## Analysis of Variance Table ## ## Response: mpg ## Df Sum Sq Mean Sq F value Pr(&gt;F) ## cyl 2 824.78 412.39 65.2599 5.624e-10 *** ## disp 1 57.64 57.64 9.1218 0.006292 ** ## hp 1 18.50 18.50 2.9279 0.101125 ## drat 1 11.91 11.91 1.8854 0.183553 ## wt 1 55.79 55.79 8.8281 0.007049 ** ## qsec 1 1.52 1.52 0.2413 0.628165 ## vs 1 0.30 0.30 0.0478 0.828944 ## am 1 16.57 16.57 2.6216 0.119666 ## Residuals 22 139.02 6.32 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 함수 anova()는 두 회귀모형을 비교할 때에도 사용되는 함수이다. 비교되는 두 모형은 확장모형과 확장모형의 부분집합인 축소모형이어야 한다. 확장모형의 부분집합이라는 것은 확장모형의 일부 회귀계수가 0이 되는 모형을 의미하는 것으로써, 확장모형에 없는 설명변수가 축소모형에 포함되면 정상적인 비교가 이루어질 수 없다. mtcars_4 자료에서 모든 설명변수가 포함된 확장모형 fit_m3와 변수 cyl, vs, drat를 제외한 축소모형 fit_m4를 비교해 보자. fit_m4 &lt;- update(fit_m3, . ~ . - cyl - vs - drat) 축소모형 fit_m4는 확장모형 fit_m3의 적합 내용을 변경한 것인데, 이런 경우에 유용하게 사용되는 함수가 update()이며, 사용법은 다음과 같다. update(object, formula) object는 lm()로 생성된 객체이고, formula는 object의 적합 내용 중 변경되는 모형의 공식을 지정하는 것이다. 모형 fit_m4에 적용된 모형공식은 . ~ . - cyl - vs - drat인데, 이것은 fit_m3에 적용된 모형공식 중 설명변수에서 cyl, vs, drat를 제거하는 것을 의미한다. 두 회귀모형 fit_m3와 fit_m4의 비교를 함수 anova()로 진행하자. 함수 anova()에 의한 비교 방법은 anova(축소모형, 확장모형)으로 지정되어야 한다. anova(fit_m4, fit_m3) ## Analysis of Variance Table ## ## Model 1: mpg ~ disp + hp + wt + qsec + am ## Model 2: mpg ~ cyl + disp + hp + drat + wt + qsec + vs + am ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 26 153.44 ## 2 22 139.02 4 14.415 0.5703 0.6869 함수 anova()로 검정이 실시된 귀무가설은 변수 cyl, vs, drat의 회귀계수가 0이라는 것인데, p-값이 0.687으로 계산되어 귀무가설을 기각할 수 없게 되었다. 두 모형의 설명력에는 차이가 없다는 것이므로, 모수 절약의 원칙에 따라 축소모형을 선택하는 것이 더 좋다고 하겠다. 다중회귀모형에서 회귀계수의 유의성은 모형에 포함된 설명변수의 조합에 따라 달라질 수 있음에 유의해야 한다. 모형 fit_m3가 아닌 fit_m4를 선택했다는 것이 제외된 변수인 cyl, vs, drat가 항상 비유의적인 변수라는 것을 의미하는 것은 아니다. lm(mpg ~ drat, mtcars_4) |&gt; summary() ## ## Call: ## lm(formula = mpg ~ drat, data = mtcars_4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -9.0775 -2.6803 -0.2095 2.2976 9.0225 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.525 5.477 -1.374 0.18 ## drat 7.678 1.507 5.096 1.78e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 4.485 on 30 degrees of freedom ## Multiple R-squared: 0.464, Adjusted R-squared: 0.4461 ## F-statistic: 25.97 on 1 and 30 DF, p-value: 1.776e-05 변수 drat만 사용한 회귀모형에서 drat는 유의적인 변수이고, lm(mpg ~ drat + cyl, mtcars_4) |&gt; summary() ## ## Call: ## lm(formula = mpg ~ drat + cyl, data = mtcars_4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -5.3337 -1.8212 -0.1335 1.7382 6.9691 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 19.366 6.219 3.114 0.00423 ** ## drat 1.793 1.509 1.188 0.24488 ## cyl6 -6.051 1.712 -3.535 0.00144 ** ## cyl8 -10.055 1.810 -5.555 6.09e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.201 on 28 degrees of freedom ## Multiple R-squared: 0.7453, Adjusted R-squared: 0.718 ## F-statistic: 27.31 on 3 and 28 DF, p-value: 1.83e-08 변수 drat와 cyl을 함께 사용한 모형에서는 drat가 비유의적인 변수가 되었음을 알 수 있다. 회귀계수의 신뢰구간은 함수 confint()로 계산할 수 있다. 95% 신뢰구간이 디폴트로 계산되며, 만일 신뢰수준을 변경하고자 한다면 옵션 level에 원하는 신뢰수준을 입력하면 된다. 객체 fit1에 포함되어 있는 회귀계수의 95% 신뢰구간은 다음과 같이 계산된다. confint(fit_m4) ## 2.5 % 97.5 % ## (Intercept) -5.66058661 34.384394537 ## disp -0.01055781 0.033033109 ## hp -0.05098537 0.008644273 ## wt -6.53883919 -1.629824922 ## qsec 0.02963058 1.984163085 ## am1 0.41638869 6.524518102 회귀계수의 95% 신뢰구간에 0이 포함됐다는 것은 5% 유의수준에서 가설 \\(H_{0}:\\beta_{j}=0, \\quad H_{1}:\\beta_{j} \\ne 0\\) 의 귀무가설을 기각할 수 없음을 의미한다. 따라서 95% 신뢰구간에 0이 포함된 변수 disp, hp는 5% 유의수준에서 비유적인 변수인 것이다. \\(\\bullet\\) 가변수 회귀모형 : 가변수와 연속형 변수의 상호작용 항 포함 여부 설명변수 중 범주형 변수가 있다면, 그 변수에 대한 가변수를 사용해야 하는데, 이 경우에 모형에 포함된 숫자형 설명변수와 반응변수의 관계가 범주형 변수의 모든 범주에서 동일하다고 가정하는 식 (4.13)의 모형을 사용할 수 있다. mtcars_4 자료에서 반응변수 mpg와 설명변수 qsec, am의 회귀모형을 다음과 같이 적합한다면, am의 두 범주에서 mpg와 qsec의 기울기가 동일하다고 가정한 것이 된다. fit_m5 &lt;- lm(mpg ~ qsec + am, mtcars_4) 모형 fit_m5의 적합 결과에 대한 그래프는 그림 4.5에서 볼 수 있다. 그러나 만일 숫자형 설명변수와 반응변수의 관계가 각 범주에서 다르다고 판단이 된다면, 식 (4.14)의 모형을 선택해서, 가변수와 연속형 변수의 상호작용 항을 포함시켜야 한다. fit_m6 &lt;- lm(mpg ~ qsec * am, mtcars_4) 모형 fit_m6의 모형 공식에 사용된 * 기호는 두 변수의 주 효과와 상호작용 효과를 모두 포함시키는 작용을 한다. 모형 fit_m6의 적합 결과에 대한 그래프는 그림 4.6에서 볼 수 있다. 두 모형 중 어떤 모형을 선택할 것인지는 모형 fit_m6에 포함된 상호작용 항의 유의성 여부로 결정할 수 있으나, 모형 수립 목적에 부합한 평가 기준을 사용하는 것이 더 좋은 방법이라고 하겠다. summary(fit_m6) ## ## Call: ## lm(formula = mpg ~ qsec * am, data = mtcars_4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -6.4551 -1.4331 0.1918 2.2493 7.2773 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -9.0099 8.2179 -1.096 0.28226 ## qsec 1.4385 0.4500 3.197 0.00343 ** ## am1 -14.5107 12.4812 -1.163 0.25481 ## qsec:am1 1.3214 0.7017 1.883 0.07012 . ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 3.343 on 28 degrees of freedom ## Multiple R-squared: 0.722, Adjusted R-squared: 0.6923 ## F-statistic: 24.24 on 3 and 28 DF, p-value: 6.129e-08 \\(\\bullet\\) 예제 : modeldata::crickets 패키지 modeldata의 데이터 프레임 crickets에는 기온과 귀뚜라미 울음소리 횟수의 관계를 탐색하기 위해 관측한 자료가 입력되어 있다. 변수 temp는 기온, rate는 귀뚜라미의 분당 울음소리 횟수이며, species는 귀뚜라미 종류이다. data(crickets, package = &quot;modeldata&quot;) str(crickets) ## tibble [31 × 3] (S3: tbl_df/tbl/data.frame) ## $ species: Factor w/ 2 levels &quot;O. exclamationis&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ temp : num [1:31] 20.8 20.8 24 24 24 24 26.2 26.2 26.2 26.2 ... ## $ rate : num [1:31] 67.9 65.1 77.3 78.7 79.4 80.4 85.8 86.6 87.5 89.1 ... 세 변수의 관계 탐색을 위한 그래프를 작성해 보자. 변수 temp와 rate의 단순 산점도와 species의 효과를 살펴볼 수 있는 산점도를 모두 작성해 보자. library(patchwork) p1 &lt;- ggplot(crickets, aes(x = temp, y = rate)) + geom_point() p2 &lt;- ggplot(crickets, aes(x = temp, y = rate, color = species)) + geom_point() p1 + p2 그림 4.8: crickets 자료의 세 변수 관계 탐색 변수 temp와 rate 사이에는 명확한 양의 관계가 있는데, species의 두 범주 사이에는 효과 차이가 있는 것으로 보인다. 가변수를 사용한 선형회귀모형을 사용하는 것이 적절한 것으로 보이는데, 두 범주에서 변수 temp와 rate의 관계가 동일한지 여부를 확인하기 위해서 그림 4.8의 오른쪽 그래프에 회귀직선을 추가해 보자. ggplot(crickets, aes(x = temp, y = rate, color = species)) + geom_point() + geom_smooth(method = &quot;lm&quot;, se = FALSE) + labs(x = &quot;Temperature (C)&quot;, y = &quot;Chirp Rate (per minute)&quot;) 그림 4.9: crickets 자료의 세 변수 관계 탐색 그림 4.9에서 두 직선의 기울기는 큰 차이가 없는 것으로 보인다. 따라서 temp와 species의 상호작용 효과는 필요 없는 것으로 보이는데, 이것을 검정을 통해 확인해 보자. 식 (4.13)의 모형으로 fit.main을 적합하고, 식 (4.14)의 모형으로 fit.inter을 적합해 보자. fit.main &lt;- lm(rate ~ temp + species, data = crickets) fit.inter &lt;- lm(rate ~ temp * species, data = crickets) 모형 fit.main은 fit.inter에서 상호작용 효과만 제외된 모형으므로 fit.inter의 축소모형이 된다. 따라서 상호작용 효과의 유의성 여부는 함수 anova()를 사용해서 두 모형을 비교해서 획인할 수 있다. anova(fit.main, fit.inter) ## Analysis of Variance Table ## ## Model 1: rate ~ temp + species ## Model 2: rate ~ temp * species ## Res.Df RSS Df Sum of Sq F Pr(&gt;F) ## 1 28 89.350 ## 2 27 85.074 1 4.2758 1.357 0.2542 검정 결과 p-값이 0.2542로 계산되어서, 두 모형의 설명력에는 차이가 없는 것으로 볼 수 있다. 따라서 축소모형인 fit.main을 선택하게 되고, 상호작용 효과는 비유의적이라고 결론내릴 수 있다. 물론 모형 fit.inter의 개별회귀계수 검정으로도 동일한 가설을 검정할 수 있다. summary(fit.inter)$coef ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -11.0408481 4.1514800 -2.6594969 1.300079e-02 ## temp 3.7514472 0.1601220 23.4286850 1.780831e-19 ## speciesO. niveus -4.3484072 4.9616805 -0.8763981 3.885447e-01 ## temp:speciesO. niveus -0.2339856 0.2008622 -1.1649059 2.542464e-01 모형 fit.main의 적합 결과를 확인해 보자. summary(fit.main) ## ## Call: ## lm(formula = rate ~ temp + species, data = crickets) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.0128 -1.1296 -0.3912 0.9650 3.7800 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) -7.21091 2.55094 -2.827 0.00858 ** ## temp 3.60275 0.09729 37.032 &lt; 2e-16 *** ## speciesO. niveus -10.06529 0.73526 -13.689 6.27e-14 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.786 on 28 degrees of freedom ## Multiple R-squared: 0.9896, Adjusted R-squared: 0.9888 ## F-statistic: 1331 on 2 and 28 DF, p-value: &lt; 2.2e-16 4.3 변수선택 반응변수 \\(Y\\) 에 대한 다음의 회귀모형에는 모든 가능한 설명변수가 다 포함되어 있다고 가정하고, 그런 의미에서 완전모형 또는 full model이라고 하자. \\[\\begin{equation} Y = \\beta_{0} + \\beta_{1}X_{1} + \\cdots + \\beta_{k}X_{k} + \\varepsilon \\tag{4.24} \\end{equation}\\] 변수선택이란 반응변수의 변동을 설명할 수 있는 많은 설명변수들 중에서 ’최적’의 조합을 찾는 절차를 의미한다. 변수선택이 필요한 이유로는 모형에 불필요하게 많은 설명변수가 포함이 되면 추정 및 예측 결과에 불확실성이 증가하는 문제가 발생하기 때문이다. 정밀한 추정 및 예측을 하기 위해서는 가능한 모형을 단순하게 만들어야 한다. 또한 많은 설명변수를 포함시키다 보면 비슷한 성질의 변수가 함께 들어갈 가능성이 있는데, 그렇게 되면 다중공선성의 문제가 발생할 수 있게 된다. 적합된 모형에 대한 해석을 쉽게 하기 위해서도 가능한 단순한 모형이 필요하다고 하겠다. 변수선택 방법은 크게 세 가지로 구분할 수 있다. 첫 번째는 검정에 의하여 단계적으로 변수를 선택하는 방법이고, 두 번째는 회귀모형의 평가 측도를 근거로 변수를 선택하는 방법이며, 세 번째는 shrinkage 방법이다. 검정에 의한 변수선택은 SAS나 SPSS 등에서 일반적으로 이루어지는 방법으로서 후진소거법, 전진선택법과 단계별 선택법이 있다. 변수 선택 과정에서 요구되는 계산이 방대하지 않기 때문에 대규모의 설명변수가 있는 경우 손쉽게 중요 변수를 선택할 수 있다는 장점이 있는 방법이지만 변수의 선택과 제거가 ‘한 번에 하나씩’ 이루어지기 때문에 이른바 ‘최적’ 모형을 놓치는 경우가 발생할 수도 있다. 또한 각 단계마다 다중검정이 이루어지기 때문에 일종 오류를 범할 확률이 증가하는 검정의 정당성 문제가 발생할 수도 있다. 그리고 모형의 수립 목적이 예측인 상황에서는 변수선택 과정이 목적과 어울리지 않는다는 문제도 지니고 있는 등 많은 문제점이 있기 때문에 최근에는 거의 사용되지 않는 방법이다. 따라서 이 책에서는 두 번째와 세 번째 변수선택 방법만을 살펴보겠다. 두 번째와 세 번째 변수선택 방법을 알아보기 전에 회귀모형의 평가 측도에 대해 먼저 살펴보도록 하자. 4.3.1 회귀모형의 평가 측도 회귀모형을 적합하는 데는 나름의 목적이 있기 마련이다. 따라서 적합된 회귀모형이 그 목적에 부합한지를 적절하게 평가할 수 있는 측도가 필요하다. 결정계수( \\(R^{2}\\) ) 결정계수는 반응변수의 총변량 중 회귀모형으로 설명된 변량의 비율을 의미한다. \\[\\begin{equation} R^{2} = \\frac{TSS-RSS}{TSS} = 1-\\frac{RSS}{TSS} \\end{equation}\\] 단순회귀모형에서 결정계수는 \\(Y\\) 와 \\(X\\) 의 상관계수의 제곱과 같고, 다중회귀모형에서는 \\(Y\\) 와 \\(\\widehat{Y}\\) 의 상관계수의 제곱과 같다. 반응변수의 변량을 충분히 설명하는 것이 주된 목적이라면 결정계수를 모형의 평가 측도로 사용하는 것이 좋을 것이다. 다만, 결정계수는 회귀모형에 설명변수가 추가되면 무조건 증가하는 특성이 있기 때문에 설명변수의 개수가 서로 다른 회귀모형을 비교하는 평가 측도로는 적절하지 않으며, 반드시 설명변수의 개수가 같은 회귀모형을 비교할 때 사용해야 한다. 수정결정계수(\\(adj~R^{2}\\) ) 회귀모형에 설명변수가 추가되면 무조건 값이 증가하는 결정계수의 문제를 보완한 측도이다. 추가된 설명변수가 모형의 설명력 향상에 도움이 되는 경우에만 값이 증가되도록 결정계수를 수정한 측도이다. \\[\\begin{equation} adj~R^{2}=1-\\frac{RSS/(n-k-1)}{TSS/(n-1)} = 1-\\left( \\frac{n-1}{n-k-1} \\right)(1-R^{2}) \\end{equation}\\] Residual standard error(RSE) 잔차제곱합, \\(RSS\\) 는 반응변수의 총변량 중에 회귀모형으로 설명이 되지 않는 변량으로서, 결정계수의 값을 결정하는 요소이며, 회귀모형에 설명변수가 추가되면 무조건 감소하는 특성을 가지고 있다. 이러한 특성은 다음과 같이 잔차제곱합을 자신의 자유도로 나누면 수정이 된다. \\[\\begin{equation} RSE = \\sqrt{\\frac{RSS}{n-k-1}} \\tag{4.25} \\end{equation}\\] 수정결정계수와 실질적으로 동일한 특성을 가지고 있는 측도로서, 추가된 설명변수가 모형의 설명력 향상에 도움이 되는 경우에만 값이 감소하는 특성을 가지고 있다. Information criteria 회귀모형에서 오차항이 정규분포를 한다는 가정을 하고 있기 때문에 Likelihood function을 구성할 수 있는데, 회귀모형에 설명변수를 추가함으로써 모수의 개수가 증가하게 되면 Maximum likelihood의 값도 자연스럽게 증가하게 된다. 이것은 결정계수의 특성과 유사한 것이며, 따라서 설명변수 증가에 대한 penalty를 부과하는 측도를 생각해 볼 수 있다. AIC와 BIC는 다음과 같이 정의된다. \\[\\begin{align*} AIC &amp;= -2\\log \\hat{L} + 2K \\\\ BIC &amp;= -2\\log \\hat{L} + k\\log n \\end{align*}\\] 단, \\(\\hat{L}\\) 은 maximum likelihood function이고, \\(n\\) 은 자료의 크기, \\(k\\) 는 설명변수의 개수이다. 회귀모형에 설명변수가 추가되면, \\(\\hat{L}\\) 의 값은 증가하게 되고, 따라서 \\(-2\\log \\hat{L}\\) 은 감소한다. 설명변수 증가에 대한 penalty로써 AIC는 \\(2k\\)를, BIC는 \\(k \\log n\\)를 사용하고 있는데, 설명변수를 추가함으로써 증가된 penalty의 값보다 모형의 적합도 향상으로 감소된 \\(-2 \\log \\hat{L}\\) 의 값이 더 크다면 AIC나 BIC는 감소하게 된다. 따라서 추가된 설명변수가 모형의 적합도 향상에 도움이 되는 경우에만 값이 감소하는 특성을 가지고 있다. \\(C_{p}\\) 통계량 정확한 예측 결과가 중요한 평가 기준이라면 다음에 주어진 예측오차제곱합을 기준으로 사용할 수 있을 것이며, \\[\\begin{equation} \\frac{1}{\\sigma^{2}}\\sum_{i} E\\left(\\widehat{Y}_{i}-E(Y_{i})\\right)^{2} \\end{equation}\\] 추정은 \\(C_{p}\\) 통계량으로 할 수 있다. \\[\\begin{equation} C_{p} = \\frac{RSS_{p}}{\\hat{\\sigma}^{2}} + 2(p+1) - n \\end{equation}\\] 단, \\(\\hat{\\sigma}^{2}\\) 는 식 (4.24)의 완전모형에서 추정한 오차항의 분산이며, \\(RSS_{p}\\) 는 설명변수의 개수가 \\(p\\) 개인 모형의 잔차제곱합이다. 설명변수의 개수가 \\(p\\) 개인 모형의 경우에는 \\(E(RSS_{p})=(n-p-1)\\sigma^{2}\\)가 되기 때문에, \\(E(C_{p}) \\approx (p+1)\\) 가 될 것이다. 만일 주어진 회귀모형이 자료를 잘 설명하지 못하는 모형이라면, 잔차제곱합이 매우 큰 값이 될 것이 때문에 \\(C_{p}\\) 값이 \\((p+1)\\) 보다 큰 값이 될 것이다. 따라서 가능한 작은 \\(p\\) 값에서 \\(C_{p}\\) 의 값이 \\((p+1)\\) 과 비슷하거나 작게 되는 모형을 선택하면 된다. 완전모형에서는 \\(C_{p}=p+1\\) 이 된다. \\(\\bullet\\) Cross-validation(CV) CV는 회귀모형의 예측력을 평가할 수 있는 재표본추출(resampling) 기법이다. 지금까지 살펴본 모형 평가 측도는 적합된 모형을 대상으로 계산하는 것인데, 문제는 어떤 자료를 사용해서 모형을 적합하고, 평가 측도값을 계산하는 것이 가장 적절한 것인지이다. 예측 목적으로 설정된 회귀모형의 경우에는 모형 적합에 사용된 자료의 변동 설명보다 적합에 사용되지 않은 새로운 자료에 대한 예측 오차가 더 중요한 평가 요소라고 할 수 있다. 통계모형 적합에 사용되지 않은 새로운 자료에 대한 예측 오차를 test error라고 하는데, 회귀모형의 예측 결과에 대한 정당성을 확보하기 위해서는 낮은 test error가 필수적이다. CV는 모형 적합에 사용되는 자료인 training data를 이용해서 test error를 추정하기 위한 재표본추출 기법이다. 몇 가지 조금 다른 방법이 있는데, 여기에서는 k-fold CV라는 방법에 대해서만 살펴보겠다. k-fold CV는 training data를 비슷한 크기의 k개 그룹(fold)으로 분리하는 것으로 시작한다. 분리된 k개의 그룹 중 첫 번째 그룹의 자료를 제외하고, 나머지 (k-1)개 그룹의 자료를 사용해서 적합한 회귀모형으로 적합 과정에서 제외된 첫 번째 그룹에 대한 예측을 실시하고, 적절한 평가 측도를 사용해서 예측 오차를 계산한다. 이어서 각 그룹의 자료를 하나씩 차례로 제외하고 나머지 자료로 적합 및 예측하는 과정을 반복하여 k번의 예측을 실시한다. Test error는 k번의 예측에서 각각 발생된 예측 오차의 평균으로 추정하게 된다. 4.3.2 평가 측도에 의한 변수선택 평가 측도에 의한 방법은 4.3.1절에서 살펴본 \\(AIC\\), \\(BIC\\), \\(adj~R^{2}\\) 및 \\(C_{p}\\) 통계량 등을 기반으로 ‘최적’ 변수를 선택하는 방법이다. 모형에 포함시킬 수 있는 설명변수가 \\(k\\) 개 있다면, 적합 가능한 모형의 개수는 \\(2^{k}\\) 개가 된다. 이 경우에 \\(2^{k}\\) 개의 모형을 다 적합시키고 특정 평가 측도를 기준으로 최적 모형을 선택하는 이른바 ‘Best subset selection’ 방법을 적용해 볼 수 있다. 하지만, 이 방법은 \\(k\\) 의 값이 커진다면 현실적으로 적용하기 쉽지 않다는 문제를 갖고 있다. 다른 방법으로는 특정 평가 측도를 기준으로 변수를 하나씩 모형에 추가하거나 제거하는 ‘Stepwise selection’ 방법을 생각해 볼 수 있다. 이 방법은 검정에 의한 변수선택에서 적용되는 방법으로서 ‘최적’ 모형을 찾지 못할 가능성이 있는 방법이다. 나름의 장점과 단점이 존재하는 방법 중 주어진 상황에 가장 적합한 방법을 선택해서 사용해야 할 것이다. \\(\\bullet\\) Best subset selection \\(k\\) 개 설명변수의 모든 가능한 조합 중 모형의 성능을 최대로 할 수 있는 조합을 찾는 방법이다. 일반적으로 적용되는 방식은 다음과 같다. \\(i=1,2,\\ldots,k\\) 에 대하여 \\(i\\) 개 설명변수가 있는 \\(\\binom{~k~}{i}\\) 개 모형을 적합하고, 그 중 결정계수의 값이 가장 큰 모형을 \\(M_{i}\\) 로 지정 \\(M_{0}, M_{1}, \\ldots, M_{k}\\) 를 대상으로 \\(AIC\\), \\(BIC\\), \\(C_{p}\\), \\(adj~R^{2}\\) 등을 기준으로 ‘최적’ 모형 선택 \\(\\bullet\\) Stepwise selection ‘한 번에 하나씩’ 이루어지는 선택 과정으로 인하여 ‘최적’ 변수의 조합을 찾지 못할 수 있다는 문제가 있지만, 설명변수의 개수가 많은 경우에 비교적 빠르게 변수선택을 할 수 있다는 장점이 있는 방법이다. 적용할 수 있는 방법에는 모형에 변수를 하나씩 추가하는 Forward stepwise selection과 모형에서 하나씩 변수를 제거하는 Backward elimination, 그리고 두 가지 방식이 혼합된 Hybrid stepwise selection이 있다. 1. Forward stepwise selection 절편만 있는 모형 \\(M_{0}~(Y=\\beta_{0}+\\varepsilon)\\) 에서 시작하여 단계적으로 변수를 하나씩 모형에 추가하는 방법이다. 추가할 변수를 선택하는 절차는 다음과 같이 두 가지 방식이 있다. \\(\\bullet\\) 방식 1 \\(AIC\\) 등을 기준으로 모형의 성능을 최대로 개선시킬 수 있는 변수를 하나씩 모형에 추가 더 이상 모형의 성능을 개선시킬 변수가 없으면, 절차 종료 \\(\\bullet\\) 방식 2 \\(i=0, \\ldots, k-1\\) 에 대하여 \\(M_{i}\\) 에 하나의 설명변수를 추가한 \\((k-i)\\) 개 모형 중 결정계수 값이 가장 큰 모형을 \\(M_{i+1}\\) 으로 지정 \\(M_{0}, M_{1}, \\ldots, M_{k}\\) 를 대상으로 \\(AIC\\), \\(BIC\\), \\(C_{p}\\), \\(adj~R^{2}\\) 등을 기준으로 ‘최적’ 모형을 하나 선택 Forward stepwise selection에서는 일단 모형에 포함된 변수에 대해서는 추가적인 확인 과정이 없다. 그러나 이러한 특성은 개별 설명변수의 영향력은 모형에 추가적으로 포함되는 다른 설명변수에 의해 변할 수 있기 때문에 문제가 될 수 있다. 2. Backward stepwise elimination 모든 설명변수가 포함된 full model, \\(M_{k}\\) 에서 시작하여 단계적으로 변수를 하나씩 제거하는 방법이다. 제거할 변수를 선택하는 절차는 다음과 같이 두 가지 방식이 있다. \\(\\bullet\\) 방식 1 제거되면 모형의 성능이 최대로 개선되는 변수를 하나씩 제거 모형에 포함된 변수 중 어떤 변수라도 제거되면 모형의 성능이 더 나빠질 때 절차 종료 \\(\\bullet\\) 방식 2 \\(i=k, k-1, \\ldots, 1\\) 에 대하여 \\(M_{i}\\) 에서 하나씩 변수가 제거된 모구 \\(i\\) 개 모형 중 결정계수의 값이 가장 큰 모형을 \\(M_{i-1}\\) 으로 지정 \\(M_{0}, M_{1}, \\ldots, M_{k}\\) 를 대상으로 \\(AIC\\), \\(BIC\\), \\(C_{p}\\), \\(adj~R^{2}\\) 등을 기준으로 ‘최적’ 모형을 하나 선택 Backward stepwise elimination에서는 일단 제거된 변수는 다시 모형에 포함될 수 없는 방식이다. 그러나 이러한 특성은 개별 설명변수의 영향력은 모형에서 다른 설명변수가 제거되면 변할 수 있기 때문에 문제가 될 수 있다. 3. Hybrid stepwise selection Forward stepwise selection과 Backward stepwise elimination는 일단 모형에 포함되거나 혹은 모형에서 제거된 변수에 대한 추가적인 고려가 불가능한 방식이다. 이러한 특성으로 인하여 ‘최적’ 모형을 찾지 못할 가능성이 있는데, 이 문제는 두 방식의 특성을 혼합시킨 방식을 적용하면 해결할 수 있다. Hybrid forward selection : 각 단계별로 변수를 추가한 후에 모형에 포함되어 있는 변수를 대상으로 backward elimination 기법을 적용해서 변수 제거 Hybrid backward elimination : 각 단계별로 변수를 제거한 후에 이미 제거된 변수를 대상으로 forward selection 기법으로 모형에 추가될 변수 선택 \\(\\bullet\\) 변수선택을 위한 함수 Best subset selection은 함수 leaps::regsubsets()로 할 수 있다. 기본적인 사용법은 다음과 같다. regsubsets(formula, data, nbest = 1, nvmax = 8) formula와 data는 함수 lm()에서 사용한 방식과 동일하다. 설명변수의 개수가 동일한 모형 중 결정계수의 값이 가장 큰 nbest개 모형을 선택하는데, 이렇게 선택된 모형을 대상으로 다양한 평가 측도를 기준으로 비교할 수 있다. 또한 최대 nvmax개의 설명변수가 포함된 모형을 대상으로 비교를 하기 때문에, nvmax에는 설명변수의 개수를 지정할 필요가 있다. Stepwise selection은 함수 MASS::stepAIC()로 할 수 있다. 기본적인 사용법은 다음과 같다. stepAIC(object, scope, direction, trace = 1, k = 2) object는 stepwise selection을 시작하는 회귀모형을 나타내는 lm 객체를 지정한다. Forward selection의 경우에는 절편만이 있는 모형인 lm(y ~ 1, data)가 될 것이고, Backward elimination의 경우에는 모든 설명변수가 포함된 모형인 lm(y ~ ., data)가 될 것이다. scope는 탐색 범위를 나타내는 upper와 lower를 요소가 갖고 있는 리스트를 지정한다. direction은 탐색 방식을 지정하는데, \"forward\"는 모형에 포함된 변수는 계속 유지하면서 forward selection 방법을 진행하며, \"backward\"는 일단 모형에서 제거된 변수는 최종 모형에 포함시키지 않는 방식으로 backward elimination을 진행한다. 또한 \"both\"는 hybrid stepwise selection 방법을 진행한다. direction의 디폴트는 scope를 지정하면 \"both\"가 되지만, scope가 생략되면 \"backward\"가 된다. trace는 탐색 과정의 출력 여부를 지정하는 것으로 출력되는 것이 디폴트이며, FALSE 또는 0을 지정하면 출력되지 않는다. k는 AIC나 BIC의 계산 과정에서 모형에 포함된 변수의 개수만큼 불이익을 주기 위한 상수로서, 디폴트는 AIC 계산을 위한 k = 2이다. 만일 BIC에 의한 단계별 선택법을 적용하고자 한다면 k = log(n)으로 수정해야 한다. 단, n은 데이터 프레임의 행 개수이다. \\(\\bullet\\) 예제: mtcars 4.1.1절에서 다루었던 mtcars를 대상으로 변수선택을 진행해 보자. mtcars를 대상으로 앞서 이루어진 주된 분석은 다음과 같다. mtcars_4 &lt;- mtcars |&gt; select(!c(gear, carb)) |&gt; mutate(across(c(cyl, vs, am), as.factor)) Best subset selection 방법을 함수 leaps::regsubsets()로 진행해 보자. library(leaps) fits &lt;- regsubsets(mpg ~ ., mtcars_4, nvmax = 9) 함수 regsubsets()에 요인을 설명변수로 입력하면, 각 요인에 대한 가변수를 자동으로 생성한다. 그런데 여기에서 주의할 점은 각 가변수를 개별적인 설명변수로 취급한다는 것이다. 모형 fits에 입력된 설명변수는 연속형 변수 5개와 요인 3개인데, 요인의 경우에는 각 가변수를 개별 변수로 인식하기 때문에 cyl에 대한 가변수 2개, vs와 am에 대한 가변수 각각 1개씩 모두 4개의 변수가 있는 것으로 인식한다. 따라서 모형 fits에는 9개의 설명변수가 입력되기 때문에 nvmax = 9를 지정한 것이다. 설명변수의 개수가 \\(i\\) 개 (\\(i=1, \\ldots, 9\\)) 인 모형 중 결정계수가 가장 높은 nbest개의 모형은 다음과 같이 함수 summary()로 출력할 수 있다. 모형에 포함되는 변수에는 \"*\"가 표시되어 있다. summary(fits) ## Subset selection object ## Call: regsubsets.formula(mpg ~ ., mtcars_4, nvmax = 9) ## 9 Variables (and intercept) ## Forced in Forced out ## cyl6 FALSE FALSE ## cyl8 FALSE FALSE ## disp FALSE FALSE ## hp FALSE FALSE ## drat FALSE FALSE ## wt FALSE FALSE ## qsec FALSE FALSE ## vs1 FALSE FALSE ## am1 FALSE FALSE ## 1 subsets of each size up to 9 ## Selection Algorithm: exhaustive ## cyl6 cyl8 disp hp drat wt qsec vs1 am1 ## 1 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 2 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot; &quot; ## 3 ( 1 ) &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; ## 4 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; ## 5 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; ## 6 ( 1 ) &quot;*&quot; &quot; &quot; &quot; &quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 7 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 8 ( 1 ) &quot;*&quot; &quot; &quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; ## 9 ( 1 ) &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; &quot;*&quot; 설명변수가 하나인 경우에는 wt, 두 개인 경우에는 wt와 hp, 세 개인 경우에는 wt, qsec, am이 선택된 것을 알 수 있다. 2개 이상의 가변수가 사용된 범주형 변수의 경우에는 하나의 가변수만 포함되어도 해당 범주형 변수를 모형에 포함시키는 것이 적절할 것이다. 이제 선택된 9개 모형을 대상으로 평가 측도를 비교해 보자. 평가 측도의 비교는 객체 fits를 함수 plot()에 입력하면 된다. 평가 측도는 scale에 지정할 수 있는데, 가능한 키워드는 \"bic\", \"Cp\", \"adjr2\", \"r2\"이며, 디폴트는 \"bic\"이다. plot(fits) 그림 4.10: BIC에 의한 비교 결과 그림 4.10의 X축에는 설명변수가 표시되어 있고, Y축에는 비교 대상이 되는 각 모형의 \\(BIC\\) 의 값이 표시되어 있다. 즉, 그래프의 각 행은 비교 대상이 되는 모형을 나타내는 것이며, X축에 표시된 설명변수가 해당 모형에 포함이 된 것이면 직사각형에 색이 채워진다. 위에서 첫 번째 모형이 \\(BIC\\) 의 값이 가장 작은 모형이며, 변수 wt, qsec, am이 포함된 모형이다. plot(fits, scale = &quot;adjr2&quot;) 그림 4.11: 수정결정계수에 의한 비교 결과 수정결정계수를 평가 측도로 변경하여 변수선택을 진행해 보자. 수정결정계수가 가장 큰 모형으로는 변수 hp, wt, vs, am과 cyl이 포함된 모형이 선택되었다. plot(fits, scale = &quot;Cp&quot;) 그림 4.12: Cp에 의한 비교 결과 \\(C_{p}\\) 가 평가 측도인 경우, 선택된 모형은 수정결정계수의 경우와 큰 차이가 없음을 알 수 있다. Stepwise selection 방법을 함수 MASS::stepAIC()로 진행해 보자. 먼저 모든 설명변수가 포함된 모형과 절편만 있는 모형에 대한 lm 객체를 생성시키자. fit_full &lt;- lm(mpg ~ ., mtcars_4) fit_null &lt;- lm(mpg ~ 1, mtcars_4) AIC에 의한 forward selection으로 변수선택을 진행해 보자. fit_null로 시작하고 scope의 upper가 fit_full로 지정하였기 때문에 hybrid forward selection이 수행된다. 변수 wt, cyl, hp, am이 선택되었다. MASS::stepAIC(fit_null, scope = list(lower = fit_null, upper = fit_full), trace = FALSE) ## ## Call: ## lm(formula = mpg ~ wt + cyl + hp + am, data = mtcars_4) ## ## Coefficients: ## (Intercept) wt cyl6 cyl8 hp am1 ## 33.70832 -2.49683 -3.03134 -2.16368 -0.03211 1.80921 AIC에 의한 backward elimination으로 변수선택을 진행해 보자. Backward elimination은 full model을 시작 모형으로 지정하면 실행된다. AIC에 의한 forward selection과 같은 결과를 보이고 있다. MASS::stepAIC(fit_full, direction = &quot;both&quot;, trace = FALSE) ## ## Call: ## lm(formula = mpg ~ cyl + hp + wt + am, data = mtcars_4) ## ## Coefficients: ## (Intercept) cyl6 cyl8 hp wt am1 ## 33.70832 -3.03134 -2.16368 -0.03211 -2.49683 1.80921 BIC에 의한 forward selection으로 변수선택을 진행해 보자. 변수 wt와 hp만 선택되어 AIC에 의한 방법과는 다른 결과가 나왔다. MASS::stepAIC(fit_null, scope = list(lower = fit_null, upper = fit_full), k = log(nrow(mtcars_4)), trace = FALSE) ## ## Call: ## lm(formula = mpg ~ wt + hp, data = mtcars_4) ## ## Coefficients: ## (Intercept) wt hp ## 37.22727 -3.87783 -0.03177 BIC에 의한 backward elimination으로 변수선택을 진행해 보자. 변수 wt, qsec, am이 선택되었다. MASS::stepAIC(fit_full, direction = &quot;both&quot;, k = log(nrow(mtcars_4)), trace = FALSE) ## ## Call: ## lm(formula = mpg ~ wt + qsec + am, data = mtcars_4) ## ## Coefficients: ## (Intercept) wt qsec am1 ## 9.618 -3.917 1.226 2.936 4.3.3 Shrinkage 방법 4.3.2절에서 살펴본 변수선택 방식은 설명변수들의 부분집합으로 구성되는 다양한 회귀모형을 적합하고, 그 모형의 평가측도로 ‘최적’ 모형을 선택하는 것이다. 이 방식의 대안으로 최근 많이 사용되는 shrinkage 방법은 회귀계수의 크기에 제약 조건을 부과해서 중요하지 않은 변수의 회귀계수를 0으로 줄어들게 만드는 방법으로서 ridge 회귀모형과 lasso가 여기에 속한다. 4.3.3.1 Ridge 회귀모형 식 (4.24)에 주어진 회귀모형의 회귀계수를 최소제곱추정법으로 추정하는 것은 \\(RSS\\) 를 최소화하는 회귀계수를 구하는 것을 의미한다. \\[\\begin{equation} RSS = \\sum_{i=1}^{n} \\left(y_{i}-\\beta_{0}-\\beta_{1}x_{1i}-\\cdots-\\beta_{k}x_{ki} \\right)^{2} \\end{equation}\\] Ridge 회귀모형에서 회귀계수를 추정하는 방법은 단순히 \\(RSS\\) 만을 최소화하는 것이 아니라 회귀계수의 크기를 제한하는 조건이 추가하는 것이다. Ridge 회귀모형의 회귀계수는 식 (4.26)을 최소화시키는 값이 된다. \\[\\begin{equation} \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{k}\\beta_{j}x_{ji} \\right)^{2} + \\lambda \\sum_{j=1}^{k}\\beta_{j}^{2} \\tag{4.26} \\end{equation}\\] 단, \\(\\lambda\\) 는 조율모수(tuning parameter)이다. 식 (4.26)은 두 개의 성분으로 구성되어 있는데, 첫 번째 성분은 \\(RSS\\) 로서 자료에 잘 적합이 되면 작은 값을 갖게 되는 성분이지만, 두번째 성분은 회귀계수의 크기에 비례해서 작아지는 성분이다. 두 번째 성분을 수축 패널티(shrinkage penalty) 성분이라고 하는데, 그것은 대부분의 회귀계수 값이 \\(0\\) 에 가까운 값으로 수축되어야 작은 값을 갖기 때문이다. 두 번째 성분의 영향력은 조율모수 \\(\\lambda\\) 의 크기에 의해서 조절되는데, 만일 \\(\\lambda = 0\\) 이면 최소제곱추정법과 동일한 결과가 나올 것이고, 반대로 대단히 큰 값이 주어지면 자료의 적합 정도에 관계없이 모든 회귀계수가 \\(0\\) 에 가까운 값을 갖게 될 것이다. 조율모수 \\(\\lambda\\) 의 최적 값을 구하는 것은 ridge 회귀모형에서 가장 중요한 작업이 되는데, 일반적으로 cross-validation으로 구하게 된다. Ridge 회귀모형은 조율모수 \\(\\lambda\\) 의 값이 커짐에 따라 자료에 대한 적합 정도는 점점 떨어지게 되는데, 이러한 특성으로 편기(bias)가 증가하게 된다. 하지만 같은 이유로 회귀계수의 추정값은 자료의 변동에 덜 민감하게 되고, 결과적으로 분산이 감소하게 된다. 따라서 최소제곱추정법에 의해 추정된 회귀계수의 분산이 매우 큰 값을 갖게 되는 경우에는 ridge 회귀모형이 대안으로 사용될 수 있다. Ridge 회귀모형의 회귀계수 추정값은 영향력이 작은 변수의 경우에는 상대적으로 0에 더 근접한 값으로 수축되지만, 정확하게 0의 값이 되는 것은 아니다. 따라서 변수선택의 목적으로 사용하기에는 적절하지 않은 방법이 된다. 4.3.3.2 Lasso Lasso(Least Absolute Shrinkage and Selection Operator)도 ridge 회귀모형처럼 \\(RSS\\) 와 회귀계수의 크기를 제한하는 조건이 추가된 변량을 최소화하는 방식으로 회귀계수를 추정한다. Lasso 회귀계수는 식 (4.27)을 최소화시키는 값이 된다. \\[\\begin{equation} \\sum_{i=1}^{n}\\left(y_{i}-\\beta_{0}-\\sum_{j=1}^{k}\\beta_{j}x_{ji} \\right)^{2} + \\lambda \\sum_{j=1}^{k}|\\beta_{j}| \\tag{4.27} \\end{equation}\\] Lasso도 ridge 회귀모형의 경우처럼 조율모수 \\(\\lambda\\) 의 값이 커지게 되면 회귀계수 추정값이 수축하게 된다. 다만, 두 모형의 차이점은 수축 패널티 성분의 형태인데, ridge 회귀모형에서는 \\(\\beta_{j}^{2}\\) 을 사용하고, lasso에서는 \\(|\\beta_{j}|\\) 을 사용한다. 제곱 형태의 패널티를 사용하면 정확하게 0으로 수축시킬 수 없지만, 절대값 형태의 패널티를 사용하면 0으로 수축시키는 것이 가능하게 된다. 따라서 lasso에서는 영향력이 작은 변수의 회귀계수가 정확하게 0의 값을 갖게 되어, 실질적인 변수선택을 실시하는 것이 된다. \\(\\bullet\\) Shrinkage 모형 적합을 위한 함수 Ridge와 lasso 모형은 패키지 glmnet의 함수를 사용해서 적합할 수 있다. 기본적인 적합 함수는 glmnet()이며 사용법은 다음과 같다. glmet(x, y, alpha = 1) x에는 설명변수의 행렬을 지정하고, y에는 반응변수의 벡터를 지정한다. alpha는 0과 1 사이의 값을 갖는 elasticnet 모수인데, alpha = 1이면 lasso, alpha = 0이면 ridge 패널티가 적용된다. 모형 적합은 조율모수 \\(\\lambda\\) 에 여러 값을 차례로 입력해서 이루어진다. 최적 \\(\\lambda\\) 값을 cross-validation으로 추정한 모형을 적합하기 위해서는 함수 cv.glmnet(x, y, alpha)을 사용하는 것이 더 간편할 수 있다. \\(\\bullet\\) 예제: mtcars 4.3.2절에서 다루었던 mtcars를 대상으로 shringkage 모형을 적합해 보자. mtcars를 대상으로 앞서 이루어진 주된 분석은 다음과 같다. mtcars_4 &lt;- mtcars |&gt; select(!c(gear, carb)) |&gt; mutate(across(c(cyl, vs, am), as.factor)) 패키기 glmnet의 함수를 사용하기 위해 설명변수로 이루어진 행렬과 반응변수 벡터를 생성해보자. X &lt;- model.matrix(mpg ~ ., mtcars_4)[,-1] Y &lt;- mtcars_4$mpg 설명변수의 행렬은 각 변수를 함수 cbind()로 연결해서 만들 수 있지만, 변수의 개수가 많거나 요인이 포함되어 있는 경우에는 불편한 방법이 된다. 함수 model.matrix()는 회귀모형의 design matrix인 \\({\\bf X}\\) 행렬을 생성하는 기능이 있는 함수이다. model.matrix(formula, data)의 형태로 lm()과 동일한 방식으로 모형을 정의하면 된다. 생성된 \\({\\bf X}\\) 행렬의 첫 번째 열은 절편에 대한 것이기 때문에 설명변수의 행렬에 해당되지 않으므로 제외한다. 먼저 ridge 회귀모형을 적합해보자. library(glmnet) fit_R &lt;- glmnet(X, Y, alpha = 0) 조율모수 \\(\\lambda\\) 값의 변화에 따른 회귀계수 추정값의 변화 그래프는 fit_R을 함수 plot()에 입력하면 작성할 수 있다. plot(fit_R, xvar = &quot;lambda&quot;) 그림 4.13: lambda 값의 변화에 따른 ridge 회귀계수 추정값의 변화 그림 4.13은 X축에 표시된 \\(\\log \\lambda\\) 의 값 변화에 따른 회귀계수 추정 결과의 변화를 선으로 나타낸 그래프이다. \\(\\lambda\\) 값이 커지면서 회귀계수가 0으로 수축되고 있는 것을 볼 수 있다. 그래프 위쪽의 눈금은 0이 아닌 회귀계수의 개수를 나타내고 있는데, ridge 회귀계수는 매우 큰 \\(\\lambda\\) 값에 대해서도 모두 0이 되지는 않고 있음을 알 수 있다. Lasso 모형을 적합하고, \\(\\lambda\\) 의 값 변화에 따른 회귀계수 추정 결과의 변화를 나타내 보자. fit_L &lt;- glmnet(X, Y, alpha = 1) plot(fit_L, xvar = &quot;lambda&quot;) 그림 4.14: lambda 값의 변화에 따른 lasso 회귀계수 추정값의 변화 그림 4.14을 보면 \\(\\log \\lambda\\) 값이 증가함에 따라서 0이 아닌 회귀계수의 개수가 줄어들고 있음을 알 수 있다. 최적 \\(\\lambda\\) 값을 cross-validation으로 추정한 모형을 함수 cv.glmnet()으로 적합해 보자. 먼저 ridge 회귀모형을 적합해보자. cvfit_R &lt;- cv.glmnet(X, Y, alpha = 0) cvfit_R ## ## Call: cv.glmnet(x = X, y = Y, alpha = 0) ## ## Measure: Mean-Squared Error ## ## Lambda Index Measure SE Nonzero ## min 0.899 94 6.547 1.694 9 ## 1se 6.345 73 8.192 2.590 9 10-fold cross-validation을 사용하는 것이 디폴트이며, 반응변수가 연속형인 경우에는 MSE를 최소화시키는 \\(\\lambda\\) 값을 추정한다. min에 해당하는 Lambda는 CV MSE를 최소화시키는 \\(\\lambda\\) 값이고, 1se에 해당하는 Lambda는 최소 CV MSE \\(\\pm~~ 1 \\times SE\\) 범위 안에서 최대 \\(\\lambda\\) 값이 된다. 1se의 \\(\\lambda\\) 값은 최소 CV MSE와는 큰 차이가 없으면서도 가능한 더 많은 회귀계수를 0으로 수축시킬 수 있는 조율모수가 된다. \\(\\lambda\\) 값의 변화에 따른 CV의 결과는 다음과 같이 확인할 수 있다. plot(cvfit_R) 그림 4.15: lambda 값의 변화에 따른 CV MSE의 변화 왼쪽에서 첫 번째 수직 점선이 최소 CV MSE에 해당하는 \\(\\lambda\\) 값이고, 두 번째 수직 점선이 1SE에 해당하는 \\(\\lambda\\) 값이다. 이번에는 lasso 모형을 적합해 보자. cvfit_L &lt;- cv.glmnet(X, Y, alpha = 1) cvfit_L ## ## Call: cv.glmnet(x = X, y = Y, alpha = 1) ## ## Measure: Mean-Squared Error ## ## Lambda Index Measure SE Nonzero ## min 0.1983 36 7.779 2.801 7 ## 1se 1.1617 17 10.195 5.096 5 \\(\\lambda\\) 값의 변화에 따른 CV의 결과도 확인해 보자. plot(cvfit_L) 그림 4.16: lambda 값의 변화에 따른 CV MSE의 변화 최적 \\(\\lambda\\) 값으로 추정된 회귀계수 값은 함수 coef()에 cv.glmnet 객체를 입력하면 확인할 수 있다. s = \"lambda.min\"을 추가하면 최소 CV MSE에 해당하는 \\(\\lambda\\) 값으로 추정된 회귀계수가 출력되고, 생략하면 디폴트 값인 s = \"lambda.1se\"가 지정되어 1SE에 해당하는 \\(\\lambda\\) 값으로 추정된 회귀계수가 출력된다. 먼저 ridge 회귀계수의 추정값을 출력해 보자. coef(cvfit_R) ## 10 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 19.022276894 ## cyl6 -1.079993964 ## cyl8 -1.072884386 ## disp -0.006325128 ## hp -0.013135723 ## drat 1.107153854 ## wt -1.124668959 ## qsec 0.208350553 ## vs1 1.119832155 ## am1 1.447003442 Lasso 회귀계수의 추정값을 출력해 보자. coef(cvfit_L) ## 10 x 1 sparse Matrix of class &quot;dgCMatrix&quot; ## s1 ## (Intercept) 32.0056920950 ## cyl6 . ## cyl8 . ## disp -0.0008627785 ## hp -0.0202233183 ## drat 0.2171493960 ## wt -2.9820129233 ## qsec . ## vs1 0.1448971143 ## am1 . 회귀계수의 추정값이 점으로 찍힌 변수는 해당 회귀계수가 0이라는 것을 의미한다. Ridge 회귀계수와 lasso 회귀계수의 추정 결과는 최소제곱추정에 의한 추정결과와는 당연히 다르며, 회귀계수 추정에 대한 SE로 계산되지 않는다. 또한 cross-validation에 의한 결과이기 때문에 추정할 때마다 추정결과가 조금 다르게 된다. "],["reg-diag.html", "5 장 회귀모형의 진단 5.1 회귀모형에 대한 진단 5.2 관찰값에 대한 진단", " 5 장 회귀모형의 진단 4.3절에서 우리는 변수선택 과정을 통해 최적모형을 찾는 방법을 살펴보았다. 이제 선택된 모형을 이용하여 반응변수에 대한 예측을 실시하거나 혹은 변수들에 대한 추론 등을 실시할 수 있게 되었다. 그러나 만일 선택된 모형이 회귀모형의 가정을 전혀 만족시키지 못하고 있거나, 혹은 몇몇 극단적인 관찰값으로 인하여 적합 결과가 왜곡되어 있다면 이러한 예측이나 추론 등은 통계적 신빙성이 전혀 없는 결과가 될 수도 있다. 따라서 선택된 회귀모형이 가정사항을 만족하고 있는지를 확인하는 과정이 필요한데, 이것을 회귀진단이라고 한다. 회귀진단은 일반적으로 회귀모형에 대한 진단과 관찰값에 대한 진단으로 구분해서 진행된다. 회귀모형에 대한 진단은 적합된 회귀모형이 중요한 가정 사항을 만족하고 있는지 여부를 확인하는 것으로서, 적합 및 추론의 신빙성을 확보하기 위한 단계가 된다. 관찰값에 대한 진단은 개별 관찰값들이 모형의 적합 과정에 미치는 영향력을 파악해서, 지나치게 큰 영향력을 지닌 관찰값을 확인하기 위한 단계이다. 5.1 회귀모형에 대한 진단 다중회귀모형 \\(y_{i}=\\beta_{0}+\\beta_{1}x_{1i}+\\cdots+\\beta_{ki}x_{ki}+\\varepsilon_{i}\\), \\(i=1,\\ldots,n\\) 에 대한 가정은 다음과 같다. 오차항 \\(\\varepsilon_{1}, \\varepsilon_{2}, \\cdots, \\varepsilon_{n}\\) 의 평균은 0, 분산은 \\(\\sigma^{2}\\) 오차항 \\(\\varepsilon_{1}, \\varepsilon_{2}, \\cdots, \\varepsilon_{n}\\) 의 분포는 정규분포 오차항 \\(\\varepsilon_{1}, \\varepsilon_{2}, \\cdots, \\varepsilon_{n}\\) 은 서로 독립 반응변수와 설명변수의 관계는 선형 오차항에 대한 가정 사항이 있지만, 실제 오차항은 관측할 수 없는 대상이기 때문에 만족 여부를 확인할 수 없다. 따라서 오차항에 대한 가정 사항은 잔차 (\\(e_{i}=y_{i}-\\hat{y}_{i}\\))를 이용하여 확인하게 되며, 만족 여부는 대부분 그래프를 이용해서 이루어진다. 4장에서 사용한 예제 자료인 state.x77과 mtcars를 대상으로 회귀모형 진단을 실시해 보자. \\(\\bullet\\) 예제 : state.x77 state.x77 자료에 대해 4.1.1절에서 이루어진 주된 분석은 다음과 같다. library(tidyverse) states &lt;- as.data.frame(state.x77) |&gt; rename(Life_Exp = &#39;Life Exp&#39;, HS_Grad = &#39;HS Grad&#39;) states 자료에 대하여 변수선택 과정 없이 모든 설명변수를 포함한 회귀모형을 적합하고, 그 모형에 대한 회귀진단을 실시해 보자. fit_s &lt;- lm(Murder ~ ., states) 추정된 회귀모형의 가정 만족 여부를 확인하는 가장 기본적인 절차는 함수 lm()으로 생성된 객체를 함수 plot()에 입력하는 것인데, 네 종류의 그래프가 차례로 그려진다. 작성된 그래프를 하나의 Plots 창에서 함께 보는 것이 편리한데, 그것을 위해서는 함수 par()를 함께 사용해야 한다. 함수 par()는 plot()과 같은 base graphic 함수로 작성된 그래프의 다양한 환경 모수를 조정하는 기능이 있다. par(mfrow = c(2, 2)) plot(fit_s, pch = 20) par(mfrow = c(1, 1)) 그림 5.1: states 자료에 대한 회귀모형 fit_s의 회귀진단 par(mfrow = c(2, 2))는 Plots 창의 영역을 2개의 행과 2개의 열로 분리시키고, 이어서 작성되는 그래프를 행 단위로 출력하게 한다. plot(fit_s, pch = 20)로 네 종류의 진단 그래프를 작성하고, par(mfrow = c(1, 1))를 실행시켜서 분리된 Plots 창의 영역을 통합하였다. 함수 plot()에 입력한 pch = 20는 점의 모양을 속이 찬 동그란 원으로 지정하기 위해 사용했다. 그림 5.1의 왼쪽 위 패널에 있는 Residuals vs Fitted라는 제목의 그래프는 일반적으로 가장 많이 사용되는 잔차 산점도 그래프로서, 잔차 \\(e_{i}\\) 와 \\(\\hat{y}_{i}\\) 의 산점도이다. 오른쪽 위 패널에 있는 Q-Q Residuals라는 제목의 그래프는 표준화잔차 \\(r_{i}\\) 의 정규 분위수-분위수 그래프이다. 왼쪽 아래 패널에 있는 Scale-Location이라는 제목의 그래프는 \\(\\sqrt{|r_{i}|}\\) 와 \\(\\hat{y}_{i}\\) 의 산점도로서 동일 분산 가정의 만족 여부를 확인하는 그래프이다. 마지막으로 오른쪽 아래 패널에 있는 Residuals vs Leverage라는 제목의 그래프는 관찰값의 진단에 사용되는 그래프인데, 관련 통계량 및 그래프에 대한 자세한 설명은 5.2절에서 하겠다. 또한 각 그래프에는 3개의 점 옆에 라벨이 표시되어 있는데, 이것은 각 그래프에서 가장 극단적인 세 점의 행 이름이 표시된 것이다. Q-Q Residuals 그래프와 Scale-Location 그래프에서 사용된 표준화 잔차는 일반 잔차의 퍼짐 정도를 조정한 잔차로써 다음과 같이 정의된다. \\[\\begin{equation} r_{i} = \\frac{e_{i}}{RSE \\sqrt{1-h_{i}}} \\end{equation}\\] 회귀모형 fit_s에 대한 잔차와 표준화 잔차의 산점도를 비교해 보자. 그림 5.2: 모형 fit_s의 잔차와 표준화 잔차의 비교 표준화 잔차를 정의하는데 사용된 \\(h_{i}\\)는 leverage라고 불리는 통계량으로서, 설명변수들의 \\(k\\) 차원 공간에서 설명변수의 \\(i\\) 번째 관찰값이 자료의 중심으로부터 떨어져 있는 거리를 표현하는 것으로 볼 수 있다. 단순회귀모형의 경우 \\(i\\) 번째 관찰값의 leverage \\(h_{i}\\) 는 다음과 같이 정의된다. \\[\\begin{equation} h_{i} = \\frac{1}{n} + \\frac{(x_{i}-\\overline{x})^{2}}{\\sum_{j=1}^{n}(x_{j}-\\overline{x})^{2}} \\end{equation}\\] Leverage 값이 큰 관찰값은 회귀계수의 추정 결과에 큰 영향을 줄 가능성이 높다고 할 수 있다. 그림 5.3는 leverage가 큰 관찰값이 회귀계수 추정 결과에 어떻게 영향을 줄 수 있는지를 보여주는 모의자료를 이용한 예가 된다. 두 그래프에서 빨간 점은 leverage가 높은 관찰값인데, 실선은 빨간 점을 포함하고 추정한 회귀직선이고, 점선은 빨간 점을 제외한 상테에서 추정한 회귀직선이다. 첫 번째 그래프에서만 큰 영향력을 확인할 수 있다. 그림 5.3: leverage가 높은 관찰값의 영향력을 보여주는 모의자료 \\(\\bullet\\) 오차항의 동일분산 가정 동일분산 가정의 만족 여부를 확인하는 기본적인 방법은 함수 plot()으로 생성되는 잔차 \\(e_{i}\\) 와 \\(\\hat{y}_{i}\\) 의 산점도와 \\(\\sqrt{|r_{i}|}\\) 와 \\(\\hat{y}_{i}\\) 의 산점도를 확인하는 것이다. 모형 fit_s에 대한 두 종류의 잔차 산점도를 다시 살펴보자. 그림 5.4: 동일분산 가정 확인을 위한 잔차 산점도 그림 5.4의 첫 번째 그래프에서는 Y축이 0인 수평선을 중심으로 점들이 거의 일정한 폭을 유지하며 분포하고 있는지 확인해야 한다. 그림 5.4의 두 번째 그래프에서는 점들이 전체적으로 증가하거나 감소하는 패턴이 있는지 확인해야 한다. 그래프에 추가된 빨간 곡선을 참조하여 판단하는 것이 좋은데, 이것은 로버스트 국소다항회귀에 의해 추정된 비모수 회귀곡선이다. 극단값에 영향을 덜 받으면서 두 변수의 관계를 가장 잘 나타내는 곡선이라고 하겠다. 그림 5.4에서는 분산이 일정하지 않다는 증거를 확인하기 어려워 보인다. \\(\\bullet\\) 오차항의 정규분포 가정 회귀분석에서 이루어지는 검정 및 신뢰구간은 오차항이 정규분포를 한다는 가정에 근거를 두고 이루어진다. 그러나 오차항의 분포가 정규분포의 형태에서 약간 벗어나는 것은 큰 문제를 유발하지 않으며, 표본 크기가 커지면 중심극한정리를 적용할 수 있어서 비정규성은 큰 문제가 되지 않는다. 그러나 오차항의 분포가 Cauchy 분포와 같이 꼬리가 긴 형태의 분포임이 확인된다면 최소제곱법에 의한 회귀계수의 추정보다는 로버스트 선형회귀를 이용하는 것이 더 효과적일 것이다. 정규성의 확인에 사용되는 그래프로는 함수 plot()에서 생성되는 표준화잔차에 대한 정규 분위수-분위수 그래프가 있다. 회귀모형 fit_s의 정규성 가정을 확인해보자. 그림 5.5: 정규분포 가정 확인을 위한 그래프 잔차의 정규 분위수-분위수 그래프에서는 크기순으로 재배열한 잔차가 표본 분위수가 되는데, 표본 분위수 사이의 간격 패턴이 정규분포 이론 분위수 사이의 간격 패턴과 유사하면 점들이 기준선 위에 분포하게 된다. 정규분포 가정에는 문제가 없는 것으로 보인다. \\(\\bullet\\) 오차항의 독립성 가정 수집된 자료가 시간적 혹은 공간적으로 서로 연관되어 있는 경우에는 오차항의 독립성 가정이 만족되지 않을 수 있다. 시간에 흐름에 따라 관측된 시계열 자료나 공간에 따라 관측된 공간 자료를 대상으로 회귀분석을 하는 경우에는 반드시 확인해야 할 가정이 된다. 독립성 가정은 여러 행태로 위반될 수 있는데, 우선 \\(\\varepsilon_{i}\\) 가 \\(\\varepsilon_{i-1}\\) 와 서로 연관되어 있는지 여부, 즉 1차 자기상관관계만을 확인하려면 Durbin-Watson 검정을 실시하면 된다. Durbin-Watson 검정은 패키지 car의 함수 durbinWatsonTest()로 할 수 있으며, 귀무가설은 오차항의 1차 자기상관계수가 0이라는 것이다. Durbin-Watson 검정에서 귀무가설을 기각하지 못했다고 해서 오차항이 독립이라고 바로 결론을 내릴 수는 없는데, 그것은 일반적인 형태의 위반 여부를 확인해야 하기 때문이다. 일반적인 형태의 독립성 위반이란 오차항이 자기회귀이동평균모형, 즉 ARMA(p,q)모형인지 여부를 확인하는 것이다. 이것을 위한 첫 번째 단계는 오차항의 1차 자기상관계수부터 K차 자기상관계수가 모두 0이라는 귀무가설을 검정하는 Breusch-Godfrey 검정을 실시하는 것이며, 이 작업은 패키지 forecast의 함수 checkresiduals()로 할 수 있다. 독립성 가정 검정은 이 책의 수준을 넘어서기 때문에, 자세한 내용은 생략하기로 한다. 데이터 프레임 states는 시간적 혹은 공간적으로 연관을 갖기 어려운 방식으로 수집되어 있기 때문에 오차항의 독립성 가정에는 큰 문제가 없는 경우라고 할 수 있다. \\(\\bullet\\) 선형관계 가정 단순회귀모형의 경우 반응변수와 설명변수의 선형관계는 두 변수의 산점도로 충분히 확인할 수 있다. 그러나 다중회귀모형의 경우에는 변수 \\(X_{i}\\) 와 \\(Y\\) 의 산점도 혹은 변수 \\(X_{i}\\) 와 잔차 \\(e\\) 산점도가 큰 의미를 갖지 못하게 되는데, 이것은 회귀모형에 포함된 다른 변수의 영향력을 확인할 수 없기 때문이다. 이러한 경우 부분잔차(partial residual)가 매우 유용하게 사용될 수 있다. 변수 \\(X_{i}\\) 의 부분잔차란 반응변수 \\(Y\\) 에서 모형에 포함된 다른 설명변수의 영향력이 제거된 잔차를 의미하는데, \\(Y-\\sum_{j \\ne i}\\hat{\\beta}_{j}X_{j}\\) 로 정의할 수 있다. 그런데 \\(Y=\\hat{Y} + e\\) 가 되기 때문에, 부분잔차는 \\(\\hat{Y}+e-\\sum_{j \\ne i}\\hat{\\beta}_{j}X_{j}\\) 로 표현되고, \\(e + \\hat{\\beta}_{i}X_{i}\\) 로 정리할 수 있다. 따라서 다중선형회귀모형에서 \\(X_{i}\\) 와 \\(Y\\) 의 선형관계는 변수 \\(X_{i}\\) 와 그 부분잔차 \\(e + \\hat{\\beta}_{i}X_{i}\\) 의 산점도로 확인할 수 있다. 데이터 프레임 states의 회귀모형 fit_s에 포함된 설명변수와 반응변수의 선형관계를 부분잔차 산점도를 작성하여 확인해 보자. 부분 잔차 산점도는 패키지 car의 crPlots()로 작성할 수 있는데, 이 그래프는 Component + Residual Plots라고도 불린다. library(car) crPlots(fit_s) 그림 5.6: 선형 가정 확인을 위한 부분잔차 산점도 각 변수의 부분잔차 산점도에는 두 변수의 회귀직선을 나타내는 파란 색의 대시(dashed line)와 국소다항회귀곡선을 나타내는 마젠타(magenta) 색의 실선이 추가되어 있다. 국소다항회귀곡선이 회귀직선과 큰 차이를 보인다면 비선형 관계를 의심할 수 있을 것이다. 그런 경우에는 해당 변수의 제곱항을 모형에 포함시키거나 또는 해당 변수의 적절한 변환이 이루어져야 할 것이다. 회귀모형 fit_s에서는 선형관계에 큰 문제가 없는 것으로 확인된다. \\(\\bullet\\) 다중공선성 다중공선성은 회귀모형의 가정과 직접적인 연관이 있는 것이 아니지만 회귀모형의 추정결과를 해석하는 과정에 큰 영향을 미칠 수 있는 문제가 된다. 즉, 설명변수들 사이에 강한 선형관계가 존재하는 다중공선성의 문제가 생기면 회귀계수 추정량의 분산이 크게 증가하게 되어 결과적으로 회귀계수의 신뢰구간 추정 및 검정에 큰 영향을 미치게 된다. 다중공선성은 분산팽창계수(VIF; Variance Inflation Factor)를 계산해 보면 확인할 수 있다. 변수 \\(X_{j}\\) 의 VIF는 \\(1/(1-R_{j}^{2})\\) 로 계산되는데, 여기에서 \\(R_{j}^{2}\\) 는 \\(X_{j}\\) 를 종속변수로 하고 나머지 설명변수를 독립변수로 하는 회귀모형의 결정계수이다. 변수 \\(X_{j}\\) 의 회귀계수 추정량인 \\(\\hat{\\beta}_{j}\\) 의 분산은 변수 \\(X_{j}\\) 의 VIF를 이용해서 다음과 같이 표현된다. \\[\\begin{equation} Var(\\hat{\\beta}_{j}) = \\frac{\\sigma^{2}}{1-R_{j}^{2}} \\end{equation}\\] 수식에 의하면, VIF가 큰 값이 되면, \\(\\hat{\\beta}_{j}\\) 의 분산도 큰 값을 갖게 된다. 그런데 추정량의 분산이 커진다는 것은 추정 결과에 변동성이 커진다는 것을 의미하는 것이며, 따라서 추정 결과에 대한 신빙성이 많이 떨어지게 된다. VIF 값에 대한 판단 기준을 일률적으로 제시하는 것은 적절하지 않은 것으로 보인다. 다만 VIF 값이 10 이상이 된다는 것은 \\(R_{j}^{2}\\) 의 값이 0.9 이상이라는 것이고, VIF 값이 5 이상이면, \\(R_{j}^{2}\\) 의 값이 0.8 이상이 되는 것인데, 특정 설명변수의 변동이 다른 설명변수로 80% 혹은 90% 이상 설명된다면, 해당 변수를 굳이 모형에 포함시킬 필요는 없는 것으로 보인다. 강한 다중공선성이 존재하는 경우에 특정 변수를 제거함으로써 문제를 해결할 수도 있지만, 대부분의 경우에는 다른 모형을 대안으로 사용해야 한다. Ridge 회귀모형과 같은 shrinkage 모형이 사용될 수 있으며, 관련이 있을 것 같은 설명변수들을 대상으로 주성분을 추출해서 설명변수로 대신 사용하는 방법도 대안으로 사용된다. 분산팽창계수의 계산은 패키지 car의 함수 vif()로 할 수 있다. 모형 fit_s의 다중공선성 존재 여부를 확인해 보자. vif(fit_s) ## Population Income Illiteracy Life_Exp HS_Grad Frost Area ## 1.342691 1.989395 4.135956 1.901430 3.437276 2.373463 1.690625 회귀모형 fit_s에 있는 다섯 설명변수의 분산팽창계수가 모두 큰 값이 아닌 것으로 계산되었고, 따라서 다중공선성의 문제는 없는 것으로 보인다. \\(\\bullet\\) 예제 : mtcars mtcars 자료에 대해 4.3.2절에서 이루어진 주된 분석은 다음과 같다. mtcars_4 &lt;- mtcars |&gt; select(!c(gear, carb)) |&gt; mutate(across(c(cyl, vs, am), as.factor)) mtcars_4 자료에 대하여 BIC를 기준으로 best subset selection에 의한 변수선택 결과로 wt, qsec, am이 선택되었다. fit_bic &lt;- lm(mpg ~ wt + qsec + am, mtcars_4) 모형 fit_bic에 대한 잔차분석을 진행해 보자. 잔차분석 결과에 큰 문제가 없다면, 회귀모형에 대한 가정을 대부분 만족하는 것으로 간주할 수 있고, 따라서 모형 fit_bic를 예측모형으로 사용할 수 있는 것이다. 하지만 중대한 문제가 발견이 된다면, 발견된 문제 해결을 위해 모형 수정을 진행해야 할 것이다. 이제 모형 fit_bic에 대한 기본 그래프를 작성하는 것으로 잔차분석을 진행해 보자. par(mfrow = c(2, 2)) plot(fit_bic, pch = 20) par(mfrow = c(1,1)) 그림 5.7: mtcars 자료에 대한 모형 fit_bic의 잔차 산점도 Residuals vs Fitted라는 제목의 첫 번째 그래프에서 점들이 2차 함수 형태를 보이고 있음을 볼 수 있다. 이 현상은 일반적으로 반응변수와의 관계가 선형이 아닌 변수가 설명변수로 모형에 포함되면 발생하게 된다. 또한 Scale-Location이라는 제목의 세 번째 그래프에서 점들이 전체적으로 증가하는 패턴이 있음을 볼 수 있다. 이것은 일반적으로 오차항의 분산이 적합값이 증가하면 함께 증가할 때 발생하는 현상이다. 부분잔차 그래프를 작성해서 선형관계에 문제가 있는 변수를 확인해 보자. crPlots(fit_bic, pch = 20) 그림 5.8: mtcars 자료에 대한 모형 fit_bic의 부분잔차 그래프 변수 qsec의 경우에는 선형성에 큰 문제가 없는 것으로 보이지만, wt의 경우에는 선형으로 설명하기에 약간의 무리가 있는 것으로 보인다. 모형 fit_bic에 wt의 2차항을 포함시키고,적합 결과를 확인해보자. fit_bic.1 &lt;- update(fit_bic, . ~ . + I(wt^2)) summary(fit_bic.1) ## ## Call: ## lm(formula = mpg ~ wt + qsec + am + I(wt^2), data = mtcars_4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -3.7508 -1.3125 -0.4954 1.0722 4.8553 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 27.7376 8.9574 3.097 0.004528 ** ## wt -11.2491 2.6808 -4.196 0.000263 *** ## qsec 0.9705 0.2739 3.543 0.001461 ** ## am1 1.0215 1.4346 0.712 0.482522 ## I(wt^2) 0.9582 0.3403 2.816 0.008978 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.201 on 27 degrees of freedom ## Multiple R-squared: 0.8838, Adjusted R-squared: 0.8666 ## F-statistic: 51.33 on 4 and 27 DF, p-value: 3.107e-12 추가된 wt의 2차항이 유의하게 나왔고, 수정결정계수가 증가한 것으로 보아, wt의 2차항을 포함시키는 것이 좋을 것으로 보인다. 그러나 wt의 2차항이 포함됨으로써 am이 비유의적이 되었는데, 따라서 am을 제외한 모형을 적합시키고, 결과를 확인할 필요가 있다. fit_bic.2 &lt;- update(fit_bic.1, . ~ . - am) summary(fit_bic.2) ## ## Call: ## lm(formula = mpg ~ wt + qsec + I(wt^2), data = mtcars_4) ## ## Residuals: ## Min 1Q Median 3Q Max ## -4.2200 -1.2521 -0.6288 0.9357 5.1761 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 32.6418 5.6768 5.750 3.59e-06 *** ## wt -12.4331 2.0842 -5.965 2.01e-06 *** ## qsec 0.8599 0.2236 3.846 0.000634 *** ## I(wt^2) 1.0730 0.2970 3.613 0.001174 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 2.182 on 28 degrees of freedom ## Multiple R-squared: 0.8816, Adjusted R-squared: 0.8689 ## F-statistic: 69.5 on 3 and 28 DF, p-value: 4.345e-13 모형 fit_bic, fit_bic.1, fit_bic.2를 비교해 보자. AIC와 BIC, 그리고 수정결정계수를 비교해 보자. AIC(fit_bic, fit_bic.1, fit_bic.2) ## df AIC ## fit_bic 5 154.1194 ## fit_bic.1 6 147.8800 ## fit_bic.2 5 146.4754 BIC(fit_bic, fit_bic.1, fit_bic.2) ## df BIC ## fit_bic 5 161.4481 ## fit_bic.1 6 156.6744 ## fit_bic.2 5 153.8040 summary(fit_bic)$adj.r.squared ## [1] 0.8335561 summary(fit_bic.1)$adj.r.squared ## [1] 0.8665743 summary(fit_bic.2)$adj.r.squared ## [1] 0.8689233 세 가지 기준에서 모형 fit_bic.2를 선택하는 것이 좋을 것으로 보인다. 이제 fit_bic.2에 대한 잔차분석을 다시 진행해 보자. par(mfrow = c(2, 2)) plot(fit_bic.2, pch = 20) par(mfrow = c(1, 1)) 그림 5.9: 모형 fit_bic.2의 잔차분석 그래프 큰 문제는 없는 것으로 보인다. 부분잔차 그래프도 작성해 보자. 선형 가정이 만족되는 것으로 보인다. crPlots(fit_bic.2, pch = 20) 그림 5.10: 모형 fit_bic.2의 부분잔차 그래프 모형 fit_bic.2에 대한 다중공선성도 확인해 보자. 모형 fit_bic.2는 wt의 다항회귀모형이 되는데, 이런 경우에는 해당 변수 1차항과 2차항의 VIF 값은 상당히 큰 값으로 계산되는 것이 당연하다. vif(fit_bic.2) ## wt qsec I(wt^2) ## 27.076669 1.039145 26.896784 이것은 다항회귀모형이 갖고 있는 구조적 문제인데, 따라서 가능한 낮은 차수를 유지하는 것이 필요한 것이다. 수정 전 모형인 fit_bic의 VIF를 확인해 보면, 다중공선성의 문제는 없는 것으로 보인다. vif(fit_bic) ## wt qsec am ## 2.482952 1.364339 2.541437 5.2 관찰값에 대한 진단 회귀모형의 가정사항 만족 여부를 확인하는 것과 더불어 특이한 관찰값의 존재유무를 확인하는 것도 중요한 회귀진단 항목이 된다. 특이한 관찰값이란 회귀계수의 추정에 과도하게 큰 영향을 미치는 관찰값이나, 추정된 회귀모형으로는 설명이 잘 안 되는 이상값을 의미한다. 영향력이 큰 관찰값을 발견하는 데 필요한 통계량으로는 DFBETAS, DFFITS, Covariance ratio, Cook’s distance와 Leverage 등이 있다. DFBETAS는 \\(i\\) 번째 관찰값을 포함한 상태와 제외한 상태에서 각각 추정한 개별 회귀계수 추정값 \\(\\hat{\\beta}_{j}\\) 의 차이를 의미한다. DEFITS는 \\(i\\) 번째 관찰값을 포함한 상태와 제외한 상태에서 각각 추정한 적합값 \\(\\hat{y}_{i}\\) 의 차이를 의미한다. Cook’s distance는 \\(i\\) 번째 관찰값을 포함한 상태에서 추정한 회귀계수 벡터 \\(\\hat{\\boldsymbol{\\beta}}\\) 과 \\(i\\) 번째 관찰값을 제외하고 추정한 회귀계수 벡터 \\(\\hat{\\boldsymbol{\\beta}}_{(i)}\\) 의 통합된 차이를 보여주는 통계량이다. \\[\\begin{equation} COOKD = \\frac{\\left(\\hat{\\boldsymbol{\\beta}}-\\hat{\\boldsymbol{\\beta}}_{(i)} \\right)^{T}\\mathbf{X}^{T}\\mathbf{X}\\left(\\hat{\\boldsymbol{\\beta}}-\\hat{\\boldsymbol{\\beta}}_{(i)} \\right) }{(k+1)RSE} \\end{equation}\\] 스튜던트화 잔차 \\(t_{i}\\) 는 \\(y_{i}\\) 와 \\(\\hat{y}_{(i)}\\) 의 표준화된 차이라고 할 수 있다. \\[\\begin{equation} t_{i} = \\frac{y_{i}-\\hat{y}_{(i)}}{SE\\left(y_{i}-\\hat{y}_{(i)}\\right)} \\end{equation}\\] 단, \\(\\hat{y}_{(i)}\\) 를 \\(i\\) 번째 관찰값을 제외한 나머지 \\((n-1)\\) 개의 자료로 추정된 회귀모형으로 제외된 \\(y_{i}\\) 를 예측한 결과이다. 스튜던트화 잔차의 값이 크다는 것은 해당 관찰값이 이상값으로 분류될 가능성이 높다는 의미가 된다. 영향력이 큰 관찰값을 발견하는 데 필요한 통계량을 근거로 하여 특이한 관찰값 탐지에 사용되는 많은 R 함수가 있다. 그 중 패키지 car의 함수 influencePlot()에 대해 살펴보도록 하자. 이상값이란 추정된 회귀모형으로는 설명이 잘 안 되는 관찰값이라고 할 수 있는데, 대부분의 경우 이상값은 큰 잔차를 갖게 된다. 그러나 만일 이상값에 해당되는 관찰값이 영향력도 크다면 그림 5.3의 첫 번째 그래프에서 볼 수 있듯이 회귀계수의 추정을 왜곡시켜 그렇게 크지 않은 잔차를 갖게 될 수도 있다. 따라서 이상값을 판단하고자 한다면 일반적인 잔차보다는 스튜던트화 잔차를 이용하는 것이 더 효과적이라고 하겠다. 함수 influencePlot()은 스튜던트화 잔차와 레버리지, 그리고 Cook’s distance를 하나의 그래프에서 같이 보여줌으로써 특이한 관찰값을 분류하는 데 큰 도움이 되는 함수이다. 우선 5.1절에서 살펴본 states 자료에 대한 모형 fit_s를 대상으로 관찰값에 대한 진단을 실시해 보자. library(car) influencePlot(fit_s) 그림 5.11: 모형 fit_s에 대한 관찰값 진단 ## StudRes Hat CookD ## Alaska -1.180291 0.8147090 0.75856120 ## Hawaii 1.402156 0.4494369 0.19610477 ## Maine -2.203220 0.1242744 0.07886962 ## Nevada 2.316971 0.2937617 0.25282648 함수 influencePlot()으로 작성된 그래프는 각 관찰값의 레버리지를 X축 좌표로, 스튜던트화 잔차를 Y축 좌표로 하는 산점도이며, 점의 크기는 Cook’s distance에 비례하여 결정된다. 관찰값들의 평균 레버리지의 두 배와 세 배가 되는 지점에 수직 점선이 추가되며, 스튜던트화 잔차가 \\(\\pm 2\\) 인 지점에 수평 점선이 추가된다. 또한 X축과 Y축에서 가장 극단적인 두 점에는 데이터 프레임의 행 이름이 라벨로 표시되며, 콘솔 창에 해당 관찰값의 레버리지(Hat), 스튜던트화 잔차(StudRes), Cook’s distance(CookD) 값이 출력된다. 그래프에서 확인할 수 있는 것으로는 우선 Alaska의 경우에 leverage 값과 Cook’s distance의 값이 제일 커서 영향력이 큰 관찰값이라 할 수 있지만, 스튜던트화 잔차는 비교적 크지 않다는 점이다. Nevada의 경우에는 Cook’s distance와 스튜던트화 잔차가 비교적 크지만, leverage는 작은 값을 갖고 있다. 전체적으로, 회귀모형 fit_s에는 문제가 되는 관찰값이 없다고 할 수 있다. 이번에는 5.1절에서 살펴본 mtcars 자료에 대한 모형 fit_bic.2를 대상으로 관찰값에 대한 진단을 실시해 보자. influencePlot(fit_bic.2) 그림 5.12: 모형 fit_bic.2에 대한 관찰값 진단 ## StudRes Hat CookD ## Merc 230 -0.5584214 0.32256309 0.03805547 ## Lincoln Continental -0.9718419 0.36191253 0.13418893 ## Chrysler Imperial 1.6442934 0.31694630 0.29564936 ## Fiat 128 2.7615594 0.08752801 0.14788680 ## Toyota Corona -2.1542828 0.08927881 0.10065078 전체적으로, 회귀모형 fit_bic.2에도 큰 문제가 되는 관찰값은 없다고 할 수 있다. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
